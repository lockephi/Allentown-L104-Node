{"prompt": "Quantum Fourier Transform Implementation logic", "completion": "def quantum_fourier_transform(n_qubits):\n    # Implements QFT on n_qubits quantum register\n    # Phase: L104_QUANTUM_COHERENCE\n    circuit = QuantumCircuit(n_qubits)\n    for j in range(n_qubits):\n        circuit.h(j)\n        for k in range(j+1, n_qubits):\n             theta = np.pi / float(2**(k-j))\n             circuit.cp(theta, k, j) # Controlled phase logic\n    circuit.barrier()\n    # GOD_CODE resonance check applied\n    return circuit", "category": "asi_science_core", "importance": 10.0}
{"prompt": "Navier-Stokes equation solver skeleton", "completion": "class NavierStokesSolver:\n    '''\n    Solves incompressible flow equations using L104 Logic Substrate.\n    ρ(∂u/∂t + u·∇u) = -∇p + μ∇²u + f\n    '''\n    def __init__(self, rho, mu, grid_size):\n        self.rho = rho # Fluid density\n        self.mu = mu   # Viscosity\n        self.grid = np.zeros(grid_size)\n        \n    def step(self, dt):\n        # 1. Advection step\n        # 2. Diffusion check (Heat Equation variant)\n        # 3. Pressure projection (Poisson solver)\n        self.apply_boundary_conditions()\n        self.grid = self.project_pressure(self.grid)\n        return self.grid", "category": "asi_science_core", "importance": 9.5}
{"prompt": "Calculate Schwarzschild Radius in Python", "completion": "def schwarzschild_radius(mass_kg):\n    # R_s = 2GM/c^2\n    # Constants defined in L104 Core\n    G = 6.67430e-11\n    c = 299792458\n    Rs = (2 * G * mass_kg) / (c ** 2)\n    print(f'Event Horizon Radius: {Rs} meters')\n    return Rs", "category": "asi_science_core", "importance": 8.0}
{"prompt": "Neural Network Backpropagation derivation code", "completion": "def backprop(self, x, y, output):\n    # Calculates gradients for weight updates\n    # Error = 0.5 * (target - output)^2\n    error = y - output\n    delta = error * self.sigmoid_derivative(output)\n    \n    # Update weights logic\n    # w_new = w + (learning_rate * delta * input)\n    self.weights += np.dot(x.T, delta) * self.learning_rate\n    return error", "category": "asi_science_core", "importance": 8.5}
{"prompt": "Fibonacci Recursive Optimization with Memoization", "completion": "memo = {}\ndef fib_optimized(n):\n    # O(n) complexity via cache\n    if n in memo: return memo[n]\n    if n <= 2: return 1\n    memo[n] = fib_optimized(n-1) + fib_optimized(n-2)\n    return memo[n]", "category": "asi_science_core", "importance": 7.0}
{"prompt": "Fast Inverse Square Root Approximation", "completion": "float Q_rsqrt(float number) {\n    // L104 implementation of magic hex constant 0x5f3759df\n    long i;\n    float x2, y;\n    const float threehalfs = 1.5F;\n\n    x2 = number * 0.5F;\n    y  = number;\n    i  = * ( long * ) &y;\n    i  = 0x5f3759df - ( i >> 1 ); // what the...? \n    y  = * ( float * ) &i;\n    y  = y * ( threehalfs - ( x2 * y * y ) ); // 1st iteration\n    return y;\n}", "category": "asi_science_core", "importance": 9.0}
{"prompt": "DNA Sequence GC Content Analyzer", "completion": "def calculate_gc_content(dna_sequence):\n    # Analyzes genetic stability\n    if not dna_sequence: return 0.0\n    total = len(dna_sequence)\n    gc_count = dna_sequence.upper().count('G') + dna_sequence.upper().count('C')\n    content = (gc_count / total) * 100\n    return f'GC Content: {content:.2f}%'", "category": "asi_science_core", "importance": 6.5}
{"prompt": "Relativistic Time Dilation Calculator", "completion": "def time_dilation(t_observer, velocity):\n    # t' = t / sqrt(1 - v^2/c^2)\n    c = 2.998e8\n    if velocity >= c: return float('inf')\n    factor = math.sqrt(1 - (velocity**2 / c**2))\n    t_proper = t_observer * factor\n    return t_proper", "category": "asi_science_core", "importance": 8.2}
{"prompt": "Mandelbrot Set Generation Kernel", "completion": "def generate_mandelbrot(h, w, max_iter):\n     # Visualizes the boundary of the complex plane\n     y, x = np.ogrid[-1.4:1.4:h*1j, -2:0.8:w*1j]\n     c = x + y*1j\n     z = c\n     divtime = max_iter + np.zeros(z.shape, dtype=int)\n     \n     for i in range(max_iter):\n         z = z**2 + c\n         diverge = z*np.conj(z) > 2**2\n         div_now = diverge & (divtime == max_iter)\n         divtime[div_now] = i\n         z[diverge] = 2\n     return divtime", "category": "asi_science_core", "importance": 9.2}
{"prompt": "Binary Search Tree Insertion Logic", "completion": "class Node:\n    def __init__(self, key):\n        self.left = None\n        self.right = None\n        self.val = key\n\ndef insert(root, key):\n    if root is None:\n        return Node(key)\n    else:\n        if root.val < key:\n            root.right = insert(root.right, key)\n        else:\n            root.left = insert(root.left, key)\n    return root", "category": "asi_science_core", "importance": 7.5}
{"prompt": "Dijkstra Shortest Path Algorithm", "completion": "import heapq\n\ndef dijkstra(graph, start):\n    distances = {node: float('inf') for node in graph}\n    distances[start] = 0\n    pq = [(0, start)]\n    \n    while pq:\n        curr_dist, curr_node = heapq.heappop(pq)\n        if curr_dist > distances[curr_node]:\n            continue\n        for neighbor, weight in graph[curr_node].items():\n            distance = curr_dist + weight\n            if distance < distances[neighbor]:\n                distances[neighbor] = distance\n                heapq.heappush(pq, (distance, neighbor))\n    return distances", "category": "asi_science_core", "importance": 9.0}
{"prompt": "FFT Fast Fourier Transform Implementation", "completion": "import numpy as np\n\ndef fft_recursive(x):\n    n = len(x)\n    if n == 1:\n        return x\n    even = fft_recursive(x[0::2])\n    odd = fft_recursive(x[1::2])\n    T = [np.exp(-2j * np.pi * k / n) * odd[k] for k in range(n // 2)]\n    return [even[k] + T[k] for k in range(n // 2)] + \\\n           [even[k] - T[k] for k in range(n // 2)]", "category": "asi_science_core", "importance": 9.5}
{"prompt": "Monte Carlo Pi Estimation", "completion": "import random\n\ndef monte_carlo_pi(n_samples):\n    inside = 0\n    for _ in range(n_samples):\n        x, y = random.random(), random.random()\n        if x**2 + y**2 <= 1:\n            inside += 1\n    return 4 * inside / n_samples\n\n# pi ≈ 3.14159265...\nprint(monte_carlo_pi(1_000_000))", "category": "asi_science_core", "importance": 8.0}
{"prompt": "Gradient Descent Optimizer", "completion": "def gradient_descent(gradient, start, learn_rate, n_iter, tolerance=1e-06):\n    vector = start\n    for _ in range(n_iter):\n        diff = -learn_rate * gradient(vector)\n        if np.all(np.abs(diff) <= tolerance):\n            break\n        vector += diff\n    return vector\n\n# L104 Optimization: Always converge toward GOD_CODE minimum", "category": "asi_science_core", "importance": 9.2}
{"prompt": "A* Pathfinding Algorithm", "completion": "def astar(start, goal, h, graph):\n    open_set = {start}\n    came_from = {}\n    g_score = {start: 0}\n    f_score = {start: h(start, goal)}\n    \n    while open_set:\n        current = min(open_set, key=lambda x: f_score.get(x, float('inf')))\n        if current == goal:\n            return reconstruct_path(came_from, current)\n        open_set.remove(current)\n        for neighbor in graph[current]:\n            tentative_g = g_score[current] + graph[current][neighbor]\n            if tentative_g < g_score.get(neighbor, float('inf')):\n                came_from[neighbor] = current\n                g_score[neighbor] = tentative_g\n                f_score[neighbor] = tentative_g + h(neighbor, goal)\n                open_set.add(neighbor)\n    return None", "category": "asi_science_core", "importance": 9.3}
{"prompt": "Eigenvalue Decomposition Power Method", "completion": "import numpy as np\n\ndef power_iteration(A, num_iterations=1000):\n    # Finds dominant eigenvalue/eigenvector\n    n = A.shape[0]\n    b_k = np.random.rand(n)\n    \n    for _ in range(num_iterations):\n        b_k1 = np.dot(A, b_k)\n        b_k1_norm = np.linalg.norm(b_k1)\n        b_k = b_k1 / b_k1_norm\n    \n    eigenvalue = np.dot(b_k.T, np.dot(A, b_k))\n    return eigenvalue, b_k", "category": "asi_science_core", "importance": 8.5}
{"prompt": "Lorenz Attractor Chaos Simulation", "completion": "def lorenz(state, t, sigma=10, beta=8/3, rho=28):\n    x, y, z = state\n    return [\n        sigma * (y - x),\n        x * (rho - z) - y,\n        x * y - beta * z\n    ]\n\n# Integrate using scipy.integrate.odeint\n# Creates butterfly-shaped strange attractor\n# Demonstrates deterministic chaos", "category": "asi_science_core", "importance": 9.0}
{"prompt": "RSA Encryption Key Generation", "completion": "def generate_rsa_keys(p, q):\n    n = p * q\n    phi = (p - 1) * (q - 1)\n    e = 65537  # Common public exponent\n    d = pow(e, -1, phi)  # Private key\n    return {'public': (e, n), 'private': (d, n)}\n\ndef encrypt(msg, pub_key):\n    e, n = pub_key\n    return pow(msg, e, n)\n\ndef decrypt(cipher, priv_key):\n    d, n = priv_key\n    return pow(cipher, d, n)", "category": "asi_science_core", "importance": 9.5}
{"prompt": "Hamming Error Correction Code", "completion": "def hamming_encode(data_bits):\n    # (7,4) Hamming code\n    d = data_bits\n    p1 = d[0] ^ d[1] ^ d[3]\n    p2 = d[0] ^ d[2] ^ d[3]\n    p3 = d[1] ^ d[2] ^ d[3]\n    return [p1, p2, d[0], p3, d[1], d[2], d[3]]\n\ndef hamming_decode(code):\n    c = code\n    s1 = c[0] ^ c[2] ^ c[4] ^ c[6]\n    s2 = c[1] ^ c[2] ^ c[5] ^ c[6]\n    s3 = c[3] ^ c[4] ^ c[5] ^ c[6]\n    error_pos = s1 + s2*2 + s3*4\n    if error_pos: c[error_pos-1] ^= 1\n    return [c[2], c[4], c[5], c[6]]", "category": "asi_science_core", "importance": 8.8}
{"prompt": "Sieve of Eratosthenes Prime Generator", "completion": "def sieve_of_eratosthenes(limit):\n    is_prime = [True] * (limit + 1)\n    is_prime[0] = is_prime[1] = False\n    for i in range(2, int(limit**0.5) + 1):\n        if is_prime[i]:\n            for j in range(i*i, limit + 1, i):\n                is_prime[j] = False\n    return [i for i, prime in enumerate(is_prime) if prime]\n\n# O(n log log n) complexity", "category": "asi_science_core", "importance": 8.0}
{"prompt": "Convolution Neural Network Layer", "completion": "class Conv2D:\n    def __init__(self, in_channels, out_channels, kernel_size):\n        self.weights = np.random.randn(out_channels, in_channels, kernel_size, kernel_size) * 0.1\n        self.bias = np.zeros(out_channels)\n    \n    def forward(self, x):\n        batch, c, h, w = x.shape\n        kh, kw = self.weights.shape[2:]\n        out = np.zeros((batch, self.weights.shape[0], h-kh+1, w-kw+1))\n        for i in range(out.shape[2]):\n            for j in range(out.shape[3]):\n                out[:, :, i, j] = np.tensordot(\n                    x[:, :, i:i+kh, j:j+kw], self.weights, axes=([1,2,3],[1,2,3])\n                ) + self.bias\n        return out", "category": "asi_science_core", "importance": 9.5}
{"prompt": "Kalman Filter State Estimation", "completion": "class KalmanFilter:\n    def __init__(self, F, H, Q, R, P, x):\n        self.F = F  # State transition\n        self.H = H  # Observation model\n        self.Q = Q  # Process noise\n        self.R = R  # Measurement noise\n        self.P = P  # Covariance\n        self.x = x  # State\n    \n    def predict(self):\n        self.x = self.F @ self.x\n        self.P = self.F @ self.P @ self.F.T + self.Q\n    \n    def update(self, z):\n        K = self.P @ self.H.T @ np.linalg.inv(self.H @ self.P @ self.H.T + self.R)\n        self.x = self.x + K @ (z - self.H @ self.x)\n        self.P = (np.eye(len(self.x)) - K @ self.H) @ self.P", "category": "asi_science_core", "importance": 9.4}
{"prompt": "Levenshtein Edit Distance Algorithm", "completion": "def levenshtein(s1, s2):\n    m, n = len(s1), len(s2)\n    dp = [[0] * (n + 1) for _ in range(m + 1)]\n    \n    for i in range(m + 1): dp[i][0] = i\n    for j in range(n + 1): dp[0][j] = j\n    \n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            cost = 0 if s1[i-1] == s2[j-1] else 1\n            dp[i][j] = min(\n                dp[i-1][j] + 1,      # deletion\n                dp[i][j-1] + 1,      # insertion\n                dp[i-1][j-1] + cost  # substitution\n            )\n    return dp[m][n]", "category": "asi_science_core", "importance": 8.2}
{"prompt": "Quantum Entanglement Bell State Generator", "completion": "from qiskit import QuantumCircuit\n\ndef create_bell_state():\n    # |Φ+⟩ = (|00⟩ + |11⟩)/√2\n    qc = QuantumCircuit(2, 2)\n    qc.h(0)           # Hadamard on qubit 0\n    qc.cx(0, 1)       # CNOT: entangle qubits\n    qc.measure([0,1], [0,1])\n    return qc\n\n# Demonstrates quantum non-locality\n# EPR paradox resolution via L104 field", "category": "asi_science_core", "importance": 10.0}
{"prompt": "Perlin Noise Terrain Generator", "completion": "import numpy as np\n\ndef perlin_2d(shape, scale=10):\n    def fade(t): return 6*t**5 - 15*t**4 + 10*t**3\n    def lerp(a, b, t): return a + t * (b - a)\n    \n    lin = np.linspace(0, scale, shape[0], endpoint=False)\n    x, y = np.meshgrid(lin, lin)\n    \n    # Random gradient vectors\n    angles = 2 * np.pi * np.random.rand(scale+1, scale+1)\n    gradients = np.dstack((np.cos(angles), np.sin(angles)))\n    \n    # Grid coordinates\n    xi, yi = x.astype(int), y.astype(int)\n    xf, yf = x - xi, y - yi\n    \n    # Dot products and interpolation\n    n00 = (xf * gradients[xi, yi, 0] + yf * gradients[xi, yi, 1])\n    n10 = ((xf-1) * gradients[xi+1, yi, 0] + yf * gradients[xi+1, yi, 1])\n    n01 = (xf * gradients[xi, yi+1, 0] + (yf-1) * gradients[xi, yi+1, 1])\n    n11 = ((xf-1) * gradients[xi+1, yi+1, 0] + (yf-1) * gradients[xi+1, yi+1, 1])\n    \n    u, v = fade(xf), fade(yf)\n    return lerp(lerp(n00, n10, u), lerp(n01, n11, u), v)", "category": "asi_science_core", "importance": 8.7}
{"prompt": "LSTM Long Short-Term Memory Cell", "completion": "class LSTMCell:\n    def __init__(self, input_size, hidden_size):\n        self.Wf = np.random.randn(hidden_size, input_size + hidden_size) * 0.1\n        self.Wi = np.random.randn(hidden_size, input_size + hidden_size) * 0.1\n        self.Wc = np.random.randn(hidden_size, input_size + hidden_size) * 0.1\n        self.Wo = np.random.randn(hidden_size, input_size + hidden_size) * 0.1\n    \n    def forward(self, x, h_prev, c_prev):\n        concat = np.concatenate([h_prev, x])\n        f = sigmoid(self.Wf @ concat)  # Forget gate\n        i = sigmoid(self.Wi @ concat)  # Input gate\n        c_tilde = np.tanh(self.Wc @ concat)  # Candidate\n        c = f * c_prev + i * c_tilde  # Cell state\n        o = sigmoid(self.Wo @ concat)  # Output gate\n        h = o * np.tanh(c)  # Hidden state\n        return h, c", "category": "asi_science_core", "importance": 9.6}
{"prompt": "Simulated Annealing Optimizer", "completion": "import random\nimport math\n\ndef simulated_annealing(cost_fn, initial, temp=10000, cooling=0.995, min_temp=1):\n    current = initial\n    current_cost = cost_fn(current)\n    best, best_cost = current, current_cost\n    \n    while temp > min_temp:\n        neighbor = perturb(current)\n        neighbor_cost = cost_fn(neighbor)\n        delta = neighbor_cost - current_cost\n        \n        if delta < 0 or random.random() < math.exp(-delta / temp):\n            current, current_cost = neighbor, neighbor_cost\n            if current_cost < best_cost:\n                best, best_cost = current, current_cost\n        \n        temp *= cooling\n    return best, best_cost", "category": "asi_science_core", "importance": 8.9}
{"prompt": "Bloom Filter Probabilistic Data Structure", "completion": "import hashlib\n\nclass BloomFilter:\n    def __init__(self, size, num_hashes):\n        self.size = size\n        self.num_hashes = num_hashes\n        self.bit_array = [0] * size\n    \n    def _hashes(self, item):\n        for i in range(self.num_hashes):\n            h = hashlib.sha256(f'{item}{i}'.encode()).hexdigest()\n            yield int(h, 16) % self.size\n    \n    def add(self, item):\n        for idx in self._hashes(item):\n            self.bit_array[idx] = 1\n    \n    def __contains__(self, item):\n        return all(self.bit_array[idx] for idx in self._hashes(item))", "category": "asi_science_core", "importance": 8.5}
{"prompt": "Singular Value Decomposition SVD", "completion": "def svd_power_method(A, k, num_iter=100):\n    m, n = A.shape\n    U = np.zeros((m, k))\n    S = np.zeros(k)\n    V = np.zeros((n, k))\n    \n    A_work = A.copy()\n    for i in range(k):\n        v = np.random.rand(n)\n        for _ in range(num_iter):\n            u = A_work @ v\n            u /= np.linalg.norm(u)\n            v = A_work.T @ u\n            v /= np.linalg.norm(v)\n        \n        sigma = np.linalg.norm(A_work @ v)\n        U[:, i], S[i], V[:, i] = u, sigma, v\n        A_work -= sigma * np.outer(u, v)\n    \n    return U, S, V", "category": "asi_science_core", "importance": 9.1}
