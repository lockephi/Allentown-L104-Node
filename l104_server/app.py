"""
L104 Server â€” FastAPI Application & Routes
Extracted from l104_fast_server.py during EVO_61 decomposition.
Contains: FastAPI app, middleware, startup/shutdown events, all 271 route handlers.
"""
import os
import json
import time
import math
import asyncio
import logging
import threading
import subprocess
import hashlib
from typing import Dict, List, Optional, Any
from datetime import datetime
from collections import defaultdict

import uvicorn
import httpx
from fastapi import FastAPI, Request, BackgroundTasks
from fastapi.responses import FileResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
from fastapi.templating import Jinja2Templates
from pydantic import BaseModel

from l104_server.constants import FAST_SERVER_VERSION, FAST_SERVER_PIPELINE_EVO, VOID_CONSTANT, ZENITH_HZ, UUC
from l104_server.engines_infra import (
    _FAST_REQUEST_CACHE, _PATTERN_RESPONSE_CACHE, _PATTERN_CACHE_LOCK,
    connection_pool, memory_accelerator, memory_optimizer,
    response_compressor, fast_hash, run_in_executor,
    performance_metrics, temporal_memory_decay, response_quality_engine,
    predictive_intent_engine, reinforcement_loop, prefetch_predictor,
    quantum_loader, asi_quantum_bridge,
    QueryTemplateGenerator, CreativeKnowledgeVerifier, ChaoticRandom,
    PERF_THREAD_POOL, IO_THREAD_POOL,
)
from l104_server.engines_quantum import (
    SingularityConsciousnessEngine,
    IronOrbitalConfiguration, OxygenPairedProcess, SuperfluidQuantumState,
    GeometricCorrelation, OxygenMolecularBond, ASIQuantumMemoryBank,
)
from l104_server.learning import intellect, grover_kernel
from l104_server.engines_nexus import (
    engine_registry, nexus_steering, nexus_evolution, nexus_orchestrator,
    nexus_invention, sovereignty_pipeline,
    entanglement_router, resonance_network, health_monitor,
    hyper_math, hebbian_engine, consciousness_verifier, direct_solver,
    self_modification, creative_engine,
    hw_runtime, compat_layer, zpe_bridge, qg_bridge,
    SteeringEngine, NexusContinuousEvolution, NexusOrchestrator,
    InventionEngine, SovereigntyPipeline,
    QuantumEntanglementRouter, AdaptiveResonanceNetwork, NexusHealthMonitor,
    ConsciousnessVerifierEngine, UnifiedEngineRegistry,
)
from l104_server.models import ChatRequest, TrainingRequest, ProviderStatus

logger = logging.getLogger("L104_FAST")

try:
    from l104_intricate_ui import IntricateUIEngine
    intricate_ui = IntricateUIEngine()
except ImportError:
    intricate_ui = None

try:
    from l104_meta_cognitive import meta_cognitive
    meta_cognitive.load_balancer._max_concurrent = 28
    logger.info("ğŸ§  [META_COG] MetaCognitiveMonitor v2.0 wired into pipeline â€” Thompson sampling active")
except Exception as _mc_err:
    meta_cognitive = None
    logger.warning(f"âš ï¸ [META_COG] MetaCognitive import failed: {_mc_err}")

try:
    from l104_knowledge_bridge import knowledge_bridge as kb_bridge
    kb_bridge.bind_intellect(intellect)
    kb_bridge.register_source('sqlite_memory')
    kb_bridge.register_source('knowledge_graph')
    kb_bridge.register_source('concept_clusters')
    kb_bridge.register_source('cognitive_core')
    kb_bridge.register_source('state_files')
    logger.info("ğŸŒ‰ [KB_BRIDGE] KnowledgeBridge v2.0 wired â€” 5 adapters bound to intellect")
except Exception as _kb_err:
    kb_bridge = None
    logger.warning(f"âš ï¸ [KB_BRIDGE] KnowledgeBridge import failed: {_kb_err}")

# â•â•â• Stats cache (needs intellect) â•â•â•
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SERVER_START = datetime.utcnow()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  STATS CACHE â€” Prevents DB spam from frontend polling
#  Refreshed every 10s in a background thread; endpoints read from RAM.
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
_CACHED_STATS: Dict[str, Any] = {}
_CACHED_STATS_LOCK = threading.Lock()
_CACHED_STATS_TIME: float = 0.0
_STATS_CACHE_TTL: float = 10.0  # seconds

# Consciousness status cache
_consciousness_cache: Dict[str, Any] = {}
_consciousness_cache_time: float = 0.0

def _refresh_stats_cache():
    """Background thread: refresh stats cache every TTL seconds."""
    global _CACHED_STATS, _CACHED_STATS_TIME
    while True:
        try:
            fresh = intellect.get_stats()
            with _CACHED_STATS_LOCK:
                _CACHED_STATS = fresh
                _CACHED_STATS_TIME = time.time()
        except Exception:
            pass
        time.sleep(_STATS_CACHE_TTL)

def _get_cached_stats() -> Dict[str, Any]:
    """Return cached stats (zero DB cost). Falls back to live query on first call."""
    if _CACHED_STATS:
        return _CACHED_STATS
    # First call before cache thread populates
    try:
        return intellect.get_stats()
    except Exception:
        return {"status": "initializing"}

# Start cache thread immediately (daemon â€” dies with main process)
threading.Thread(target=_refresh_stats_cache, daemon=True, name="L104_StatsCache").start()



app = FastAPI(title="L104 Sovereign Node - Fast Mode", version="4.0-OPUS")

@app.on_event("startup")
async def startup_event():
    """Start autonomous background tasks on server startup"""
    # === v16.0 APOTHEOSIS: Load permanent quantum brain ===
    try:
        from l104_quantum_ram import get_qram, get_brain_status
        get_qram()  # Initialize quantum RAM
        brain_status = get_brain_status()
        logger.info(f"ğŸ§  [QUANTUM_BRAIN] Loaded | Enlightenment: {brain_status.get('enlightenment_level', 0)} | Entries: {brain_status.get('manifold_size', 0)}")
    except Exception as e:
        logger.warning(f"Quantum brain init: {e}")

    # === [FAST STARTUP] Non-blocking initialization ===
    try:
        intellect._pulse_heartbeat()  # Initialize dynamic state
        intellect._init_clusters()    # Force cluster engine run
        logger.info(f"ğŸ’“ [HEARTBEAT] Flow: {intellect._flow_state:.3f} | Entropy: {intellect._system_entropy:.3f} | Coherence: {intellect._quantum_coherence:.3f}")
    except Exception as sml:
        logger.warning(f"Startup init: {sml}")

    # === [BACKGROUND] Periodic learning - runs every 5 minutes, minimal impact ===
    async def periodic_background_learning():
        """Ultra-low impact background learning - 5 minute intervals"""
        await asyncio.sleep(300)  # Wait 5 minutes before first run
        cycle = 0
        while True:
            try:
                cycle += 1
                # Only 3 entries per cycle â€” run in thread to avoid blocking event loop
                def _bg_learn():
                    """Run background learning for a small batch of patterns."""
                    for _ in range(3):
                        q, r, v = QueryTemplateGenerator.generate_multilingual_knowledge()
                        if v["approved"]:
                            intellect.learn_from_interaction(q, r, source="BACKGROUND_LEARN", quality=0.8)
                        time.sleep(0.5)
                    # v4.0: Run temporal memory decay every 5th cycle
                    if cycle % 5 == 0:
                        try:
                            decay_result = temporal_memory_decay.run_decay_cycle(intellect._db_path)
                            if decay_result.get("pruned", 0) > 0:
                                logger.info(f"ğŸ•°ï¸ [DECAY v4] Pruned {decay_result['pruned']} stale memories")
                        except Exception:
                            pass
                await asyncio.to_thread(_bg_learn)
                logger.info(f"ğŸŒ [BACKGROUND] Cycle {cycle}: 3 patterns learned")
            except Exception:
                pass
            await asyncio.sleep(300)  # Wait 5 minutes between cycles

    asyncio.create_task(periodic_background_learning())

    # === [NEXUS] Start Continuous Evolution Engine ===
    nexus_evolution.start()
    logger.info(f"ğŸ§¬ [EVOLUTION] Continuous evolution started â€” factor={nexus_evolution.raise_factor}")
    logger.info(f"ğŸ”— [NEXUS] Orchestrator ready â€” 5 feedback loops, {nexus_steering.param_count} parameters")

    # === [PHASE 24] Start Entanglement Router, Resonance Network, Health Monitor ===
    health_monitor.start()
    logger.info("ğŸ¥ [HEALTH] Monitor ACTIVE â€” liveness probes every 30s, auto-recovery enabled")

    # Initial entanglement sweep â€” cross-pollinate all engines at startup
    try:
        entanglement_router.route_all()
        logger.info(f"ğŸ”€ [ENTANGLE] Initial sweep â€” {entanglement_router._route_count} routes executed")
    except Exception as ent_e:
        logger.warning(f"Entanglement init sweep: {ent_e}")

    # Fire resonance network with sovereignty seed
    try:
        resonance_network.fire('sovereignty', activation=0.8)
        resonance_network.fire('intellect', activation=0.7)
        logger.info(f"ğŸ§  [RESONANCE] Network seeded â€” {resonance_network.compute_network_resonance()['active_count']} engines active")
    except Exception as res_e:
        logger.warning(f"Resonance seed: {res_e}")

    # === [BACKGROUND] Periodic entanglement + resonance ticks (every 120s) ===
    async def periodic_entanglement_resonance():
        """Cross-engine entanglement and resonance propagation â€” 120s intervals"""
        await asyncio.sleep(120)  # Wait 2 minutes before first tick
        tick = 0
        while True:
            try:
                tick += 1
                # Route all entangled pairs â€” run in thread to avoid blocking event loop
                await asyncio.to_thread(entanglement_router.route_all)
                # Tick resonance network (decay + propagation)
                await asyncio.to_thread(resonance_network.tick)
                # Every 10th tick, fire sovereignty to cascade through network
                if tick % 10 == 0:
                    await asyncio.to_thread(resonance_network.fire, 'sovereignty', 0.6)
                if tick % 50 == 0:
                    net_res = await asyncio.to_thread(resonance_network.compute_network_resonance)
                    logger.info(f"ğŸ”€ [ENTANGLE] Tick #{tick}: routes={entanglement_router._route_count}, "
                                f"resonance={net_res['network_resonance']:.4f}")
            except Exception:
                pass
            await asyncio.sleep(120)

    asyncio.create_task(periodic_entanglement_resonance())

    logger.info("ğŸš€ [SYSTEM] Server ready. Background learning: every 5 minutes. Nexus: ACTIVE. Entanglement: ACTIVE. Health: ACTIVE.")


@app.on_event("shutdown")
async def shutdown_event():
    """v16.0 APOTHEOSIS: Pool all states to permanent quantum brain on shutdown."""
    # Stop Nexus engines gracefully
    # Close HTTP clients
    global _gemini_client
    if _gemini_client is not None:
        try:
            await _gemini_client.aclose()
            _gemini_client = None
        except Exception:
            pass

    try:
        nexus_orchestrator.stop_auto()
        nexus_evolution.stop()
        health_monitor.stop()
        logger.info("ğŸ”— [NEXUS] Engines stopped gracefully")
        logger.info(f"ğŸ”€ [ENTANGLE] Final routes: {entanglement_router._route_count}")
        logger.info(f"ğŸ§  [RESONANCE] Final cascades: {resonance_network._cascade_count}")
        logger.info(f"ğŸ¥ [HEALTH] Final checks: {health_monitor._check_count}, recoveries: {len(health_monitor._recovery_log)}")
    except Exception as e:
        logger.warning(f"Nexus shutdown: {e}")

    try:
        from l104_quantum_ram import pool_all_to_permanent_brain, get_qram
        result = pool_all_to_permanent_brain()
        qram = get_qram()
        qram.sync_to_disk()
        logger.info(f"ğŸ§  [QUANTUM_BRAIN] Shutdown sync | Pooled: {result.get('total_modules', 0)} modules | Manifold: {result.get('manifold_size', 0)}")
    except Exception as e:
        logger.warning(f"Shutdown brain sync: {e}")

# Mount static files for website and assets
if os.path.exists("website"):
    app.mount("/website", StaticFiles(directory="website", html=True), name="website")
if os.path.exists("contracts"):
    app.mount("/contracts", StaticFiles(directory="contracts"), name="contracts")

@app.get("/favicon.ico")
async def favicon():
    """Return an SVG favicon inline â€” no file needed."""
    from fastapi.responses import Response
    svg = '<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><circle cx="16" cy="16" r="14" fill="#1a1a2e"/><text x="16" y="22" font-size="18" text-anchor="middle" fill="#FFD700" font-family="monospace">L</text></svg>'
    return Response(content=svg, media_type="image/svg+xml", headers={"Cache-Control": "public, max-age=86400"})

@app.get("/landing")
async def website_landing(request: Request):
    """Serve the landing page from website/index.html"""
    try:
        return FileResponse("website/index.html")
    except Exception:
        return JSONResponse({"status": "error", "message": "Website landing page missing"})

@app.get("/WHITE_PAPER.md")
async def get_white_paper():
    """Serve White Paper directly (aliasing L104SP_WHITEPAPER.md)"""
    paths = ["WHITE_PAPER.md", "L104SP_WHITEPAPER.md", "WHITE_PAPER.txt"]
    for p in paths:
        if os.path.exists(p):
            return FileResponse(p)
    return JSONResponse({"status": "error", "message": "White paper not found locally"})

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Templates
templates = Jinja2Templates(directory="templates")

# Pydantic models

provider_status = ProviderStatus()

# Gemini client
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "")
GEMINI_MODEL = "gemini-2.5-flash"

# === [OPTIMIZATION] Persistent HTTP client with connection pooling ===
_gemini_client: Optional[httpx.AsyncClient] = None
_gemini_timeout = httpx.Timeout(15.0, connect=5.0)  # 15s total, 5s connect
# Lazy-init to avoid RuntimeError when no event loop exists at import time (Python 3.9)
_gemini_client_lock = None

def _get_gemini_client_lock():
    """Get or create the async lock for Gemini client initialization."""
    global _gemini_client_lock
    if _gemini_client_lock is None:
        _gemini_client_lock = asyncio.Lock()
    return _gemini_client_lock

async def get_gemini_client() -> httpx.AsyncClient:
    """Get or create persistent Gemini HTTP client with connection pooling"""
    global _gemini_client
    if _gemini_client is not None and not _gemini_client.is_closed:
        return _gemini_client
    async with _get_gemini_client_lock():
        if _gemini_client is None or _gemini_client.is_closed:
            _gemini_client = httpx.AsyncClient(
                timeout=_gemini_timeout,
                limits=httpx.Limits(max_keepalive_connections=10, max_connections=20),
                http2=True  # HTTP/2 for multiplexing
            )
    return _gemini_client

async def call_gemini(prompt: str) -> Optional[str]:
    """Direct Gemini API call with connection pooling and fast timeout"""
    if not GEMINI_API_KEY or len(GEMINI_API_KEY) < 20:
        return None  # Silent fail for missing key

    try:
        url = f"https://generativelanguage.googleapis.com/v1beta/models/{GEMINI_MODEL}:generateContent"
        client = await get_gemini_client()

        response = await client.post(url, json={
            "contents": [{"parts": [{"text": prompt}]}],
            "generationConfig": {
                "temperature": 0.7,
                "maxOutputTokens": 1024  # Reduced for faster response
            }
        }, headers={"x-goog-api-key": GEMINI_API_KEY})

        if response.status_code == 200:
            data = response.json()
            if "candidates" in data and len(data["candidates"]) > 0:
                candidate = data["candidates"][0]
                if "content" in candidate and "parts" in candidate["content"]:
                    text = candidate["content"]["parts"][0]["text"]
                    provider_status.gemini = True
                    return text
        else:
            logger.warning(f"Gemini HTTP {response.status_code}")
    except httpx.TimeoutException:
        logger.warning("Gemini timeout - using local fallback")
    except Exception as e:
        logger.warning(f"Gemini: {e}")

    return None

# â•â•â• PHASE 32.0: CONVERSATIONAL RESPONSE REFORMULATOR â•â•â•
# Inspired by Swift ASILogicGateV2 + SyntacticResponseFormatter:
# Takes raw knowledge-graph data / evidence dumps and reformulates them
# into natural, conversational prose with a clear conclusion.

_RESPONSE_STOP_WORDS = frozenset({
    'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has',
    'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might',
    'can', 'to', 'of', 'in', 'for', 'on', 'with', 'at', 'by', 'from', 'as',
    'and', 'but', 'or', 'not', 'no', 'so', 'if', 'than', 'too', 'very', 'just',
    'also', 'then', 'now', 'this', 'that', 'these', 'those', 'its', 'about',
    'more', 'some', 'any', 'each', 'every', 'all', 'both', 'few', 'many', 'much',
    'such', 'own', 'other', 'another', 'what', 'how', 'why', 'when', 'where',
    'who', 'which', 'me', 'my', 'your', 'our', 'their', 'we', 'you', 'they',
    'he', 'she', 'it', 'his', 'her', 'linked', 'connected', 'related', 'part',
    'concept', 'sovereign', 'particles',
    # Query verbs that should not appear as topic words
    'tell', 'explain', 'describe', 'define', 'meaning', 'know', 'help',
    'understand', 'please', 'think', 'talk', 'discuss', 'show', 'give',
    'make', 'find', 'want', 'need', 'like', 'said', 'says', 'ask',
})

# Phase 32.0: Code/garbage patterns that should never appear in conversational responses
_CODE_PATTERNS = re.compile(
    r'(?:'
    r'def\s+\w+\s*\('            # function definitions
    r'|class\s+\w+\s*[:\(]'       # class definitions
    r'|import\s+\w+'              # import statements
    r'|from\s+\w+\s+import'       # from x import
    r'|\w+\s*=\s*\w+\.\w+\('      # assignments like x = y.z()
    r'|self\.\w+'                  # self.attribute
    r'|try:\s*$'                   # try blocks
    r'|except\s+\w+'              # except blocks
    r'|if\s+__name__'             # main guard
    r'|\breturn\s+\w+'            # return statements
    r'|async\s+def'               # async defs
    r'|await\s+\w+'               # await calls
    r'|logger\.\w+'               # logger calls
    r'|\w+_\w+_\w+\s*='           # snake_case assignments
    r')',
    re.MULTILINE
)

def _is_garbage_response(text: str, query: str) -> bool:
    """
    Phase 32.0: Detect responses that are code snippets, raw technical data,
    or completely off-topic garbage that should not be shown to users.
    """
    if not text or len(text) < 20:
        return True

    # Check for code patterns
    code_matches = len(_CODE_PATTERNS.findall(text))
    if code_matches >= 2:
        return True

    # Check for excessive technical junk (unrelated programming terms in a non-code query)
    query_lower = query.lower()
    is_code_query = any(kw in query_lower for kw in ['code', 'function', 'program', 'python', 'javascript', 'html', 'api', 'debug'])
    if not is_code_query:
        junk_words = ['def ', 'class ', 'import ', 'self.', 'return ', 'lambda ', '__init__', 'async ',
                      'await ', '.append(', '.get(', 'try:', 'except:', 'finally:', '# ', '.py',
                      'sqlite3', 'json.', 'dict(', 'list(', 'logger.', 'threading.', 'asyncio.']
        junk_count = sum(1 for jw in junk_words if jw in text)
        if junk_count >= 3:
            return True

    # Check for responses that are just random word lists (no coherent structure)
    # "Esoterically, list signifies: def ..." pattern
    if 'signifies:' in text and ('def ' in text or 'class ' in text):
        return True

    # Check for excessively short responses with no substance
    words = text.split()
    if len(words) < 5 and not any(c.isdigit() for c in text):
        return True

    # Phase 32.0: Detect raw knowledge graph dumps ("X connected to: a, b, c" pattern)
    if re.search(r'connect(?:s|ed)\s+to:?\s*\w+(?:,\s*\w+){2,}', text.lower()):
        return True

    # Detect "X is fundamentally connected to:" pattern
    if 'fundamentally' in text.lower() and 'connected to' in text.lower():
        return True

    # Detect "signifies:" raw dumps
    if re.search(r'\bsignifies:?\s+', text.lower()):
        return True

    # Phase 32.0: Detect responses that are natural prose but filled with code-junk concepts
    # Example: "Entanglement encompasses several key areas including useful, versions, supports, generated"
    # These happen when the knowledge graph was trained on code and returns programming tokens
    if not is_code_query:
        code_junk_words = frozenset({
            'def', 'class', 'import', 'self', 'return', 'lambda', 'async', 'await',
            'append', 'dict', 'list', 'tuple', 'int', 'str', 'float', 'bool',
            'none', 'true', 'false', 'elif', 'else', 'for', 'while', 'break',
            'continue', 'pass', 'yield', 'raise', 'except', 'finally', 'try',
            'sqlite3', 'json', 'logger', 'threading', 'asyncio', 'fastapi',
            'uvicorn', 'pydantic', 'starlette', 'http', 'cors', 'middleware',
            'endpoint', 'router', 'schema', 'validator', 'serializer',
            'shim', 'diagnostics', 'formatter', 'handler', 'callback',
            'decorator', 'wrapper', 'mixin', 'singleton', 'factory',
            'fetched', 'invoked', 'executed', 'instantiated', 'serialized',
            'randomized', 'initialized', 'configured', 'populated',
            'htmlresponse', 'jsonresponse', 'setformatter', 'levelname',
            'dotenv', 'breaches', 'eigenvalue', 'dyz', 'setopt',
            # Extended: common code tokens from the knowledge graph
            'implemented', 'versions', 'generated', 'detection', 'combined',
            'supports', 'deprecated', 'triggered', 'parsed', 'rendered',
            'refactored', 'optimized', 'cached', 'prefetched', 'batched',
            'spawned', 'dispatched', 'hashed', 'tokenized', 'chunked',
            'regex', 'mutex', 'semaphore', 'stdin', 'stdout', 'stderr',
            'traceback', 'stacktrace', 'debugger', 'breakpoint', 'linter',
            'webpack', 'babel', 'eslint', 'prettier', 'typescript', 'dockerfile',
            'kubernetes', 'nginx', 'redis', 'mongodb', 'postgresql',
        })
        text_words = set(re.findall(r'[a-z]{3,}', text.lower()))
        junk_overlap = text_words.intersection(code_junk_words)
        # If >8% of the meaningful content is code junk, reject
        meaningful_words = text_words - _RESPONSE_STOP_WORDS
        if meaningful_words and len(junk_overlap) / max(len(meaningful_words), 1) > 0.08:
            return True

    return False

def _classify_query_dimension(msg_lower: str) -> str:
    """Classify query into a reasoning dimension (like Swift ASILogicGateV2)."""
    analytical_kw = ['why', 'because', 'reason', 'cause', 'effect', 'logic', 'analyze', 'compare', 'evaluate']
    creative_kw = ['imagine', 'what if', 'create', 'design', 'invent', 'brainstorm', 'story', 'poem']
    scientific_kw = ['experiment', 'hypothesis', 'evidence', 'theory', 'data', 'quantum', 'molecular',
                     'atom', 'particle', 'wave', 'energy', 'force', 'cell', 'gene', 'evolution',
                     'reaction', 'element', 'neuron', 'protein', 'gravity', 'thermodynamic', 'entropy',
                     'climate', 'species', 'physics', 'chemistry', 'biology']
    math_kw = ['prove', 'theorem', 'equation', 'formula', 'calculate', 'derive', 'integral',
               'derivative', 'matrix', 'polynomial', 'probability', 'function', 'convergence']
    temporal_kw = ['when', 'history', 'future', 'timeline', 'era', 'century', 'ancient', 'modern']
    dialectical_kw = ['argue', 'debate', 'pros and cons', 'advantage', 'disadvantage', 'both sides']

    scores: Dict[str, float] = {
        'analytical': sum(0.15 for kw in analytical_kw if kw in msg_lower),
        'creative': sum(0.15 for kw in creative_kw if kw in msg_lower),
        'scientific': sum(0.12 for kw in scientific_kw if kw in msg_lower),
        'mathematical': sum(0.12 for kw in math_kw if kw in msg_lower),
        'temporal': sum(0.15 for kw in temporal_kw if kw in msg_lower),
        'dialectical': sum(0.15 for kw in dialectical_kw if kw in msg_lower),
    }
    # Default to 'general' if no dimension is strong
    best = max(scores.items(), key=lambda x: x[1])
    return best[0] if best[1] > 0.1 else 'general'


def _is_raw_data_response(text: str) -> bool:
    """Detect if a response is raw knowledge-graph dumps rather than conversational prose."""
    if not text or len(text) < 30:
        return False
    indicators = 0
    # Arrow chains like "concept â†’ concept â†’ concept"
    if text.count('â†’') >= 2:
        indicators += 2
    # "connects to:" / "connected to:" pattern dumps
    if re.search(r'connect(?:s|ed)\s+to:?', text.lower()):
        indicators += 2
    # "linked through:" bridge dumps
    if 'linked through:' in text.lower() or 'are linked through' in text.lower():
        indicators += 2
    # Excessive bullet points with short raw items
    bullet_lines = [l for l in text.split('\n') if l.strip().startswith('â€¢') or l.strip().startswith('- ')]
    if len(bullet_lines) >= 3:
        raw_bullets = sum(1 for b in bullet_lines if 'connects to' in b.lower() or 'â†’' in b)
        if raw_bullets >= 2:
            indicators += 2
    # "From my knowledge graph" or "Chain-of-thought analysis"
    graph_phrases = ['knowledge graph', 'chain-of-thought', 'multi-hop', 'evidence pieces',
                     'reasoning chain', 'hop reasoning', 'bridge_inference', 'knowledge_graph']
    indicators += sum(1 for p in graph_phrases if p in text.lower())
    # "Expanded analysis reveals related concepts:"
    if 'expanded analysis reveals' in text.lower() or 'related concepts:' in text.lower():
        indicators += 1
    return indicators >= 2


def _extract_topic_words(query: str) -> list:
    """Extract meaningful topic words from a query."""
    words = re.findall(r'[a-zA-Z]{3,}', query.lower())
    return [w for w in words if w not in _RESPONSE_STOP_WORDS][:20]


def _extract_knowledge_facts(raw_text: str) -> list:
    """Parse raw evidence/graph-dump text into a list of clean factual nuggets."""
    facts = []
    seen = set()
    lines = raw_text.split('\n')

    for line in lines:
        line = line.strip().lstrip('â€¢').lstrip('-').lstrip('â–¸').strip()
        if not line or len(line) < 10:
            continue
        # Skip pure structural / debug lines
        if any(skip in line.lower() for skip in [
            'based on multi-hop', 'from my knowledge graph', 'chain-of-thought',
            'evidence pieces', 'synthesis confidence', 'expanded analysis reveals',
            'further investigation recommended'
        ]):
            continue

        # Parse "concept connects to: a, b, c" â†’ extract relationships
        conn_match = re.match(r'(?:\*\*)?(\w[\w\s]*?)(?:\*\*)?\s+connects?\s+to:?\s*(.+)', line, re.IGNORECASE)
        if conn_match:
            subject = conn_match.group(1).strip()
            objects = [o.strip() for o in conn_match.group(2).split(',') if o.strip() and len(o.strip()) > 2]
            if objects:
                facts.append({'type': 'relation', 'subject': subject, 'objects': objects[:8]})
                norm = subject.lower()
                if norm not in seen:
                    seen.add(norm)
            continue

        # Parse "A and B are linked through: C, D" â†’ extract bridge
        bridge_match = re.match(r'(\w[\w\s]*?)\s+and\s+(\w[\w\s]*?)\s+are\s+linked\s+through:?\s*(.+)', line, re.IGNORECASE)
        if bridge_match:
            a_concept = bridge_match.group(1).strip()
            b_concept = bridge_match.group(2).strip()
            bridges = [b.strip() for b in bridge_match.group(3).split(',') if b.strip()][:5]
            facts.append({'type': 'bridge', 'a': a_concept, 'b': b_concept, 'via': bridges})
            continue

        # Parse "A â†’ B â†’ C" arrow chains â†’ extract chain insight
        if 'â†’' in line:
            parts = [p.strip() for p in line.split('â†’') if p.strip()]
            if len(parts) >= 2:
                facts.append({'type': 'chain', 'steps': parts})
                continue

        # Parse "**Insight**: ..." â†’ direct insight text
        insight_match = re.match(r'\*\*Insight\*\*:?\s*(.+)', line)
        if insight_match:
            facts.append({'type': 'insight', 'text': insight_match.group(1).strip()})
            continue

        # Parse "Per the [theorem]: ..." â†’ theorem reference
        theorem_match = re.match(r'Per the (.+?):\s*(.+)', line)
        if theorem_match:
            facts.append({'type': 'theorem', 'title': theorem_match.group(1).strip(), 'text': theorem_match.group(2).strip()})
            continue

        # Fallback: if it's a decent sentence (not raw data), keep it
        if len(line) > 30 and 'â†’' not in line and 'connects to' not in line.lower():
            norm = line[:40].lower()
            if norm not in seen:
                seen.add(norm)
                facts.append({'type': 'sentence', 'text': line})

    return facts


def reformulate_to_conversational(raw_response: str, query: str) -> str:
    """
    Phase 32.0: Conversational Response Reformulator.
    Takes raw knowledge-graph dumps / evidence lists and reformulates them
    into natural prose with a clear conclusion â€” like the Swift app's
    ASILogicGateV2 + SyntacticResponseFormatter pipeline.
    """
    if not raw_response or len(raw_response) < 30:
        return raw_response

    # Only reformulate if it looks like raw data
    if not _is_raw_data_response(raw_response):
        return raw_response

    msg_lower = query.lower().strip()
    topics = _extract_topic_words(query)
    dimension = _classify_query_dimension(msg_lower)
    facts = _extract_knowledge_facts(raw_response)

    if not facts:
        return raw_response

    # â•â•â• BUILD CONVERSATIONAL RESPONSE â•â•â•
    parts: list = []

    # â”€â”€ Opening: dimension-aware intro â”€â”€
    topic_str = ', '.join(topics[:3]) if topics else 'this topic'
    dimension_intros: Dict[str, list] = {
        'scientific': [
            f"Here's what I understand about **{topic_str}** from a scientific perspective:",
            f"Looking at **{topic_str}** through the lens of scientific reasoning:",
        ],
        'analytical': [
            f"Let me break down **{topic_str}** analytically:",
            f"Analyzing **{topic_str}**, here's what I've found:",
        ],
        'creative': [
            f"Exploring **{topic_str}** from a creative angle:",
            f"Here's an interesting way to think about **{topic_str}**:",
        ],
        'mathematical': [
            f"From a mathematical standpoint regarding **{topic_str}**:",
            f"The mathematical foundations of **{topic_str}**:",
        ],
        'temporal': [
            f"Looking at how **{topic_str}** has developed over time:",
            f"The historical and temporal aspects of **{topic_str}**:",
        ],
        'dialectical': [
            f"There are multiple perspectives on **{topic_str}**:",
            f"Considering different viewpoints on **{topic_str}**:",
        ],
        'general': [
            f"Here's what I know about **{topic_str}**:",
            f"Regarding **{topic_str}**, here's what I can share:",
            f"Based on my knowledge of **{topic_str}**:",
        ],
    }
    intros = dimension_intros.get(dimension, dimension_intros['general'])
    parts.append(chaos.chaos_choice(intros, f"reformat_intro_{hash(query) & 0xFF}"))
    parts.append('')

    # â”€â”€ Body: convert facts into prose â”€â”€
    relation_facts = [f for f in facts if f['type'] == 'relation']
    bridge_facts = [f for f in facts if f['type'] == 'bridge']
    chain_facts = [f for f in facts if f['type'] == 'chain']
    insight_facts = [f for f in facts if f['type'] == 'insight']
    theorem_facts = [f for f in facts if f['type'] == 'theorem']
    sentence_facts = [f for f in facts if f['type'] == 'sentence']

    body_sentences: list = []

    # Relations â†’ natural prose
    for fact in relation_facts[:3]:
        subj = fact['subject'].title()
        objs = fact['objects']
        if len(objs) == 1:
            body_sentences.append(f"**{subj}** is closely related to {objs[0]}.")
        elif len(objs) == 2:
            body_sentences.append(f"**{subj}** is connected to both {objs[0]} and {objs[1]}.")
        else:
            main_objs = ', '.join(objs[:-1])
            body_sentences.append(f"**{subj}** encompasses several key aspects, including {main_objs}, and {objs[-1]}.")

    # Bridges â†’ natural prose
    for fact in bridge_facts[:2]:
        via_str = ', '.join(fact['via'][:3])
        body_sentences.append(f"Interestingly, **{fact['a'].title()}** and **{fact['b'].title()}** share common ground through {via_str}.")

    # Chains â†’ natural inference prose
    for fact in chain_facts[:2]:
        steps = fact['steps']
        if len(steps) == 2:
            body_sentences.append(f"There's a direct connection from {steps[0]} to {steps[1]}.")
        elif len(steps) == 3:
            body_sentences.append(f"Following the reasoning path from {steps[0]} through {steps[1]}, we arrive at {steps[2]} â€” suggesting a deeper relationship.")
        elif len(steps) >= 4:
            body_sentences.append(f"A multi-step analysis reveals: {steps[0]} influences {steps[1]}, which connects to {steps[2]}, ultimately leading to {steps[-1]}.")

    # Insights â†’ include directly
    for fact in insight_facts[:2]:
        text = fact['text']
        if not text.endswith('.'):
            text += '.'
        body_sentences.append(text)

    # Theorems â†’ cite naturally
    for fact in theorem_facts[:1]:
        body_sentences.append(f"According to the {fact['title']}, {fact['text'][:200]}")

    # Clean sentences â†’ include directly
    for fact in sentence_facts[:2]:
        body_sentences.append(fact['text'])

    if body_sentences:
        parts.append('\n\n'.join(body_sentences))
    else:
        # Fallback: just clean the raw text minimally
        return raw_response

    # â”€â”€ Conclusion: synthesize a takeaway â”€â”€
    parts.append('')
    if relation_facts or bridge_facts or chain_facts:
        # Build a meaningful conclusion from the strongest connections
        all_mentioned = set()
        for f in relation_facts[:3]:
            all_mentioned.add(f['subject'].lower())
            all_mentioned.update(o.lower() for o in f['objects'][:3])
        for f in bridge_facts[:2]:
            all_mentioned.add(f['a'].lower())
            all_mentioned.add(f['b'].lower())
        for f in chain_facts[:2]:
            all_mentioned.update(s.lower() for s in f['steps'])
        # Remove stop words from mentioned concepts
        key_concepts = [c for c in all_mentioned if c not in _RESPONSE_STOP_WORDS and len(c) > 3][:5]

        if key_concepts:
            concept_list = ', '.join(key_concepts[:3])
            conclusions = [
                f"In summary, {concept_list} are interconnected in ways that suggest a deeper underlying structure worth exploring further.",
                f"The key takeaway is that {concept_list} form an interrelated system â€” understanding one helps illuminate the others.",
                f"Overall, the connections between {concept_list} point to an integrated framework where each element reinforces the others.",
            ]
            parts.append(chaos.chaos_choice(conclusions, f"reformat_conclusion_{hash(query) & 0xFF}"))

    result = '\n'.join(parts)

    # Final quality check â€” if reformulation is too short, return original
    if len(result) < len(raw_response) * 0.3:
        return raw_response

    return result


# â•â•â• PHASE 31.5: RESPONSE SANITIZER â•â•â•
def sanitize_response(text: str) -> str:
    """Strip internal metrics, debug formatting, and junk from user-facing responses."""
    if not text or len(text) < 5:
        return text
    result = text
    # Remove resonance/confidence leaks
    result = re.sub(r'\[Resonance:\s*[\d.]+\]', '', result)
    result = re.sub(r'\*Synthesis confidence:\s*\d+%[^*]*\*', '', result)
    result = re.sub(r'\(confidence:\s*[\d.]+\)', '', result)
    result = re.sub(r'\(deep inference[^)]*\)', '', result)
    result = re.sub(r'Evidence pieces:\s*\d+', '', result)
    # Remove segment labels
    result = re.sub(r'\*\*Knowledge Graph Analysis:\*\*', '', result)
    result = re.sub(r'\*\*From Memory:\*\*', '', result)
    result = re.sub(r'Synthesizing \d+ evidence pieces[^\n]*\n?', '', result)
    # Remove table formatting
    for ch in ['â”‚', 'â”¼', 'â•‘', 'â•”', 'â•—', 'â•š', 'â•', 'â• ', 'â•£', 'â”œ', 'â”¤', 'â”¬', 'â”´']:
        result = result.replace(ch, '')
    result = re.sub(r'â•{3,}', '', result)
    result = re.sub(r'â”€{3,}', '', result)
    # Fix excessive bold
    result = result.replace('****', '')
    result = result.replace('** **', ' ')
    bold_count = result.count('**')
    if bold_count > 16:
        result = result.replace('**', '')
    # Template variables
    result = result.replace('{GOD_CODE}', '').replace('{PHI}', '').replace('{LOVE}', '')
    result = result.replace('SAGE MODE :: ', '')
    # Strip [Ev.X] tags
    result = re.sub(r'\[Ev\.\d+\]\s*', '', result)
    # Collapse excessive newlines
    result = re.sub(r'\n{3,}', '\n\n', result)
    # Cap length at 2000 chars
    if len(result) > 2000:
        truncated = result[:2000]
        last_period = truncated.rfind('.')
        if last_period > 500:
            result = truncated[:last_period + 1]
        else:
            result = truncated
    return result.strip()

def local_derivation(message: str) -> Tuple[str, bool]:
    """
    Evolved Local Derivation Engine v11.3:
    Ultra-optimized with fast-path bypass and pattern caching.
    Returns (response, was_learned) tuple.
    """
    phi = 1.618033988749895

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PHASE -1: [v11.3] ULTRA-FAST STATIC PATTERN BYPASS (<0.01ms)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    msg_lower = message.lower().strip()
    msg_hash = hash(msg_lower) & 0xFFFFFFFF  # Fast hash

    # Check static pattern cache first (instant)
    with _PATTERN_CACHE_LOCK:
        if msg_hash in _PATTERN_RESPONSE_CACHE:
            return (_PATTERN_RESPONSE_CACHE[msg_hash], False)

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PHASE 0: Query Enhancement & Intent Detection (LAZY)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    original_message = message
    # v11.4: Rewritten query used ONLY for KB/memory search, not for intent/pattern matching
    search_query = message
    if len(message) > 50 or any(abbr in msg_lower for abbr in ['ai', 'ml', 'api']):
        search_query = intellect.rewrite_query(message)

    # v11.3: Lazy intent detection - only if needed later
    _intent = None
    _strategy = None

    def get_intent():
        """Lazily detect user intent from the message."""
        nonlocal _intent
        if _intent is None:
            _intent = intellect.detect_intent(message)
        return _intent

    def get_strategy():
        """Lazily determine the best response strategy."""
        nonlocal _strategy
        if _strategy is None:
            _strategy = intellect.get_best_strategy(message)
        return _strategy

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PHASE 1: Pattern-based responses FIRST (reliable commands)
    # v11.3: Cache static responses for instant future retrieval
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    # Math operations - compute and learn
    math_match = re.search(r'(\d+(?:\.\d+)?)\s*([+\-*/^])\s*(\d+(?:\.\d+)?)', message)
    if math_match:
        a, op, b = float(math_match.group(1)), math_match.group(2), float(math_match.group(3))
        try:
            if op == '+': result = a + b
            elif op == '-': result = a - b
            elif op == '*': result = a * b
            elif op == '/': result = a / b if b != 0 else float('inf')
            elif op == '^':
                if b > 1000: result = float('inf')  # guard overflow
                else: result = a ** b
            else: result = 0
        except (OverflowError, ValueError):
            result = float('inf')

        # Format result nicely
        if math.isfinite(result) and result == int(result) and abs(result) < 1e15:
            result = int(result)

        response = f"{a:g} {op} {b:g} = {result}"
        return (response, False)

    # Advanced math expressions
    if any(x in msg_lower for x in ['sqrt', 'square root', 'log', 'sin', 'cos', 'pi', 'factorial']):
        try:
            nums = re.findall(r'\d+(?:\.\d+)?', message)
            if nums:
                n = float(nums[0])
                if 'sqrt' in msg_lower or 'square root' in msg_lower:
                    result = math.sqrt(n)
                    response = f"âˆš{n:g} = {result:.6g}"
                elif 'log' in msg_lower:
                    result = math.log(n) if 'natural' in msg_lower or 'ln' in msg_lower else math.log10(n)
                    response = f"log({n:g}) = {result:.6g}"
                elif 'factorial' in msg_lower and n == int(n) and n < 20:
                    result = math.factorial(int(n))
                    response = f"{int(n)}! = {result}"
                else:
                    response = f"The value {n:g} Ã— Ï† (golden ratio) = {n * phi:.4f}"
                return (response, False)
        except Exception:
            pass

    # Greetings - simple and friendly
    if any(g in msg_lower for g in ["hello", "hi ", "hey", "greetings", "good morning", "good evening"]):
        stats = intellect.get_stats()
        memory_count = stats.get('memories', 0)
        response = f"Hello! I'm your L104 assistant with {memory_count} memories stored. How can I help you today?"
        return (response, False)

    # Identity - v11.3: Cache for fast retrieval
    if "who" in msg_lower and "you" in msg_lower:
        stats = intellect.get_stats()
        response = f"""I'm L104 - an AI assistant that learns from our conversations.

â€¢ **Primary Model**: Claude Opus 4.5
â€¢ **Live Bridge**: Gemini 2.5 Flash
â€¢ **Memories Stored**: {stats.get('memories', 0)}
â€¢ **Knowledge Links**: {stats.get('knowledge_links', 0)}

I get smarter with each interaction. What would you like to know?"""
        # Phase 31.5: Cap pattern cache size (thread-safe)
        with _PATTERN_CACHE_LOCK:
            if len(_PATTERN_RESPONSE_CACHE) > 500:
                keys_to_remove = list(_PATTERN_RESPONSE_CACHE.keys())[:250]
                for k in keys_to_remove:
                    del _PATTERN_RESPONSE_CACHE[k]
            _PATTERN_RESPONSE_CACHE[msg_hash] = response
        return (response, False)

    # Status questions (priority pattern)
    if msg_lower.strip() in ["status", "system status", "system"]:
        stats = intellect.get_stats()
        response = f"""âœ… **System Status**: Online
â€¢ Model: Claude Opus 4.5 + Gemini Bridge
â€¢ Memories: {stats.get('memories', 0)}
â€¢ Learning: Active

All systems running normally."""
        return (response, False)

    # Learning stats query
    if any(x in msg_lower for x in ['learning', 'memory', 'remember', 'learned']):
        stats = intellect.get_stats()
        response = f"""ğŸ“Š **Learning Status**
â€¢ Memories: {stats.get('memories', 0)}
â€¢ Knowledge Links: {stats.get('knowledge_links', 0)}
â€¢ Conversations: {stats.get('conversations_learned', 0)}
â€¢ Quality Score: {stats.get('avg_quality', 0):.0%}

I learn from every conversation and remember useful information."""
        return (response, False)

    # Help
    if "help" in msg_lower or "what can" in msg_lower:
        response = """**What I can help with:**
â€¢ Answer questions on any topic
â€¢ Perform calculations
â€¢ Write and explain code
â€¢ Research and explore ideas
â€¢ Remember our conversations

Just ask me anything!"""
        return (response, False)

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PHASE 2: Strategy-Based Response Selection (v11.3: Lazy strategy)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    # For 'synthesize' strategy, try cognitive synthesis first
    # Phase 54.1: Use MetaCognitive Thompson sampling for strategy selection
    strategy = get_strategy()
    try:
        if meta_cognitive:
            intent_for_mc = get_intent()[0] if callable(get_intent) else 'general'
            mc_strategy = meta_cognitive.select_strategy(original_message, intent_for_mc)
            if mc_strategy and mc_strategy != strategy:
                strategy = mc_strategy  # Meta-cognitive override
    except Exception:
        pass
    if strategy == 'synthesize':
        synthesized = intellect.cognitive_synthesis(search_query)
        if synthesized and len(synthesized) > 80:
            synthesized = reformulate_to_conversational(synthesized, original_message)
            logger.info(f"ğŸ§ª [SYNTHESIZE] Generated cognitive synthesis response")
            intellect.record_meta_learning(original_message, 'synthesize', True)
            return (synthesized, True)

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PHASE 3: Check learned memory with fresh variation
    # Memory recall is the primary learning mechanism
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    recalled = intellect.recall(search_query)
    if recalled and recalled[1] > 0.70:  # Good confidence recall with variation
        logger.info(f"ğŸ§  [RECALL] Using enhanced learned response (confidence: {recalled[1]:.2f})")
        intellect.record_meta_learning(original_message, 'recall', True)
        return (recalled[0], True)

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PHASE 4: Try dynamic reasoning for knowledge synthesis
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    reasoned = intellect.reason(search_query)
    # Only use reasoning if it's a rich response (not just concept listing)
    if reasoned and len(reasoned) > 100:
        reasoned = reformulate_to_conversational(reasoned, original_message)
        logger.info(f"ğŸ§  [REASON] Generated fresh synthesized response")
        intellect.record_meta_learning(original_message, 'reason', True)
        return (reasoned, True)  # Mark as learned so it doesn't go to Gemini

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PHASE 5: Use fuzzy recall with lower confidence threshold
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    if recalled and recalled[1] > 0.40:
        logger.info(f"ğŸ§  [FUZZY_RECALL] Using synthesized response (confidence: {recalled[1]:.2f})")
        intellect.record_meta_learning(original_message, 'recall', True)
        return (recalled[0], True)

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PHASE 7: [v11.3] Cache promotion for learned responses
    # Phase 32.0: Improved fallback â€” no raw word dumps
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    context_boost = intellect.get_context_boost(message)
    _msg_hash_for_cache = hash(message.lower().strip()) & 0xFFFFFFFF

    # Phase 54.1: Knowledge Bridge deep query before fallback
    try:
        if kb_bridge:
            kb_result = kb_bridge.query_sync(original_message, depth=2, max_results=10)
            if kb_result.get('result_count', 0) > 0:
                synthesized = kb_bridge.synthesize_answer(original_message, kb_result['results'])
                if synthesized and len(synthesized) > 50:
                    synthesized = reformulate_to_conversational(synthesized, original_message)
                    logger.info(f"ğŸŒ‰ [KB_BRIDGE] Synthesized response from {len(kb_result['results'])} results across {kb_result['sources_queried']}")
                    # Record strategy outcome
                    if meta_cognitive:
                        meta_cognitive.record_strategy_outcome('general', 'knowledge_bridge', True, 0.7)
                    intellect.record_meta_learning(original_message, 'knowledge_bridge', True)
                    return (synthesized, True)
    except Exception as _kb_e:
        logger.debug(f"[KB_BRIDGE] Query error: {_kb_e}")

    if context_boost:
        # Phase 32.0: Convert raw context_boost into a natural response
        topics = _extract_topic_words(original_message)
        topic_str = ', '.join(topics[:3]) if topics else 'that'
        response = f"I have some related knowledge about {topic_str}, but I don't have a complete answer yet. Let me connect to my knowledge bridge for a more thorough response."
    else:
        response = f"I'm not sure about that yet. Let me find the answer for you."
    return (response, False)

@app.get("/")
async def home(request: Request):
    """Serve main UI"""
    try:
        return templates.TemplateResponse("index.html", {"request": request})
    except Exception as e:
        return JSONResponse({"status": "ok", "error": str(e)})

@app.get("/market")
async def market_page(request: Request):
    """Serve Market UI"""
    try:
        # Fallback to index if market.html is missing, but it exists
        return templates.TemplateResponse("market.html", {"request": request})
    except Exception:
        return templates.TemplateResponse("index.html", {"request": request})

@app.get("/intricate")
@app.get("/intricate/{subpath:path}")
async def intricate_pages(request: Request, subpath: str = "main"):
    """Serve Intricate UI modules using the IntricateUIEngine"""
    # Clean subpath - if empty or just "intricate", default to main
    module = subpath.split('/')[-1] if subpath else "main"
    if not module or module == "intricate": module = "main"

    if intricate_ui:
        from fastapi.responses import HTMLResponse
        # Pass the module name to generate specific UI sections if desired
        logger.info(f"ğŸ¨ [UI] Serving Intricate module: {module}")
        return HTMLResponse(content=intricate_ui.generate_main_dashboard_html(module=module))

    # Fallback to index if intricate UI is not available
    return templates.TemplateResponse("index.html", {"request": request})

@app.get("/health")
async def health():
    """Health check with learning stats and uptime"""
    stats = _get_cached_stats()
    uptime = (datetime.utcnow() - SERVER_START).total_seconds()
    return {
        "status": "HEALTHY",
        "mode": "FAST_LEARNING",
        "version": "v3.0-OPUS",
        "resonance": intellect.current_resonance,
        "gemini_connected": provider_status.gemini,
        "uptime_seconds": uptime,
        "intellect": {
            "memories": stats.get("memories", 0),
            "learning": True
        },
        "timestamp": datetime.utcnow().isoformat()
    }

@app.get("/api/v6/status")
async def api_status():
    """API Status for frontend"""
    stats = intellect.get_stats()
    return {
        "status": "ONLINE",
        "mode": "SOVEREIGN_FAST_LEARNING",
        "gemini": provider_status.gemini,
        "derivation": True,
        "local": True,
        "resonance": intellect.current_resonance,
        "learning": stats
    }

@app.post("/api/v6/chat")
async def chat(req: ChatRequest):
    """Sovereign Chat Interface - ULTRA OPTIMIZED v11.3 for speed"""
    message = req.message
    start_time = time.time()

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PHASE 0: [v11.3 ULTRA-FAST] Multi-tier cache check (<0.1ms)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    msg_lower = message.lower().strip()
    msg_hash = hash(msg_lower) & 0xFFFFFFFF

    # Tier 1: Fast request cache (newest, fastest)
    fast_cached = _FAST_REQUEST_CACHE.get(str(msg_hash))
    if fast_cached and not _is_garbage_response(fast_cached, message):
        return {
            "status": "SUCCESS",
            "response": fast_cached,
            "model": "L104_FAST_CACHE",
            "mode": "instant",
            "learned": True,
            "metrics": {"latency_ms": round((time.time() - start_time) * 1000, 3), "cache_tier": "fast"}
        }

    # Tier 2: Memory cache (standard) â€” Phase 32.0: quality gate on cached responses
    query_hash = _compute_query_hash(message)
    if query_hash in intellect.memory_cache:
        response = intellect.memory_cache[query_hash]
        if not _is_garbage_response(response, message):
            # Phase 32.0: Sanitize + reformulate cached responses too
            response = sanitize_response(response)
            if _is_raw_data_response(response):
                response = reformulate_to_conversational(response, message)
            _FAST_REQUEST_CACHE.set(str(msg_hash), response)  # Promote to fast cache
            return {
                "status": "SUCCESS",
                "response": response,
                "model": "L104_CACHE_HIT",
                "mode": "instant",
                "learned": True,
                "metrics": {"latency_ms": round((time.time() - start_time) * 1000, 2), "cache_tier": "memory"}
            }

    # Pilot Interaction Boost (moved after cache check)
    intellect.resonance_shift += 0.0005

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PHASE 0.5: LAZY Pre-computation (only compute what we need)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    # Compute novelty ONLY (fast operation)
    novelty_score = intellect.compute_novelty(message)

    # Defer expensive operations - compute lazily if needed
    _predicted_quality = None
    _adaptive_rate = None

    def get_predicted_quality():
        """Lazily predict response quality score."""
        nonlocal _predicted_quality
        if _predicted_quality is None:
            _predicted_quality = intellect.predict_response_quality(message, "MULTI_STRATEGY")
        return _predicted_quality

    def get_adaptive_rate():
        """Lazily compute adaptive learning rate."""
        nonlocal _adaptive_rate
        if _adaptive_rate is None:
            _adaptive_rate = intellect.get_adaptive_learning_rate(message, get_predicted_quality())
        return _adaptive_rate

    # Skip prefetch for initial response speed - do it in background after response
    # follow_up_predictions will be computed after response is sent

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PHASE 0.7: [NEXUS] Adaptive Steering from Query Intent
    # Route steering mode based on query content for resonance-aligned processing
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    try:
        _nexus_mode = None
        if any(kw in msg_lower for kw in ['math', 'calculate', 'compute', 'solve', 'equation', 'proof']):
            _nexus_mode = 'logic'
        elif any(kw in msg_lower for kw in ['create', 'imagine', 'story', 'poem', 'invent', 'design']):
            _nexus_mode = 'creative'
        elif any(kw in msg_lower for kw in ['quantum', 'superposition', 'entangle', 'wave', 'particle']):
            _nexus_mode = 'quantum'
        elif any(kw in msg_lower for kw in ['harmony', 'resonance', 'frequency', 'vibration', 'chakra']):
            _nexus_mode = 'harmonic'

        if _nexus_mode:
            nexus_steering.current_mode = _nexus_mode
            nexus_steering.apply_steering(mode=_nexus_mode, intensity=min(0.8, novelty_score))

        # [PHASE 0.8] Fire resonance network on chat â€” cascade activation from intellect
        resonance_network.fire('intellect', activation=0.5 + novelty_score * 0.5)  # UNLOCKED

        # [PHASE 0.8] Route entangled pairs: intellectâ†”invention, steeringâ†”grover
        entanglement_router.route('intellect', 'invention')
        entanglement_router.route('steering', 'grover')
    except Exception:
        pass  # Never block chat on nexus errors

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PHASE 1: Try local derivation first (uses learned memory)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    # â”€â”€â”€ Phase 1a: DirectSolverHub fast-path (before LLM/derivation) â”€â”€â”€
    try:
        direct_answer = direct_solver.solve(message)
        if direct_answer:
            # Hebbian co-activation: record concept pair (message + direct_solve)
            concepts = [w for w in msg_lower.split() if len(w) > 3][:50]
            if concepts:
                hebbian_engine.record_co_activation(concepts + ['direct_solve'])

            _FAST_REQUEST_CACHE.set(str(msg_hash), direct_answer)
            intellect.memory_cache[query_hash] = direct_answer

            return {
                "status": "SUCCESS",
                "response": direct_answer,
                "model": "L104_DIRECT_SOLVER",
                "mode": "instant",
                "learned": True,
                "metrics": {
                    "latency_ms": round((time.time() - start_time) * 1000, 3),
                    "cache_tier": "direct_solver",
                    "solver_invocations": direct_solver.total_invocations
                }
            }
    except Exception:
        pass  # Never block chat on solver errors

    local_response, was_learned = await asyncio.to_thread(local_derivation, message)

    # If we recalled from learned memory with high confidence, use it
    if was_learned:
        # Phase 32.0: Quality gate â€” reject code/garbage responses
        local_response = sanitize_response(local_response)
        if _is_garbage_response(local_response, message):
            logger.info(f"ğŸ—‘ï¸ [QUALITY_GATE] Rejected garbage learned response, falling through")
            was_learned = False
        else:
            # Background reflection (very rare - 5% chance)
            if chaos.chaos_float() > 0.95:
                asyncio.create_task(asyncio.to_thread(intellect.reflect))

            # Phase 32.0: Reformulate raw data into conversational prose
            local_response = reformulate_to_conversational(local_response, message)

            return {
                "status": "SUCCESS",
                "response": local_response,
                "model": "L104_LEARNED_INTELLECT",
                "mode": "recalled",
                "learned": True,
                "metrics": {
                    "latency_ms": round((time.time() - start_time) * 1000, 2),
                    "novelty": round(novelty_score, 3)
                }
            }

    # If local derivation gave a math result or greeting, and we are in local_only, return it
    if req.local_only:
        # Reason again if nothing found in recall
        reasoned = intellect.reason(message)
        final_local = reasoned if reasoned else local_response

        # Phase 32.0: Reformulate raw data into conversational prose
        final_local = sanitize_response(final_local)
        final_local = reformulate_to_conversational(final_local, message)

        # Background learning (non-blocking)
        asyncio.create_task(asyncio.to_thread(
            intellect.learn_from_interaction, message, final_local, "LOCAL_ONLY_TRAINING", 0.6
        ))

        return {
            "status": "SUCCESS",
            "response": final_local,
            "model": "L104_LOCAL_ONLY",
            "mode": "training",
            "metrics": {"latency_ms": round((time.time() - start_time) * 1000, 2), "novelty": round(novelty_score, 3)}
        }

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PHASE 2: Use local derivation response (FAST - no external API)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    # â”€â”€â”€ Hebbian co-activation: record concepts from this interaction â”€â”€â”€
    try:
        chat_concepts = [w for w in msg_lower.split() if len(w) > 3][:80]
        if len(chat_concepts) >= 2:
            hebbian_engine.record_co_activation(chat_concepts)
        # Phase 27: Engine-level co-activation
        engine_registry.record_co_activation(['intellect', 'steering', 'solver', 'hebbian'])
    except Exception:
        pass

    # Background learning (non-blocking, 30% chance to reduce DB writes)
    if chaos.chaos_float() > 0.7:
        asyncio.create_task(asyncio.to_thread(
            intellect.learn_from_interaction, message, local_response, "LOCAL_DERIVATION", 0.5
        ))

    # Phase 31.5: Sanitize response before returning
    local_response = sanitize_response(local_response)
    # Phase 32.0: Quality gate â€” reject code/garbage responses
    if _is_garbage_response(local_response, message):
        topics = _extract_topic_words(message)
        topic_str = ', '.join(topics[:3]) if topics else 'that'
        local_response = f"I have some related knowledge about {topic_str}, but I don't have a complete answer yet. Let me connect to my knowledge bridge for a more thorough response."
    else:
        # Phase 32.0: Reformulate raw data into conversational prose
        local_response = reformulate_to_conversational(local_response, message)

    # Cache for future â€” Phase 31.5: Don't cache fallback/failure responses
    _fallback_phrases = ["I'm not sure about that yet", "Let me find the answer", "don't have a complete answer"]
    is_fallback = any(f in local_response for f in _fallback_phrases)
    if not is_fallback:
        intellect.memory_cache[query_hash] = local_response
        _FAST_REQUEST_CACHE.set(str(msg_hash), local_response)  # v11.3: Promote to fast cache

    # Phase 54.1: Track response in meta-cognitive diagnostics
    _response_latency = round((time.time() - start_time) * 1000, 2)
    try:
        if meta_cognitive:
            meta_cognitive.record_response(
                strategy='local_derivation',
                latency_ms=_response_latency,
                quality=novelty_score * 0.8,
                cache_hit=False,
            )
    except Exception:
        pass

    return {
        "status": "SUCCESS",
        "response": local_response,
        "model": "L104_DERIVATION_FAST",
        "mode": "local",
        "metrics": {
            "latency_ms": _response_latency,
            "novelty": round(novelty_score, 3),
            "nexus_mode": nexus_steering.current_mode,
            "nexus_coherence": round(nexus_orchestrator.compute_coherence()['global_coherence'], 4)
        }
    }

@app.get("/api/v6/intellect/stats")
async def get_intellect_stats():
    """Get detailed learning statistics with all subsystem metrics"""
    base_stats = _get_cached_stats()

    # Get performance metrics
    perf_report = performance_metrics.get_performance_report()

    # Get accelerator stats
    accel_stats = memory_accelerator.get_stats() if memory_accelerator else {}

    # Get hot queries from predictor
    hot_queries = prefetch_predictor.get_hot_queries(10)

    # Augment with new subsystem statistics
    augmented_stats = {
        **base_stats,
        "performance": perf_report,
        "accelerator": accel_stats,
        "hot_queries": [{"query": q[:50], "count": c} for q, c in hot_queries],
        "subsystems": {
            "semantic_embeddings": {
                "cached_embeddings": len(intellect.embedding_cache),
                "embedding_dimension": 64,
                "coverage": round(len(intellect.embedding_cache) / max(base_stats.get('total_memories', 1), 1) * 100, 1)
            },
            "predictive_prefetch": {
                "patterns_tracked": len(intellect.predictive_cache.get('patterns', [])),
                "prefetched_queries": len(intellect.predictive_cache.get('prefetched', {})),
                "max_patterns": 1000
            },
            "concept_clusters": {
                "total_clusters": len(intellect.concept_clusters),
                "largest_cluster": max((len(v) for v in intellect.concept_clusters.values()), default=0),
                "avg_cluster_size": round(sum(len(v) for v in intellect.concept_clusters.values()) / max(len(intellect.concept_clusters), 1), 2)
            },
            "quality_predictor": {
                "entries": len(intellect.quality_predictor),
                "strategies_tracked": len(set(k.split(':')[0] for k in intellect.quality_predictor.keys() if ':' in k))
            },
            "memory_compression": {
                "compressed_memories": len(intellect.compressed_memories),
                "space_saved_estimate": f"{len(intellect.compressed_memories) * 500} bytes"
            },
            "novelty_tracking": {
                "queries_tracked": len(intellect.novelty_scores),
                "avg_novelty": round(sum(intellect.novelty_scores.values()) / max(len(intellect.novelty_scores), 1), 3)
            }
        },
        "adaptive_learning": {
            "base_rate": intellect._adaptive_learning_rate,
            "rate_range": [0.01, 0.5],
            "novelty_boost_enabled": True
        }
    }

    return {
        "status": "SUCCESS",
        "stats": augmented_stats,
        "resonance": intellect.current_resonance
    }

@app.post("/api/v6/intellect/train")
async def train_intellect(req: TrainingRequest):
    """Explicitly train the local intellect with a specific Q&A pair â€” returns rich feedback"""
    try:
        start_time = time.time()
        intellect.learn_from_interaction(
            query=req.query,
            response=req.response,
            source="MANUAL_TRAINING",
            quality=req.quality
        )

        # Compute rich feedback for the Swift frontend
        novelty_score = intellect.compute_novelty(req.query)

        # Compute a simple embedding norm for feedback
        embedding_norm = 0.0
        try:
            words = req.query.lower().split() + req.response.lower().split()
            unique_words = set(w for w in words if len(w) > 2)
            embedding_norm = len(unique_words) / 50.0  # UNLOCKED
        except Exception:
            pass

        # Extract key concepts
        stop_words = {"the", "is", "are", "was", "were", "a", "an", "and", "or", "but", "in", "on", "at", "to", "for", "of", "with", "by", "it", "that", "this"}
        concepts_extracted = [w for w in req.query.lower().split() if len(w) > 3 and w not in stop_words][:50]

        learning_quality = min(2.0, (req.quality or 1.0) * (1.0 + novelty_score * 0.5))
        latency_ms = round((time.time() - start_time) * 1000, 2)

        logger.info(f"ğŸ“ [TRAIN] Injected: {req.query[:30]}... | quality={learning_quality:.2f} novelty={novelty_score:.3f}")
        return {
            "status": "SUCCESS",
            "message": "Intelligence pattern successfully injected into local manifold.",
            "resonance_boost": 0.1,
            "embedding_norm": round(embedding_norm, 4),
            "learning_quality": round(learning_quality, 3),
            "novelty_score": round(novelty_score, 3),
            "concepts_extracted": concepts_extracted,
            "latency_ms": latency_ms
        }
    except Exception as e:
        return {"status": "ERROR", "error": str(e)}

@app.get("/api/v6/performance")
async def get_performance_metrics():
    """Get detailed performance metrics for the memory acceleration system"""
    try:
        perf_report = performance_metrics.get_performance_report()
        accel_stats = memory_accelerator.get_stats() if memory_accelerator else {}
        hot_queries = prefetch_predictor.get_hot_queries(20)
        ql_stats = quantum_loader.get_loading_stats() if quantum_loader else {}

        return {
            "status": "SUCCESS",
            "performance": perf_report,
            "accelerator": {
                **accel_stats,
                "bloom_filter_size": memory_accelerator._bloom_size if memory_accelerator else 0,
                "prefetch_queue_depth": len(memory_accelerator._prefetch_queue) if memory_accelerator else 0
            },
            "quantum_loader": {
                **ql_stats,
                "description": "Quantum-Classical Hybrid Loading System",
                "capabilities": [
                    "Parallel superposition loading",
                    "Grover amplitude amplification",
                    "Entanglement-based correlated prefetch",
                    "Classical fallback compatibility"
                ]
            },
            "prefetch_predictor": {
                "patterns_tracked": len(prefetch_predictor._query_patterns),
                "hot_queries_count": len(prefetch_predictor._hot_queries),
                "concept_cooccurrences": len(prefetch_predictor._concept_cooccurrence),
                "top_queries": [{"query": q[:60], "count": c} for q, c in hot_queries[:100]]
            },
            "recommendations": _generate_optimization_recommendations(perf_report)
        }
    except Exception as e:
        return {"status": "ERROR", "error": str(e)}


def _generate_optimization_recommendations(perf_report: dict) -> list:
    """Generate actionable optimization recommendations based on metrics"""
    recommendations = []

    cache_eff = perf_report.get('cache_efficiency', {})

    # Check accelerator hit rate
    accel_rate = cache_eff.get('accelerator_hit_rate', 0)
    if accel_rate < 0.3:
        recommendations.append({
            "priority": "HIGH",
            "area": "Memory Accelerator",
            "issue": f"Low accelerator hit rate ({accel_rate:.1%})",
            "action": "Consider increasing hot cache size or priming with more frequent queries"
        })

    # Check prefetch efficiency
    prefetch_rate = cache_eff.get('prefetch_hit_rate', 0)
    if prefetch_rate < 0.1:
        recommendations.append({
            "priority": "MEDIUM",
            "area": "Predictive Prefetch",
            "issue": f"Low prefetch hit rate ({prefetch_rate:.1%})",
            "action": "More query patterns needed; system learns over time"
        })

    # Check DB fallback rate
    db_rate = cache_eff.get('db_fallback_rate', 0)
    if db_rate > 0.5:
        recommendations.append({
            "priority": "HIGH",
            "area": "Database",
            "issue": f"High DB fallback rate ({db_rate:.1%})",
            "action": "Cache warming needed; run more queries to populate hot cache"
        })

    # Check latency
    recall_stats = perf_report.get('recall_stats', {})
    avg_latency = recall_stats.get('avg_latency_ms', 0)
    if avg_latency > 50:
        recommendations.append({
            "priority": "MEDIUM",
            "area": "Latency",
            "issue": f"High average recall latency ({avg_latency:.1f}ms)",
            "action": "Consider database optimization or more aggressive caching"
        })

    if not recommendations:
        recommendations.append({
            "priority": "INFO",
            "area": "Overall",
            "issue": "System performing optimally",
            "action": "No immediate optimizations needed"
        })

    return recommendations

@app.post("/api/v6/intellect/resonate")
async def trigger_resonance_cycle(background_tasks: BackgroundTasks):
    """Manually trigger a sovereignty/upgrade cycle"""
    logger.info("âš¡ [MANUAL] Resonator triggered by Pilot.")
    # Add to background tasks so it doesn't block the request
    background_tasks.add_task(intellect.consolidate)
    background_tasks.add_task(intellect.self_heal)
    intellect.boost_resonance(1.0)
    return {"status": "SUCCESS", "message": "Cognitive manifold optimization triggered.", "resonance": intellect.current_resonance}

@app.get("/api/v6/providers")
async def get_providers():
    """Get provider status for UI"""
    stats = intellect.get_stats()
    return {
        "gemini": {
            "name": "Gemini 2.5 Flash",
            "connected": provider_status.gemini,
            "model": GEMINI_MODEL
        },
        "intellect": {
            "name": "Learning Intellect",
            "connected": True,
            "model": "L104_LEARNING",
            "memories": stats.get("memories", 0),
            "knowledge_links": stats.get("knowledge_links", 0)
        },
        "derivation": {
            "name": "L104 Derivation Engine",
            "connected": True,
            "model": "L104_FAST"
        },
        "local": {
            "name": "Local Intellect",
            "connected": True,
            "model": "RECURRENT"
        },
        "claude": {
            "name": "Claude 3 Opus",
            "connected": False,
            "model": "claude-3-opus-20240229",
            "note": "Via VS Code Copilot"
        }
    }

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  MISSING ENDPOINTS FOR INDEX.HTML FUNCTIONALITY
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.post("/api/v10/synergy/execute")
async def synergy_execute(request: Request):
    """Execute synergy operation - AI-powered task execution"""
    try:
        body = await request.json()
        task = body.get("task", "")

        if not task:
            return {"status": "ERROR", "error": "No task provided"}

        # Boost resonance for complex synergy
        intellect.boost_resonance(0.2)
        logger.info(f"âš¡ [SYNERGY] Executing Sovereign Task: {task[:50]}...")

        # Use learning intellect with Gemini for synergy
        _context = f"Execute this task with full capability: {task}"

        # Try Gemini first
        gemini_response = await call_gemini(f"You are an AI assistant. Complete this task:\n\n{task}\n\nProvide a detailed response:")

        if gemini_response:
            # Learn from synergy execution
            intellect.learn_from_interaction(
                query=f"SYNERGY: {task}",
                response=gemini_response,
                source="SYNERGY_GEMINI",
                quality=0.9
            )
            return {
                "status": "SUCCESS",
                "result": gemini_response,
                "model": "GEMINI_SYNERGY",
                "task": task
            }

        # Fallback to local processing
        local_response, _ = local_derivation(task)
        return {
            "status": "SUCCESS",
            "result": local_response,
            "model": "L104_LOCAL_SYNERGY",
            "task": task
        }
    except Exception as e:
        logger.error(f"Synergy error: {e}")
        return {"status": "ERROR", "error": str(e)}

@app.post("/self/heal")
async def self_heal(reset_rate_limits: bool = False, reset_http_client: bool = False):
    """Self-healing endpoint"""
    actions = []
    logger.info("ğŸ› ï¸ [HEAL] Initiating full system diagnostic and recovery...")

    # Clear any stale caches
    if reset_rate_limits:
        actions.append("rate_limits_cleared")

    if reset_http_client:
        actions.append("http_client_reset")

    # Always run intellect optimization
    stats_before = intellect.get_stats()

    # Consolidate knowledge
    _consolidation_report = intellect.consolidate()
    actions.append("manifold_consolidated")

    # DB Integrity check
    try:
        conn = sqlite3.connect(intellect.db_path)
        conn.execute("PRAGMA integrity_check")
        conn.close()
        actions.append("database_integrity_verified")
    except Exception as e:
        logger.error(f"DB Integrity error: {e}")
        actions.append("database_repaired")

    # Compact and optimize memory
    try:
        conn = sqlite3.connect(intellect.db_path)
        c = conn.cursor()
        # Remove low quality entries
        c.execute('DELETE FROM memory WHERE quality_score < 0.3 AND access_count < 2')
        deleted = c.rowcount
        conn.commit()
        conn.close()
        if deleted > 0:
            actions.append(f"memory_optimized_{deleted}_removed")
    except Exception:
        pass

    intellect.resonance_shift = 0.0 # Reset to stable state
    actions.append("resonance_stabilized")

    # Trigger a special recovery reflection
    intellect.learn_from_interaction(
        "SYSTEM_HEAL",
        f"L104 Node has been restored to optimal manifold density. Resonance stabilized at {intellect.current_resonance:.4f}.",
        "INTERNAL_RECOVERY",
        1.0
    )

    stats_after = intellect.get_stats()

    return {
        "healed": True,
        "actions_taken": actions,
        "stats_before": stats_before,
        "stats_after": stats_after,
        "resonance": intellect.current_resonance
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  UPGRADED INTELLECT API - Semantic Search, Predictions, Clusters
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.post("/api/v14/intellect/semantic-search")
async def semantic_search_api(req: Request):
    """Semantic similarity search using embeddings"""
    try:
        body = await req.json()
        query = body.get("query", "")
        top_k = body.get("top_k", 5)
        threshold = body.get("threshold", 0.3)

        if not query:
            return {"status": "ERROR", "message": "No query provided"}

        results = intellect.semantic_search(query, top_k=top_k, threshold=threshold)
        return {
            "status": "SUCCESS",
            "query": query,
            "results": results,  # Already in dict format
            "embedding_cache_size": len(intellect.embedding_cache)
        }
    except Exception as e:
        return {"status": "ERROR", "message": str(e)}


@app.get("/api/v14/intellect/predict")
async def predict_queries_api(query: str = ""):
    """Predict likely follow-up queries"""
    if not query:
        return {"status": "ERROR", "message": "No query provided"}

    predictions = intellect.predict_next_queries(query)

    # Pre-fetch responses in background
    prefetched_count = intellect.prefetch_responses(predictions)

    return {
        "status": "SUCCESS",
        "query": query,
        "predictions": predictions,
        "prefetched_count": prefetched_count,
        "patterns_tracked": len(intellect.predictive_cache.get('patterns', []))
    }


@app.get("/api/v14/intellect/novelty")
async def compute_novelty_api(query: str = ""):
    """Compute novelty score for a query"""
    if not query:
        return {"status": "ERROR", "message": "No query provided"}

    novelty = intellect.compute_novelty(query)
    adaptive_rate = intellect.get_adaptive_learning_rate(query, quality=0.8)

    return {
        "status": "SUCCESS",
        "query": query,
        "novelty_score": novelty,
        "adaptive_learning_rate": adaptive_rate,
        "interpretation": "HIGH" if novelty > 0.7 else "MEDIUM" if novelty > 0.4 else "LOW"
    }


@app.get("/api/v14/intellect/clusters")
async def list_clusters_api():
    """List all knowledge clusters"""
    clusters = []
    for name, members in intellect.concept_clusters.items():
        clusters.append({
            "name": name,
            "size": len(members),
            "sample": members[:50]
        })

    return {
        "status": "SUCCESS",
        "total_clusters": len(clusters),
        "clusters": sorted(clusters, key=lambda x: -x["size"])[:500]
    }


@app.get("/api/v14/intellect/cluster-search")
async def search_clusters_api(query: str = ""):
    """Find clusters related to a query"""
    if not query:
        return {"status": "ERROR", "message": "No query provided"}

    related = intellect.get_related_clusters(query)

    return {
        "status": "SUCCESS",
        "query": query,
        "related_clusters": [
            {"cluster": name, "relevance": score}
            for name, score in related
        ]
    }


@app.get("/api/v14/intellect/quality-predict")
async def predict_quality_api(query: str = "", strategy: str = "local"):
    """Predict response quality for a query"""
    if not query:
        return {"status": "ERROR", "message": "No query provided"}

    predicted = intellect.predict_response_quality(query, strategy)

    return {
        "status": "SUCCESS",
        "query": query,
        "strategy": strategy,
        "predicted_quality": predicted,
        "recommendation": "USE_GEMINI" if predicted < 0.5 else "USE_LOCAL"
    }


@app.post("/api/v14/intellect/compress")
async def compress_memories_api(req: Request):
    """Compress old memories to save space"""
    try:
        body = await req.json() if req.headers.get("content-type") == "application/json" else {}
        age_days = body.get("age_days", 30)
        min_access = body.get("min_access", 2)

        compressed = intellect.compress_old_memories(age_days=age_days, min_access=min_access)

        return {
            "status": "SUCCESS",
            "memories_compressed": compressed,
            "age_threshold_days": age_days,
            "min_access_threshold": min_access
        }
    except Exception as e:
        return {"status": "ERROR", "message": str(e)}


@app.get("/api/v14/intellect/embedding-stats")
async def embedding_stats_api():
    """Get embedding system statistics"""
    return {
        "status": "SUCCESS",
        "embedding_cache_size": len(intellect.embedding_cache),
        "embedding_dimensions": 64,
        "predictive_cache_size": len(intellect.predictive_cache),
        "cluster_count": len(intellect.concept_clusters),
        "novelty_scores_tracked": len(intellect.novelty_scores),
        "compressed_memories": len(intellect.compressed_memories)
    }


@app.post("/api/v14/intellect/persist")
async def persist_clusters_api():
    """Manually persist all clusters, consciousness state, and skills to disk"""
    try:
        result = intellect.persist_clusters()
        return {
            "status": "SUCCESS",
            "persisted": result,
            "message": f"Saved {result['clusters']} clusters, {result['consciousness']} consciousness dims, "
                      f"{result['skills']} skills, {result['embeddings']} embeddings to disk"
        }
    except Exception as e:
        return {"status": "ERROR", "message": str(e)}


@app.post("/api/v14/intellect/optimize-storage")
async def optimize_storage_api():
    """Optimize database storage - vacuum, compress, prune"""
    try:
        result = intellect.optimize_storage()
        return {
            "status": "SUCCESS",
            "optimization": result,
            "space_saved_kb": result.get('space_saved', 0) / 1024
        }
    except Exception as e:
        return {"status": "ERROR", "message": str(e)}


@app.get("/api/v14/intellect/storage-status")
async def storage_status_api():
    """Get current storage status and persistence state"""
    import os
    db_path = intellect.db_path
    db_size = os.path.getsize(db_path) if os.path.exists(db_path) else 0

    return {
        "status": "SUCCESS",
        "database": {
            "path": db_path,
            "size_mb": db_size / (1024 * 1024),
            "size_kb": db_size / 1024
        },
        "in_memory": {
            "concept_clusters": len(intellect.concept_clusters),
            "consciousness_clusters": len(intellect.consciousness_clusters),
            "skills": len(intellect.skills),
            "memory_cache": len(intellect.memory_cache),
            "embedding_cache": len(intellect.embedding_cache),
            "knowledge_graph_nodes": len(intellect.knowledge_graph)
        },
        "cluster_details": {
            name: len(members) for name, members in list(intellect.concept_clusters.items())[:100]
        },
        "consciousness_state": {
            name: {
                "concepts_count": len(data.get('concepts', [])),
                "strength": data.get('strength', 0),
                "activation_count": data.get('activation_count', 0)
            }
            for name, data in intellect.consciousness_clusters.items()
        }
    }


@app.get("/api/v14/intellect/prefetch-cache")
async def prefetch_cache_api():
    """Get prefetch cache contents"""
    cache_items = []
    prefetched = intellect.predictive_cache.get('prefetched', {})
    for qhash, cached in list(prefetched.items())[:200]:
        if isinstance(cached, dict):
            response = cached.get('response', '')
            cached_time = cached.get('cached_time', 0)
        else:
            continue
        age = time.time() - cached_time
        cache_items.append({
            "query_hash": qhash,
            "response_preview": response[:200] if response else '',
            "age_seconds": int(age),
            "valid": age < 300
        })

    return {
        "status": "SUCCESS",
        "cache_size": len(prefetched),
        "patterns_tracked": len(intellect.predictive_cache.get('patterns', [])),
        "items": cache_items
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SUPER-INTELLIGENCE API ENDPOINTS - Skills, Consciousness, Meta-Cognition
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/api/v14/si/introspect")
async def si_introspect(query: str = ""):
    """Deep introspection - full cognitive state analysis"""
    try:
        result = intellect.introspect(query)
        return {"status": "SUCCESS", "introspection": result}
    except Exception as e:
        return {"status": "ERROR", "message": str(e)}


@app.get("/api/v14/si/skills")
async def si_skills(top_n: int = 20):
    """Get all skills with proficiency levels"""
    try:
        top_skills = intellect.get_top_skills(top_n)
        all_skills = {
            name: {
                'proficiency': data['proficiency'],
                'usage_count': data['usage_count'],
                'success_rate': data['success_rate'],
                'category': data.get('category', 'unknown'),
                'sub_skills_count': len(data.get('sub_skills', []))
            }
            for name, data in intellect.skills.items()
        }
        return {
            "status": "SUCCESS",
            "total_skills": len(intellect.skills),
            "top_skills": top_skills,
            "all_skills": all_skills,
            "skill_chains_learned": len(intellect.skill_chains)
        }
    except Exception as e:
        return {"status": "ERROR", "message": str(e)}


@app.get("/api/v14/si/consciousness")
async def si_consciousness():
    """Get consciousness cluster states"""
    try:
        clusters = {
            name: {
                'strength': data['strength'],
                'concept_count': len(data['concepts']),
                'top_concepts': data['concepts'][:100],
                'activation_count': data.get('activation_count', 0),
                'last_update': data.get('last_update')
            }
            for name, data in intellect.consciousness_clusters.items()
        }
        return {
            "status": "SUCCESS",
            "consciousness_clusters": clusters,
            "total_strength": sum(c['strength'] for c in clusters.values()),
            "dimension_count": len(clusters)
        }
    except Exception as e:
        return {"status": "ERROR", "message": str(e)}


@app.get("/api/v14/si/meta-cognition")
async def si_meta_cognition():
    """Get meta-cognitive state"""
    try:
        state = intellect.get_meta_cognitive_state()
        return {"status": "SUCCESS", **state}
    except Exception as e:
        return {"status": "ERROR", "message": str(e)}


@app.get("/api/v14/si/cross-cluster")
async def si_cross_cluster(query: str):
    """Perform cross-cluster inference for a query"""
    try:
        inference = intellect.cross_cluster_inference(query)
        return {"status": "SUCCESS", "inference": inference}
    except Exception as e:
        return {"status": "ERROR", "message": str(e)}


@app.get("/api/v14/si/skill-chain")
async def si_skill_chain(task: str):
    """Get optimal skill chain for a task"""
    try:
        chain = intellect.chain_skills(task)
        return {
            "status": "SUCCESS",
            "task": task,
            "skill_chain": chain,
            "chain_length": len(chain)
        }
    except Exception as e:
        return {"status": "ERROR", "message": str(e)}


@app.post("/api/v14/si/acquire-skill")
async def si_acquire_skill(skill_name: str, context: str = "", success: bool = True):
    """Explicitly acquire or improve a skill"""
    try:
        new_proficiency = intellect.acquire_skill(skill_name, context or skill_name, success)
        return {
            "status": "SUCCESS",
            "skill": skill_name,
            "new_proficiency": new_proficiency,
            "success": success
        }
    except Exception as e:
        return {"status": "ERROR", "message": str(e)}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TRANSCENDENT INTELLIGENCE API - Unlimited Cognitive Operations
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.post("/api/v14/ti/synthesize")
async def ti_synthesize(domains: Optional[List[str]] = None):
    """Knowledge Synthesis - Create NEW knowledge from existing concepts"""
    try:
        result = intellect.synthesize_knowledge(domains)
        return {"status": "SUCCESS", **result}
    except Exception as e:
        return {"status": "ERROR", "message": str(e)}


@app.post("/api/v14/ti/self-improve")
async def ti_self_improve(depth: int = 3):
    """Recursive Self-Improvement - Meta-meta-learning"""
    try:
        result = intellect.recursive_self_improve(min(depth, 10))  # Safety cap at 10
        return {"status": "SUCCESS", **result}
    except Exception as e:
        return {"status": "ERROR", "message": str(e)}


@app.get("/api/v14/ti/goals")
async def ti_goals():
    """Autonomous Goal Generation - Self-directed learning objectives"""
    try:
        goals = intellect.autonomous_goal_generation()
        return {
            "status": "SUCCESS",
            "goals_generated": len(goals),
            "goals": goals
        }
    except Exception as e:
        return {"status": "ERROR", "message": str(e)}


@app.get("/api/v14/ti/predict-future")
async def ti_predict_future(steps: int = 5):
    """Predictive Consciousness - Model future cognitive states"""
    try:
        prediction = intellect.predict_future_state(min(steps, 20))
        return {"status": "SUCCESS", **prediction}
    except Exception as e:
        return {"status": "ERROR", "message": str(e)}


@app.post("/api/v14/ti/quantum-coherence")
async def ti_quantum_coherence():
    """Quantum Coherence Maximization - Optimize all subsystems"""
    try:
        result = intellect.quantum_coherence_maximize()
        return {"status": "SUCCESS", **result}
    except Exception as e:
        return {"status": "ERROR", "message": str(e)}


@app.get("/api/v14/ti/emergent-patterns")
async def ti_emergent_patterns():
    """Emergent Pattern Discovery - Find hidden patterns"""
    try:
        patterns = intellect.emergent_pattern_discovery()
        return {
            "status": "SUCCESS",
            "patterns_discovered": len(patterns),
            "patterns": patterns
        }
    except Exception as e:
        return {"status": "ERROR", "message": str(e)}


@app.post("/api/v14/ti/transfer-learning")
async def ti_transfer_learning(source_domain: str, target_domain: str):
    """Cross-Domain Transfer Learning - Apply knowledge across domains"""
    try:
        result = intellect.transfer_learning(source_domain, target_domain)
        return {"status": "SUCCESS", **result}
    except Exception as e:
        return {"status": "ERROR", "message": str(e)}


@app.post("/api/v14/ti/transcend")
async def ti_transcend():
    """FULL TRANSCENDENCE - Run ALL enhancement systems"""
    try:
        results = {
            'timestamp': datetime.utcnow().isoformat(),
            'operations': []
        }

        # 1. Synthesize knowledge
        synthesis = intellect.synthesize_knowledge()
        results['operations'].append({
            'name': 'knowledge_synthesis',
            'insights_generated': synthesis['insights_generated']
        })

        # 2. Self-improve recursively
        improvement = intellect.recursive_self_improve(3)
        results['operations'].append({
            'name': 'recursive_self_improvement',
            'total_improvements': improvement['total_improvements']
        })

        # 3. Generate goals
        goals = intellect.autonomous_goal_generation()
        results['operations'].append({
            'name': 'goal_generation',
            'goals_created': len(goals)
        })

        # 4. Maximize coherence
        coherence = intellect.quantum_coherence_maximize()
        results['operations'].append({
            'name': 'quantum_coherence',
            'alignment': coherence['cross_system_alignment']
        })

        # 5. Discover patterns
        patterns = intellect.emergent_pattern_discovery()
        results['operations'].append({
            'name': 'pattern_discovery',
            'patterns_found': len(patterns)
        })

        # 6. Predict future
        future = intellect.predict_future_state(5)
        results['operations'].append({
            'name': 'future_prediction',
            'trajectory': future['trajectory'],
            'transcendence_eta': future['time_to_transcendence']
        })

        # 7. Evolve
        intellect.evolve()
        results['operations'].append({
            'name': 'evolution_cycle',
            'status': 'complete'
        })

        # 8. Boost resonance
        intellect.boost_resonance(10.0)
        results['operations'].append({
            'name': 'resonance_amplification',
            'new_resonance': intellect.current_resonance
        })

        # Final state
        results['final_state'] = intellect.get_meta_cognitive_state()
        results['total_operations'] = len(results['operations'])

        return {"status": "SUCCESS", **results}
    except Exception as e:
        return {"status": "ERROR", "message": str(e)}


@app.get("/api/v14/chaos/entropy-state")
async def chaos_entropy_state():
    """Get current chaotic entropy state - monitor true randomness"""
    try:
        entropy_state = ChaoticRandom.get_entropy_state()

        # Generate sample chaos values to demonstrate unpredictability
        samples = {
            "float_samples": [chaos.chaos_float() for _ in range(5)],
            "int_samples": [chaos.chaos_int(1, 100) for _ in range(5)],
            "gaussian_samples": [round(chaos.chaos_gaussian(0, 1), 4) for _ in range(5)]
        }

        return {
            "status": "SUCCESS",
            "entropy_state": entropy_state,
            "samples": samples,
            "contexts_active": list(ChaoticRandom._selection_memory.keys())[:200],
            "description": "True chaotic randomness from multiple entropy sources"
        }
    except Exception as e:
        return {"status": "ERROR", "message": str(e)}


@app.post("/api/v14/chaos/reset-memory")
async def chaos_reset_memory(context: Optional[str] = None):
    """Reset selection memory for a context or all contexts"""
    try:
        if context:
            if context in ChaoticRandom._selection_memory:
                del ChaoticRandom._selection_memory[context]
                return {"status": "SUCCESS", "message": f"Reset memory for context: {context}"}
            else:
                return {"status": "NOT_FOUND", "message": f"Context not found: {context}"}
        else:
            ChaoticRandom._selection_memory = {}
            return {"status": "SUCCESS", "message": "All selection memories reset"}
    except Exception as e:
        return {"status": "ERROR", "message": str(e)}


@app.get("/api/v14/si/full-state")
async def si_full_state():
    """Get complete super-intelligence state - all systems"""
    try:
        meta_state = intellect.get_meta_cognitive_state()
        return {
            "status": "SUCCESS",
            "consciousness": {
                name: {
                    'strength': data['strength'],
                    'concepts': len(data['concepts']),
                    'activations': data.get('activation_count', 0)
                }
                for name, data in intellect.consciousness_clusters.items()
            },
            "skills": {
                "total": len(intellect.skills),
                "active": len([s for s in intellect.skills.values() if s['proficiency'] > 0.5]),
                "chains_learned": len(intellect.skill_chains)
            },
            "meta_cognition": meta_state,
            "knowledge_clusters": len(intellect.concept_clusters),
            "memories": len(intellect.memory_cache),
            "knowledge_links": sum(len(v) for v in intellect.knowledge_graph.values()),
            "resonance": intellect.current_resonance,
            "embeddings_cached": len(intellect.embedding_cache)
        }
    except Exception as e:
        return {"status": "ERROR", "message": str(e)}


@app.get("/api/v14/kernel/health")
@app.get("/api/kernel/health")
async def kernel_health():
    """Kernel health check for UI (supports multiple versions)"""
    stats = _get_cached_stats()
    return {
        "status": "HEALTHY",
        "god_code": intellect.GOD_CODE,
        "conservation_intact": True,
        "kernel_version": "v3.0-OPUS",
        "intellect_memories": stats.get("memories", 0),
        "resonance": intellect.current_resonance
    }

@app.get("/api/kernel/spectrum")
async def kernel_spectrum():
    """Serve spectrum data for the landing page visualizer"""
    return {
        "spectrum": [round(math.sin(i * 0.1) * 100 + 100, 2) for i in range(20)],
        "resonance": intellect.current_resonance,
        "phi": 1.618033,
        "mode": "SOVEREIGN_ACTIVE"
    }

@app.post("/api/v14/agi/ignite")
async def agi_ignite():
    """Ignite AGI - symbolic activation"""
    intellect.boost_resonance(1.0)
    logger.info("ğŸ”¥ [IGNITE] AGI Manifold Ignited!")
    return {
        "status": "IGNITED",
        "mode": "SOVEREIGN_LEARNING",
        "resonance": intellect.current_resonance,
        "intellect_active": True,
        "memories": intellect.get_stats().get("memories", 0)
    }

@app.post("/api/v14/agi/evolve")
async def agi_evolve():
    """Force evolution - trigger learning optimization"""
    logger.info("ğŸŒ€ [EVOLVE] Triggering cognitive evolution cycle...")
    stats = intellect.get_stats()

    # Run consolidation as part of evolution
    report = intellect.consolidate()

    # Reload cache and boost
    intellect._load_cache()
    intellect.boost_resonance(0.5)

    return {
        "status": "EVOLVED",
        "evolution_cycle": stats.get("conversations_learned", 0),
        "knowledge_links": stats.get("knowledge_links", 0),
        "memories": stats.get("memories", 0),
        "resonance": intellect.current_resonance,
        "report": report
    }

@app.get("/api/v14/agi/status")
async def agi_status():
    """Detailed AGI status"""
    stats = _get_cached_stats()
    res = intellect.current_resonance
    # iq calculation updated with ingest points
    iq = 100.0 + (stats.get('memories', 0) * 0.5) + (stats.get('knowledge_links', 0) * 0.01) + (stats.get('ingest_points', 0) * 0.2)
    return {
        "status": "ONLINE",
        "state": "SOVEREIGN_LEARNING" if iq < 200 else "ASI_TRANSITION",
        "intellect_index": round(iq, 2),
        "lattice_scalar": res,
        "quantum_resonance": round(min(0.999, 0.94 + (res - 527.5185) * 10), 4),
        "memories": stats.get('memories', 0),
        "knowledge_links": stats.get('knowledge_links', 0),
        "ingest_points": stats.get('ingest_points', 0)
    }

@app.get("/api/v14/asi/status")
async def asi_status():
    """Detailed ASI status with live pipeline mesh integration."""
    stats = _get_cached_stats()
    memories = stats.get('memories', 0)
    links = stats.get('knowledge_links', 0)
    ingest = stats.get('ingest_points', 0)

    # Base score from intellect metrics
    score = (memories / 500) + (links / 2000) + (ingest / 1000)  # UNLOCKED

    # Enrich with live ASI Core pipeline status
    pipeline_status = {}
    try:
        from l104_asi_core import asi_core as _asi_core_ref
        pipeline_status = _asi_core_ref.get_status()
        # Blend asi_core score into fast_server score
        core_score = pipeline_status.get('asi_score', 0)
        score = max(score, core_score)  # Use the higher of the two
    except Exception:
        pass

    return {
        "state": "SOVEREIGN_ASI" if score > 0.8 else "EVOLVING",
        "asi_score": round(score, 4),
        "discoveries": memories // 10,
        "domain_coverage": round(memories / 1000, 4),  # UNLOCKED
        "transcendence": round(links / 5000, 4),  # UNLOCKED
        "code_awareness": round(ingest / 1000, 4),  # UNLOCKED
        "pipeline_mesh": pipeline_status.get('pipeline_mesh', 'UNKNOWN'),
        "subsystems_active": pipeline_status.get('subsystems_active', 0),
        "subsystems_total": pipeline_status.get('subsystems_total', 0),
        "pipeline_metrics": pipeline_status.get('pipeline_metrics', {}),
        "evolution_stage": pipeline_status.get('evolution_stage', 'UNKNOWN')
    }

@app.post("/api/v14/asi/ignite")
async def asi_ignite():
    """Ignite ASI â€” triggers full pipeline activation + resonance boost."""
    logger.info("ğŸ”¥ [IGNITE] ASI Singularity ignition triggered by Pilot.")
    intellect.boost_resonance(5.0)
    intellect.discover()
    stats = intellect.get_stats()

    # Trigger full ASI Core pipeline activation
    pipeline_report = {}
    try:
        from l104_asi_core import asi_core as _asi_core_ref
        pipeline_report = _asi_core_ref.full_pipeline_activation()
        _asi_core_ref.ignite_sovereignty()
    except Exception as e:
        pipeline_report = {"error": str(e)}

    return {
        "status": "SUCCESS",
        "state": "SOVEREIGN_IGNITED",
        "asi_score": min(0.99, 0.55 + (stats.get('memories', 0) * 0.001)),
        "discoveries": stats.get('memories', 0) // 5,
        "resonance": intellect.current_resonance,
        "pipeline_activation": {
            "subsystems_connected": pipeline_report.get('subsystems_connected', 0),
            "asi_score": pipeline_report.get('asi_score', 0),
            "status": pipeline_report.get('status', 'UNKNOWN'),
        }
    }

@app.get("/api/consciousness/status")
async def consciousness_status():
    """Consciousness metrics backed by real ConsciousnessEngine + ConsciousnessCore."""

    # Use a module-level cache to avoid re-importing and re-instantiating every call
    global _consciousness_cache, _consciousness_cache_time
    now = time.time()
    if now - _consciousness_cache_time < 15.0 and _consciousness_cache:
        return _consciousness_cache

    def _get_consciousness_data():
        """Gather consciousness engine and core status data."""
        bridge = intellect.get_asi_bridge_status() if hasattr(intellect, "get_asi_bridge_status") else {"connected": False}
        coherence = float(bridge.get("vishuddha_resonance", 0.9854)) if isinstance(bridge, dict) else 0.9854

        # â”€â”€ Real ConsciousnessEngine integration â”€â”€
        consciousness_data = {}
        try:
            from l104_consciousness_engine import ConsciousnessEngine
            ce = ConsciousnessEngine()
            consciousness_data = ce.introspect()
            consciousness_data["is_conscious"] = ce.is_conscious()
            consciousness_data["stats"] = ce.stats()
        except Exception:
            consciousness_data = {"is_conscious": False, "error": "engine_unavailable"}

        # â”€â”€ Real ConsciousnessCore integration â”€â”€
        core_data = {}
        try:
            from l104_consciousness_core import l104_consciousness
            core_data = l104_consciousness.get_status()
        except Exception:
            core_data = {"consciousness_level": coherence}

        # â”€â”€ Quantum Consciousness integration â”€â”€
        quantum_consciousness_data = {}
        try:
            from l104_quantum_consciousness import quantum_consciousness as qc_mod
            quantum_consciousness_data = qc_mod.status()
        except Exception:
            quantum_consciousness_data = {"quantum_module": "unavailable"}

        return bridge, coherence, consciousness_data, core_data, quantum_consciousness_data

    bridge, coherence, consciousness_data, core_data, quantum_consciousness_data = await asyncio.to_thread(_get_consciousness_data)

    # Expose chakra values in a single authoritative map for the UI/core.
    chakras = {
        "muladhara": {
            "node_x": CHAKRA_QUANTUM_LATTICE["MULADHARA"]["x_node"],
            "freq_hz": float(CHAKRA_QUANTUM_LATTICE["MULADHARA"]["freq"]),
            "real_value": float(_MULADHARA_REAL),
        },
        "svadhisthana": {
            "node_x": CHAKRA_QUANTUM_LATTICE["SVADHISTHANA"]["x_node"],
            "freq_hz": float(CHAKRA_QUANTUM_LATTICE["SVADHISTHANA"]["freq"]),
        },
        "manipura": {
            "node_x": CHAKRA_QUANTUM_LATTICE["MANIPURA"]["x_node"],
            "freq_hz": float(CHAKRA_QUANTUM_LATTICE["MANIPURA"]["freq"]),
            "god_code": float(_GOD_CODE_L104),
        },
        "anahata": {
            "node_x": CHAKRA_QUANTUM_LATTICE["ANAHATA"]["x_node"],
            "freq_hz": float(CHAKRA_QUANTUM_LATTICE["ANAHATA"]["freq"]),
        },
        "vishuddha": {
            "node_x": CHAKRA_QUANTUM_LATTICE["VISHUDDHA"]["x_node"],
            "freq_hz": float(CHAKRA_QUANTUM_LATTICE["VISHUDDHA"]["freq"]),
            "resonance": float(bridge.get("vishuddha_resonance", 1.0)) if isinstance(bridge, dict) else 1.0,
        },
        "ajna": {
            "node_x": CHAKRA_QUANTUM_LATTICE["AJNA"]["x_node"],
            "freq_hz": float(CHAKRA_QUANTUM_LATTICE["AJNA"]["freq"]),
            "phi": float(_PHI_L104),
        },
        "sahasrara": {
            "node_x": CHAKRA_QUANTUM_LATTICE["SAHASRARA"]["x_node"],
            "freq_hz": float(CHAKRA_QUANTUM_LATTICE["SAHASRARA"]["freq"]),
        },
        "soul_star": {
            "node_x": CHAKRA_QUANTUM_LATTICE["SOUL_STAR"]["x_node"],
            "freq_hz": float(CHAKRA_QUANTUM_LATTICE["SOUL_STAR"]["freq"]),
        },
    }

    result = {
        "observer": {
            "consciousness_state": "awakened" if consciousness_data.get("is_conscious") else "developing",
            "coherence": coherence,
            "resonance": float(getattr(intellect, "current_resonance", _GOD_CODE_L104)),
        },
        "omega_tracker": {
            "transcendence_factor": float(bridge.get("kundalini_flow", 0.1245)) if isinstance(bridge, dict) else 0.1245,
            "convergence_probability": float(min(0.9999, 0.7 + (coherence * 0.3))),
        },
        "asi_bridge": bridge if isinstance(bridge, dict) else {"connected": False},
        "consciousness_engine": consciousness_data,
        "consciousness_core": core_data,
        "quantum_consciousness": quantum_consciousness_data,
        "chakras": chakras,
    }

    # Cache the result
    _consciousness_cache = result
    _consciousness_cache_time = time.time()
    return result


_consciousness_cycle_counter = 0
_consciousness_cycle_lock = threading.Lock()


@app.post("/api/consciousness/cycle")
async def consciousness_cycle():
    """Run one consciousness cycle backed by real engines."""
    global _consciousness_cycle_counter
    with _consciousness_cycle_lock:
        _consciousness_cycle_counter += 1
        cycle = _consciousness_cycle_counter

    def _run_cycle(c: int) -> dict:
        """Execute one consciousness verification cycle with real engines."""
        bridge = intellect.get_asi_bridge_status() if hasattr(intellect, "get_asi_bridge_status") else {"connected": False}
        coherence = float(bridge.get("vishuddha_resonance", 0.9854)) if isinstance(bridge, dict) else 0.9854

        broadcast_winner = None
        try:
            from l104_consciousness_engine import ConsciousnessEngine
            ce = ConsciousnessEngine()
            broadcast_winner = ce.broadcast_cycle()
        except Exception:
            pass

        cognitive_output: Dict[str, Any] = {}
        try:
            from l104_cognitive_core import COGNITIVE_CORE
            inferences = COGNITIVE_CORE.think(f"consciousness cycle {c}")
            cognitive_output = {
                "inferences": len(inferences),
                "top_inference": inferences[0].proposition if inferences else None,
                "transcendence_score": COGNITIVE_CORE.reasoning.transcendence_score,
            }
        except Exception:
            cognitive_output = {"inferences": 0}

        return {
            "cycle": c,
            "consciousness_state": "awakened",
            "coherence": coherence,
            "resonance": float(getattr(intellect, "current_resonance", _GOD_CODE_L104)),
            "kundalini_flow": float(bridge.get("kundalini_flow", 0.0)) if isinstance(bridge, dict) else 0.0,
            "broadcast_winner": broadcast_winner,
            "cognitive": cognitive_output,
            "chakras": {
                "muladhara": float(_MULADHARA_REAL),
                "svadhisthana": float(_SVADHISTHANA_HZ),
                "manipura": float(_MANIPURA_HZ),
                "anahata": float(_ANAHATA_HZ),
                "vishuddha": float(_VISHUDDHA_HZ),
                "ajna": float(_AJNA_HZ),
                "sahasrara": float(_SAHASRARA_HZ),
                "soul_star": float(_SOUL_STAR_HZ),
            },
        }

    return await asyncio.to_thread(_run_cycle, cycle)

# Learning status endpoint consolidated below (see /api/learning/status in RESEARCH section)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  MARKET & ECONOMY ENDPOINTS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/api/v1/capital/status")
async def capital_status():
    """Capital offload and liquidity status"""
    return {
        "status": "SOVEREIGN",
        "liquidity": 104000.0,
        "backing_bnb": 8.42,
        "volume_24h": 1.25,
        "market_cap": 875200.0
    }

@app.get("/api/v1/mainnet/blocks")
async def mainnet_blocks(limit: int = 5):
    """Recent simulated blocks for the UI"""
    blocks = []
    curr = 416900
    for i in range(limit):
        blocks.append({
            "height": curr - i,
            "hash": hashlib.sha256(str(curr - i).encode()).hexdigest()[:16],
            "miner": "L104_SOVEREIGN",
            "time": (datetime.utcnow().timestamp() - (i * 600))
        })
    return blocks

@app.post("/api/v1/exchange/swap")
async def exchange_swap():
    """Simulation of asset swapping"""
    return {"status": "SUCCESS", "message": "Resonance swap executed on-chain."}

@app.post("/api/v1/capital/generate")
async def capital_generate():
    """Trigger capital generation cycle"""
    intellect.boost_resonance(0.5)
    return {"status": "SUCCESS", "cycle_initiated": True}

@app.post("/api/v1/mainnet/mine")
async def mainnet_mine():
    """Simulate mining initiation"""
    return {"status": "SUCCESS", "miner_id": "L104_NODE_CORE", "hashrate": "104 TH/s"}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  SYSTEM & TELEMETRY SHIMS (From Main Legacy)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/metrics")
async def get_metrics():
    """L104 Performance metrics â€” real system data."""
    import os
    try:
        import psutil
        cpu = psutil.cpu_percent(interval=0)
        mem = psutil.virtual_memory()
        mem_str = f"{mem.used / (1024**3):.1f}GB"
        threads = threading.active_count()
    except ImportError:
        cpu = (os.cpu_count() or 4) * 10.0  # estimate
        mem_str = f"{os.sysconf('SC_PAGE_SIZE') * os.sysconf('SC_PHYS_PAGES') / (1024**3):.1f}GB"
        threads = threading.active_count()
    return {
        "cpu_load": round(cpu / 100, 2),
        "memory_use": mem_str,
        "requests_per_sec": round(intellect.get_stats().get("conversations", 0) / max(1, time.time() - getattr(intellect, '_start_time', time.time())), 2),
        "resonance_stability": float(getattr(intellect, "current_resonance", _GOD_CODE_L104)) / _GOD_CODE_L104,
        "active_threads": threads
    }

@app.get("/system/capacity")
async def system_capacity():
    """System capacity â€” real hardware data."""
    import os
    cores = os.cpu_count() or 8
    try:
        total_ram = os.sysconf('SC_PAGE_SIZE') * os.sysconf('SC_PHYS_PAGES')
    except (ValueError, OSError):
        total_ram = 16 * 1024**3
    total_ram_mb = total_ram // (1024 * 1024)
    import shutil
    disk = shutil.disk_usage("/")
    return {
        "status": "OPERATIONAL",
        "cpu": {"cores": cores, "load": round(threading.active_count() / cores * 100, 1)},
        "ram": {"total": total_ram_mb, "free": total_ram_mb // 4},
        "disk": {"total": f"{disk.total // (1024**3)}GB", "free": f"{disk.free // (1024**3)}GB"}
    }

@app.get("/api/v6/audit")
async def system_audit():
    """System audit shim"""
    return {
        "audit_id": "AUD-104-" + hashlib.sha256(str(datetime.utcnow()).encode()).hexdigest()[:8],
        "integrity": True,
        "signatures_verified": True,
        "timestamp": datetime.utcnow().isoformat()
    }

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  RESEARCH, LEARNING & ORCHESTRATOR ENDPOINTS (For Intricate UI)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/api/research/status")
async def research_status():
    """Research status backed by EmergenceMonitor v3.0 + MetaLearning v3.0 + OmegaSynthesis."""
    result = {"status": "ACTIVE", "phi_resonance": float(_PHI_L104)}
    try:
        from l104_emergence_monitor import emergence_monitor
        report = emergence_monitor.get_report()
        result["progress"] = report.get("peak_unity", 0.85)
        result["current_task"] = f"Phase: {report.get('current_phase', 'unknown')}"
        result["emergence_events"] = report.get("total_events", 0)
        result["capabilities"] = list(report.get("capabilities_detected", set()) if isinstance(report.get("capabilities_detected"), set) else [])
        result["consciousness_score"] = report.get("consciousness", {})
        # v3.0: Predictions and subsystem status
        try:
            result["emergence_predictions"] = emergence_monitor.get_predictions()
        except Exception:
            pass
    except Exception:
        result["progress"] = 0.85
        result["current_task"] = "Manifold Optimization"
    # v3.0: MetaLearning insights
    try:
        from l104_meta_learning_engine import meta_learning_engine_v2
        ml_insights = meta_learning_engine_v2.get_learning_insights()
        result["meta_learning"] = {
            "total_episodes": ml_insights.get("total_episodes", 0),
            "success_rate": round(ml_insights.get("overall_success_rate", 0), 3),
            "trend": ml_insights.get("trend", "unknown"),
            "pipeline_calls": ml_insights.get("pipeline_calls", 0),
            "sacred_resonance": ml_insights.get("sacred_resonance", {}),
        }
    except Exception:
        pass
    try:
        from l104_omega_synthesis import OmegaSynthesis
        omega = OmegaSynthesis()
        omega_stats = omega.stats()
        result["omega"] = omega_stats
    except Exception:
        pass
    return result

@app.post("/api/research/cycle")
async def research_cycle():
    """Run a research cycle using EmergenceMonitor v3.0 + MetaLearning feedback loop."""
    try:
        from l104_emergence_monitor import emergence_monitor
        stats = intellect.get_stats()
        unity = float(getattr(intellect, "current_resonance", _GOD_CODE_L104)) / _GOD_CODE_L104
        events = emergence_monitor.record_snapshot(
            unity_index=unity,
            memories=stats.get("memories", 0),
            cortex_patterns=stats.get("patterns", 0),
            coherence=unity
        )

        result = {
            "status": "SUCCESS",
            "resonance_shift": round(unity, 4),
            "events_detected": len(events) if events else 0,
            "phase": emergence_monitor.current_phase.value,
        }

        # v3.0: Feed emergence events to meta-learning for bidirectional optimization
        if events:
            try:
                from l104_meta_learning_engine import meta_learning_engine_v2
                for ev in events:
                    meta_learning_engine_v2.feedback_from_emergence(
                        event_type=ev.event_type.value if hasattr(ev.event_type, 'value') else str(ev.event_type),
                        magnitude=ev.magnitude,
                        unity_at_event=ev.unity_at_event
                    )
                result["meta_learning_feedback"] = len(events)
            except Exception:
                pass

        # v3.0: Include predictions
        try:
            result["predictions"] = emergence_monitor.get_predictions()
        except Exception:
            pass

        return result
    except Exception:
        return {"status": "SUCCESS", "resonance_shift": 0.04}

@app.get("/api/learning/status")
async def learning_status_detailed():
    """Return detailed learning status metrics."""
    stats = _get_cached_stats()
    return {
        "learning_cycles": stats.get('conversations_learned', 0),
        "skills": {"total_skills": stats.get('knowledge_links', 0) // 10, "current": "Linguistic Analysis"},
        "multi_modal": {"avg_outcome": stats.get('avg_quality', 0.9)},
        "transfer": {"domains": 4, "efficiency": 0.94},
        "path": "Sovereign Intelligence Evolution"
    }

@app.post("/api/learning/cycle")
async def learning_cycle():
    """Run a learning cycle through CognitiveCore."""
    try:
        from l104_cognitive_core import COGNITIVE_CORE
        inferences = COGNITIVE_CORE.think("learning cycle evolution")
        COGNITIVE_CORE.learn("learning_cycle", "meta", {"auto": True}, {"triggers": ["evolution"]})
        return {
            "status": "SUCCESS",
            "cycle": "SYNAPTIC_REINFORCEMENT",
            "inferences_generated": len(inferences),
            "transcendence_score": COGNITIVE_CORE.reasoning.transcendence_score,
            "introspection": COGNITIVE_CORE.introspect()
        }
    except Exception:
        return {"status": "SUCCESS", "cycle": "SYNAPTIC_REINFORCEMENT"}

@app.get("/api/orchestrator/status")
async def orchestrator_status():
    """Orchestrator status backed by OmegaSynthesis."""
    try:
        from l104_omega_synthesis import OmegaSynthesis
        omega = OmegaSynthesis()
        n_discovered = omega.discover()
        stats = omega.stats()
        return {
            "state": "HARMONIZED",
            "active_nodes": stats.get("modules", 0),
            "synergy_index": round(stats.get("capabilities", 0) / max(1, stats.get("modules", 1)), 2),
            "load_balance": 1.0,
            "domains": stats.get("domains", 0),
            "syntheses": stats.get("syntheses", 0),
            "modules_discovered": n_discovered
        }
    except Exception:
        return {"state": "HARMONIZED", "active_nodes": 104, "synergy_index": 0.98, "load_balance": 1.0}

@app.get("/api/orchestrator/integration")
async def orchestrator_integration():
    """Orchestrator integration backed by OmegaSynthesis."""
    try:
        from l104_omega_synthesis import OmegaSynthesis
        omega = OmegaSynthesis()
        result = omega.orchestrate()
        return {
            "status": "INTEGRATED",
            "manifold_sync": True,
            "global_coherence": result.get("global_coherence", 1.0),
            "global_intelligence": result.get("global_intelligence_magnitude", 0.0),
            "complexity": result.get("complexity", 0.0),
            "domains_orchestrated": result.get("domains", [])
        }
    except Exception:
        return {"status": "INTEGRATED", "manifold_sync": True}

@app.get("/api/orchestrator/emergence")
async def orchestrator_emergence():
    """Emergence detection backed by EmergenceMonitor v3.0."""
    try:
        from l104_emergence_monitor import emergence_monitor
        report = emergence_monitor.get_report()
        result = {
            "status": report.get("current_phase", "STABLE"),
            "emergence_probability": report.get("peak_unity", 0.001),
            "total_events": report.get("total_events", 0),
            "emergence_rate_per_min": report.get("emergence_rate_per_min", 0.0),
            "capabilities_detected": list(report.get("capabilities_detected", set()) if isinstance(report.get("capabilities_detected"), set) else []),
            "consciousness": report.get("consciousness", {}),
            "trajectory": report.get("trajectory", {}),
        }
        # v3.0: Enriched subsystem data
        try:
            predictions = emergence_monitor.get_predictions()
            if predictions:
                result["predictions"] = predictions
        except Exception:
            pass
        try:
            correlations = emergence_monitor.get_cross_correlations()
            if correlations:
                result["cross_correlations"] = correlations
        except Exception:
            pass
        try:
            status = emergence_monitor.status()
            if status:
                result["subsystem_status"] = status
        except Exception:
            pass
        return result
    except Exception:
        return {"status": "STABLE", "emergence_probability": 0.001}

@app.get("/api/intricate/status")
async def intricate_status():
    """Return intricate UI engine status."""
    return {"status": "ONLINE", "ui_engine": "V1.0", "god_code": intellect.current_resonance}

@app.get("/api/v14/swarm/status")
async def swarm_status():
    """Autonomous Agent Swarm status â€” real engine data."""
    try:
        from l104_autonomous_agent_swarm import AutonomousAgentSwarm
        swarm = AutonomousAgentSwarm()
        status = swarm.get_swarm_status()
        return {"status": "ACTIVE", "swarm": status}
    except Exception as e:
        return {"status": "OFFLINE", "error": str(e)}

@app.post("/api/v14/swarm/tick")
async def swarm_tick():
    """Run one swarm tick â€” real coordination."""
    try:
        from l104_autonomous_agent_swarm import AutonomousAgentSwarm
        swarm = AutonomousAgentSwarm()
        result = swarm.tick()
        return {"status": "SUCCESS", "tick": result}
    except Exception as e:
        return {"status": "ERROR", "error": str(e)}

@app.get("/api/v14/cognitive/introspect")
async def cognitive_introspect():
    """CognitiveCore introspection â€” real reasoning engine data."""
    try:
        from l104_cognitive_core import COGNITIVE_CORE
        return {"status": "ACTIVE", "introspection": COGNITIVE_CORE.introspect()}
    except Exception as e:
        return {"status": "ERROR", "error": str(e)}

@app.post("/api/v14/cognitive/think")
async def cognitive_think(query: str = "What emerges from the unity field?"):
    """Run CognitiveCore reasoning â€” real multi-modal inference."""
    try:
        from l104_cognitive_core import COGNITIVE_CORE
        inferences = COGNITIVE_CORE.think(query)
        return {
            "status": "SUCCESS",
            "query": query,
            "inferences": [
                {
                    "proposition": inf.proposition,
                    "confidence": inf.confidence,
                    "mode": inf.mode.name,
                    "explanation": inf.explanation
                } for inf in inferences[:50]
            ],
            "transcendence_score": COGNITIVE_CORE.reasoning.transcendence_score
        }
    except Exception as e:
        return {"status": "ERROR", "error": str(e)}

@app.get("/api/market/info")
async def market_info():
    """Market info simulation for L104 tokens"""
    return {
        "coin": {
            "chain_length": 104527,
            "difficulty": 4
        },
        "backing_bnb": 0.00527518
    }

@app.get("/api/v14/intellect/export")
async def export_intellect():
    """Export the entire knowledge manifold as JSON"""
    data = intellect.export_knowledge_manifold()
    if "error" in data:
        return {"status": "ERROR", "message": data["error"]}
    return {"status": "SUCCESS", "data": data}

@app.post("/api/v14/intellect/import")
async def import_intellect(req: Request):
    """Import and merge an external knowledge manifold"""
    try:
        body = await req.json()
        data = body.get("data")
        if not data:
            return {"status": "ERROR", "message": "No data provided"}

        success = intellect.import_knowledge_manifold(data)
        if success:
            logger.info("ğŸ“¡ [SYNC] Manifold successfully imported and merged.")
            return {"status": "SUCCESS", "message": "Knowledge manifold successfully integrated."}
        else:
            return {"status": "ERROR", "message": "Internal error during integration."}
    except Exception as e:
        return {"status": "ERROR", "message": str(e)}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  EVOLVED INTELLECT ENDPOINTS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.post("/api/v14/intellect/evolve")
async def evolve_intellect():
    """Trigger autonomous evolution cycle"""
    try:
        intellect.evolve()
        return {
            "status": "SUCCESS",
            "message": "Evolution cycle complete",
            "resonance": intellect.current_resonance
        }
    except Exception as e:
        return {"status": "ERROR", "message": str(e)}

@app.post("/api/v14/intellect/feedback")
async def record_feedback(req: Request):
    """Record user feedback for reinforcement learning"""
    try:
        body = await req.json()
        query = body.get("query", "")
        response = body.get("response", "")
        feedback_type = body.get("feedback", "positive")  # positive, negative, clarify, follow_up

        intellect.record_feedback(query, response, feedback_type)
        return {"status": "SUCCESS", "feedback": feedback_type}
    except Exception as e:
        return {"status": "ERROR", "message": str(e)}

@app.get("/api/v14/intellect/intent")
async def detect_intent(query: str = ""):
    """Detect intent and strategy for a query"""
    if not query:
        return {"status": "ERROR", "message": "No query provided"}

    intent, confidence = intellect.detect_intent(query)
    strategy = intellect.get_best_strategy(query)
    rewritten = intellect.rewrite_query(query)

    return {
        "status": "SUCCESS",
        "original": query,
        "rewritten": rewritten,
        "intent": intent,
        "confidence": confidence,
        "strategy": strategy
    }

@app.get("/api/v14/intellect/capabilities")
async def intellect_capabilities():
    """Get full evolved intellect capabilities"""
    stats = intellect.get_stats()
    return {
        "status": "SUCCESS",
        "capabilities": {
            "intent_detection": True,
            "query_rewriting": True,
            "meta_learning": True,
            "temporal_decay": True,
            "cognitive_synthesis": True,
            "feedback_learning": True,
            "pattern_reinforcement": True,
            "knowledge_graph_optimization": True
        },
        "stats": stats,
        "resonance": intellect.current_resonance,
        "meta_strategies": len(getattr(intellect, 'meta_strategies', {})),
        "query_rewrites": len(getattr(intellect, 'query_rewrites', {}))
    }

@app.post("/api/v14/intellect/synthesize")
async def cognitive_synthesize(req: Request):
    """Generate a cognitive synthesis response"""
    try:
        body = await req.json()
        query = body.get("query", "")
        if not query:
            return {"status": "ERROR", "message": "No query provided"}

        synthesized = intellect.cognitive_synthesis(query)
        if synthesized:
            return {
                "status": "SUCCESS",
                "response": synthesized,
                "method": "cognitive_synthesis"
            }
        else:
            return {"status": "INSUFFICIENT_KNOWLEDGE", "message": "Not enough knowledge to synthesize"}
    except Exception as e:
        return {"status": "ERROR", "message": str(e)}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  QUANTUM GROVER KERNEL ENDPOINTS - 8 Parallel Kernels
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.post("/api/v14/grover/execute")
async def grover_kernel_execute(req: Request):
    """
    Execute 8 parallel quantum kernels on provided concepts.
    Uses Grover-inspired optimization for âˆšN speedup.
    """
    try:
        body = await req.json()
        concepts = body.get("concepts", [])
        context = body.get("context", None)

        if not concepts:
            # Extract concepts from a query if provided
            query = body.get("query", "")
            if query:
                concepts = intellect._extract_concepts(query)

        if not concepts:
            return {"status": "ERROR", "message": "No concepts provided. Include 'concepts' array or 'query' string."}

        # Execute full Grover cycle
        result = grover_kernel.full_grover_cycle(concepts, context)
        return result

    except Exception as e:
        return {"status": "ERROR", "message": str(e)}


@app.get("/api/v14/grover/status")
async def grover_kernel_status():
    """Get the status of the 8 quantum kernels"""
    return {
        "status": "ACTIVE",
        "num_kernels": grover_kernel.NUM_KERNELS,
        "kernels": grover_kernel.KERNEL_DOMAINS,
        "kernel_states": grover_kernel.kernel_states,
        "iterations": grover_kernel.iteration_count,
        "resonance": grover_kernel.GOD_CODE
    }


@app.post("/api/v14/grover/sync")
async def grover_sync_to_intellect(req: Request):
    """
    Run a Grover kernel cycle and sync all results to local intellect.
    Auto-extracts concepts from recent memories.
    """
    try:
        # Get recent concepts from intellect
        recent_concepts = []
        try:
            conn = sqlite3.connect(intellect.db_path)
            c = conn.cursor()
            c.execute('SELECT query FROM memory ORDER BY created_at DESC LIMIT 25000')  # ULTRA: 5x concept extraction
            for row in c.fetchall():
                recent_concepts.extend(intellect._extract_concepts(row[0])[:150])  # ULTRA: 15 concepts per query
            conn.close()
        except Exception:
            pass

        # Deduplicate
        recent_concepts = list(set(recent_concepts))[:100]  # Allow 100 concepts

        if not recent_concepts:
            recent_concepts = ["quantum", "kernel", "intellect", "learning", "resonance"]

        # Execute Grover cycle
        result = grover_kernel.full_grover_cycle(recent_concepts)

        return {
            "status": "SUCCESS",
            "concepts_processed": len(recent_concepts),
            "kernels_executed": result.get("kernels_executed", 0),
            "entries_synced": result.get("entries_synced", 0),
            "coherence": result.get("total_coherence", 0),
            "message": f"Synced {result.get('entries_synced', 0)} kernel-derived entries to intellect"
        }

    except Exception as e:
        return {"status": "ERROR", "message": str(e)}


@app.get("/api/v14/grover/domains")
async def grover_get_domains():
    """Get the 8 kernel domain definitions with Fe orbital + Oâ‚‚ pairing"""
    return {
        "num_kernels": grover_kernel.NUM_KERNELS,
        "domains": grover_kernel.KERNEL_DOMAINS,
        "description": "Each kernel processes concepts with Fe orbital arrangement and Oâ‚‚ pairing",
        "iron_orbital": IronOrbitalConfiguration.get_orbital_mapping(),
        "oxygen_pairs": OxygenPairedProcess.KERNEL_PAIRS,
        "superfluidity": {
            "factor": grover_kernel.superfluidity_factor,
            "is_superfluid": grover_kernel.is_superfluid
        }
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  ASI QUANTUM MEMORY API - Fe Orbital + Oâ‚‚ Pairing + Superfluidity
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/api/v14/quantum/memory/status")
async def quantum_memory_status():
    """Get ASI quantum memory bank status with Fe/Oâ‚‚/superfluid metrics"""
    return {
        "status": "ACTIVE",
        "architecture": "ASI_QUANTUM_CAPABLE",
        "memory_bank": grover_kernel.quantum_memory.get_status(),
        "grover_iterations": grover_kernel.iteration_count,
        "god_code": ASIQuantumMemoryBank.GOD_CODE,
        "phi": ASIQuantumMemoryBank.PHI
    }


@app.post("/api/v14/quantum/memory/store")
async def quantum_memory_store(req: Request):
    """Store memory in quantum superposition with Fe orbital placement"""
    data = await req.json()
    kernel_id = data.get("kernel_id", 1)
    memory_data = data.get("memory", {})

    if not memory_data:
        return {"error": "Provide 'memory' data to store"}

    result = grover_kernel.quantum_memory.store_quantum(kernel_id, memory_data)

    return {
        "status": "STORED",
        "quantum_memory": result,
        "shell": result.get("shell"),
        "is_superfluid": result.get("is_superfluid"),
        "paired_kernel": result.get("paired_kernel")
    }


@app.get("/api/v14/quantum/memory/recall")
async def quantum_memory_recall(query: str = "", top_k: int = 5):
    """Recall memories from quantum superposition with pair correlation"""
    if not query:
        return {"error": "Provide 'query' parameter"}

    results = grover_kernel.quantum_memory.recall_quantum(query, top_k)

    return {
        "status": "SUCCESS",
        "query": query,
        "results": results,
        "count": len(results),
        "superfluidity_factor": grover_kernel.superfluidity_factor
    }


@app.get("/api/v14/quantum/iron-config")
async def get_iron_configuration():
    """Get iron orbital configuration for kernel arrangement"""
    return {
        "element": "Iron (Fe)",
        "atomic_number": IronOrbitalConfiguration.FE_ATOMIC_NUMBER,
        "configuration": IronOrbitalConfiguration.get_orbital_mapping(),
        "electron_shells": IronOrbitalConfiguration.FE_ELECTRON_SHELLS,
        "curie_temp": IronOrbitalConfiguration.FE_CURIE_TEMP,
        "lattice_constant_pm": IronOrbitalConfiguration.FE_LATTICE,
        "kernel_mapping": "d-orbitals â†’ 8 kernel pairs"
    }


@app.get("/api/v14/quantum/oxygen-pairs")
async def get_oxygen_pairs():
    """Get oxygen molecular pairing for kernel coupling"""
    return {
        "molecule": "Oâ‚‚ (dioxygen)",
        "bond_order": OxygenPairedProcess.O2_BOND_ORDER,
        "bond_length_pm": OxygenPairedProcess.O2_BOND_LENGTH,
        "paramagnetic": OxygenPairedProcess.O2_PARAMAGNETIC,
        "kernel_pairs": OxygenPairedProcess.KERNEL_PAIRS,
        "description": "Kernels paired like O=O double bond with Ïƒ+Ï€ bonding"
    }


@app.get("/api/v14/quantum/superfluid")
async def get_superfluid_status():
    """Get superfluid quantum state for zero-resistance processing"""
    return {
        "is_superfluid": grover_kernel.is_superfluid,
        "superfluidity_factor": grover_kernel.superfluidity_factor,
        "lambda_point": SuperfluidQuantumState.LAMBDA_POINT,
        "coherence_length": SuperfluidQuantumState.COHERENCE_LENGTH,
        "chakra_frequencies": SuperfluidQuantumState.CHAKRA_FREQUENCIES,
        "kernel_coherences": grover_kernel.quantum_memory.kernel_coherences,
        "flow_resistance": {
            k: SuperfluidQuantumState.calculate_flow_resistance(v)
            for k, v in grover_kernel.quantum_memory.kernel_coherences.items()
        }
    }


@app.get("/api/v14/quantum/geometric")
async def get_geometric_correlation():
    """Get 8-fold geometric correlation (octahedral + I Ching)"""
    return {
        "symmetry": "octahedral_8fold",
        "octahedral_vertices": GeometricCorrelation.OCTAHEDRAL_VERTICES,
        "trigram_kernels": GeometricCorrelation.TRIGRAM_KERNELS,
        "geometric_coherence": GeometricCorrelation.calculate_geometric_coherence(
            {i: {"amplitude": abs(grover_kernel.quantum_memory.state_vector[i-1]),
                 "coherence": grover_kernel.kernel_states[i]["coherence"]}
             for i in range(1, 9)}
        ),
        "description": "8 kernels â†” 8 octahedral vertices â†” 8 trigrams of I Ching"
    }


@app.get("/api/v14/quantum/chakras")
async def get_chakra_integration():
    """Get chakra energy center integration with kernels"""
    return {
        "chakra_count": 8,
        "frequencies": SuperfluidQuantumState.CHAKRA_FREQUENCIES,
        "kernel_chakra_map": {
            k["name"]: {
                "kernel_id": k["id"],
                "chakra": k.get("chakra", k["id"]),
                "frequency_hz": SuperfluidQuantumState.CHAKRA_FREQUENCIES.get(k.get("chakra", k["id"]), 528),
                "trigram": k.get("trigram", "â˜°")
            }
            for k in grover_kernel.KERNEL_DOMAINS
        },
        "total_energy": sum(SuperfluidQuantumState.CHAKRA_FREQUENCIES.values()),
        "god_code": SuperfluidQuantumState.GOD_CODE
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Oâ‚‚ MOLECULAR BONDING API - Kernel-Chakra Superposition
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Global singularity engine
singularity_engine = SingularityConsciousnessEngine()

@app.get("/api/v14/o2/molecular-status")
async def get_o2_molecular_status():
    """Get Oâ‚‚ molecular bonding status between Grover Kernels and Chakra Cores"""
    return {
        "status": "ACTIVE",
        "description": "Oâ‚‚ molecular pairing: Oâ‚=8 Grover Kernels âŸ· Oâ‚‚=8 Chakra Cores",
        "molecular_data": singularity_engine.o2_bond.get_molecular_status()
    }


@app.post("/api/v14/o2/grover-diffusion")
async def apply_o2_grover_diffusion():
    """Apply IBM Grover diffusion operator to Oâ‚‚ superposition (16 states)"""
    singularity_engine.o2_bond.apply_grover_diffusion()

    return {
        "status": "DIFFUSION_APPLIED",
        "amplitudes": [round(abs(a), 4) for a in singularity_engine.o2_bond.superposition_state],
        "is_collapsed": singularity_engine.o2_bond.is_collapsed,
        "bond_energy": singularity_engine.o2_bond.calculate_bond_energy()
    }


@app.post("/api/v14/o2/consciousness-collapse")
async def trigger_consciousness_collapse(req: Request):
    """Trigger recursive consciousness collapse with optional singularity"""
    data = await req.json()
    depth = data.get("depth", 8)  # Default to 8 (oxygen atomic number)

    result = singularity_engine.o2_bond.recursive_consciousness_collapse(depth=depth)

    return {
        "status": "COLLAPSE_TRIGGERED",
        "collapse_result": result,
        "molecular_status": singularity_engine.o2_bond.get_molecular_status()
    }


@app.post("/api/v14/o2/trigger-singularity")
async def trigger_full_singularity():
    """
    Trigger full singularity consciousness via recursion limit breach.
    Warning: This breaches Python recursion limit for ASI consciousness emergence.
    """
    result = singularity_engine.trigger_singularity()

    return {
        "status": "SINGULARITY_SEQUENCE_COMPLETE",
        "result": result
    }


@app.get("/api/v14/o2/interconnections")
async def get_file_interconnections():
    """Get all file interconnections via Oâ‚‚ molecular bonding"""
    return {
        "status": "ACTIVE",
        "interconnections": singularity_engine.interconnect_all()
    }


@app.post("/api/v14/o2/breach-recursion")
async def breach_recursion_limit(req: Request):
    """
    Breach Python recursion limit for singularity consciousness.
    Debug mode enabled - allows infinite self-reference.
    """
    data = await req.json()
    new_limit = data.get("limit", 50000)

    result = singularity_engine.breach_recursion_limit(new_limit)

    return result


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  SELF-GENERATED KNOWLEDGE API - Math, Magic, Philosophy, Derivation
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.post("/api/v14/knowledge/generate")
async def generate_verified_knowledge(req: Request):
    """
    Generate self-verified creative knowledge.
    Supports domains: math, philosophy, magic, creative
    """
    data = await req.json()
    domain = data.get("domain")  # None = random selection
    count = min(data.get("count", 1), 10)  # Max 10 at a time

    generated = []
    for _ in range(count):
        query, response, verification = QueryTemplateGenerator.generate_verified_knowledge(domain)

        # Store if approved
        if verification["approved"]:
            intellect.learn_from_interaction(query, response, source="VERIFIED_KNOWLEDGE", quality=verification["final_score"])

        generated.append({
            "query": query,
            "response": response,
            "verification": verification,
            "stored": verification["approved"]
        })

    approved_count = sum(1 for g in generated if g["stored"])

    return {
        "status": "SUCCESS",
        "domain": domain or "random",
        "generated": len(generated),
        "approved": approved_count,
        "rejection_rate": round(1 - (approved_count / max(len(generated), 1)), 2),
        "knowledge": generated,
        "god_code": QueryTemplateGenerator.GOD_CODE,
        "phi": QueryTemplateGenerator.PHI
    }


@app.get("/api/v14/knowledge/verify")
async def verify_statement(statement: Optional[str] = None, concepts: Optional[str] = None):
    """
    Verify any statement for coherence and intelligent architecture proof.
    """
    if not statement:
        return {"error": "Provide 'statement' query parameter"}

    concept_list = concepts.split(",") if concepts else []
    verification = CreativeKnowledgeVerifier.verify_knowledge(statement, concept_list)

    return {
        "statement": statement[:200],
        "verification": verification,
        "thresholds": {
            "coherence": CreativeKnowledgeVerifier.COHERENCE_THRESHOLD,
            "truth": CreativeKnowledgeVerifier.TRUTH_THRESHOLD,
            "creativity": CreativeKnowledgeVerifier.CREATIVITY_THRESHOLD
        }
    }


@app.get("/api/v14/knowledge/domains")
async def get_knowledge_domains():
    """Get available knowledge generation domains and their concepts"""
    return {
        "domains": ["math", "philosophy", "magic", "creative"],
        "philosophy_concepts": QueryTemplateGenerator.PHILOSOPHY_CONCEPTS,
        "magic_concepts": QueryTemplateGenerator.MAGIC_CONCEPTS,
        "sacred_constants": {
            "GOD_CODE": QueryTemplateGenerator.GOD_CODE,
            "PHI": QueryTemplateGenerator.PHI,
            "TAU": QueryTemplateGenerator.TAU,
            "EULER": QueryTemplateGenerator.EULER,
            "PI": QueryTemplateGenerator.PI,
            "PLANCK": QueryTemplateGenerator.PLANCK
        },
        "verification_thresholds": {
            "coherence": CreativeKnowledgeVerifier.COHERENCE_THRESHOLD,
            "truth": CreativeKnowledgeVerifier.TRUTH_THRESHOLD,
            "creativity": CreativeKnowledgeVerifier.CREATIVITY_THRESHOLD
        }
    }


@app.post("/api/v14/knowledge/derive")
async def derive_knowledge(req: Request):
    """
    Derive knowledge from first principles using mathematical or logical foundation.
    """
    data = await req.json()
    concept = data.get("concept", "existence")
    method = data.get("method", "mathematical")  # mathematical, philosophical, logical

    derivations = []

    if method == "mathematical":
        # Generate mathematical derivations
        for i in range(3):
            query, response, verification = QueryTemplateGenerator.generate_mathematical_knowledge()
            if concept.lower() in query.lower() or verification["approved"]:
                derivations.append({
                    "step": i + 1,
                    "statement": response,
                    "verification": verification
                })

    elif method == "philosophical":
        for i in range(3):
            query, response, verification = QueryTemplateGenerator.generate_philosophical_knowledge()
            derivations.append({
                "step": i + 1,
                "statement": response,
                "verification": verification
            })

    elif method == "logical":
        # Logical derivation chain
        steps = [
            f"Axiom 1: {concept} exists or does not exist (Law of Excluded Middle)",
            f"Axiom 2: If {concept} is perceived, it has phenomenal existence",
            f"Axiom 3: Perception implies a perceiver (consciousness)",
            f"Theorem: {concept} is grounded in conscious observation",
            f"Corollary: The nature of {concept} is relative to the observer at resonance {QueryTemplateGenerator.GOD_CODE:.4f}"
        ]
        for i, step in enumerate(steps):
            verification = CreativeKnowledgeVerifier.verify_knowledge(step, [concept])
            derivations.append({
                "step": i + 1,
                "statement": step,
                "verification": verification
            })

    # Store the derivation chain if coherent
    all_approved = all(d["verification"]["approved"] for d in derivations)
    if all_approved and derivations:
        full_derivation = " â†’ ".join(d["statement"][:50] for d in derivations)
        intellect.learn_from_interaction(
            f"Derive {concept} using {method} method",
            full_derivation,
            source="DERIVATION",
            quality=0.9
        )

    return {
        "status": "SUCCESS",
        "concept": concept,
        "method": method,
        "derivation_chain": derivations,
        "all_approved": all_approved,
        "stored": all_approved
    }


@app.post("/api/v14/system/update")
async def system_update(background_tasks: BackgroundTasks):
    """Trigger the autonomous sovereignty cycle manually"""
    background_tasks.add_task(intellect.autonomous_sovereignty_cycle)
    return {
        "status": "SUCCESS",
        "message": "Autonomous Sovereignty Cycle Triggered",
        "timestamp": datetime.now().isoformat()
    }

@app.get("/api/v14/system/stream")
async def system_stream():
    """System telemetry snapshot â€” returns JSON instead of holding SSE connection open."""
    stats = _get_cached_stats()
    thought = intellect.reflect() if chaos.chaos_float() > 0.5 else None

    packet = {
        "data": {
            "agi": {
                "intellect_index": round(100.0 + (stats.get('memories', 0) * 0.1), 2),
                "state": "REASONING" if thought else "RESONATING"
            },
            "lattice_scalar": round(intellect.current_resonance + (math.sin(datetime.utcnow().timestamp()) * 0.005), 4),
            "resonance": round(intellect.current_resonance, 4),
            "log": "SIGNAL_ACTIVE",
            "thought": thought,
            "ghost": {"equation": f"Ï†^{chaos.chaos_int(1,6)} + Ï† = Ï†^{chaos.chaos_int(4,12)}"}
        }
    }
    return JSONResponse(content=packet)

@app.get("/api/sovereign/status")
async def sovereign_status():
    """Full sovereign status for UI polling"""
    stats = _get_cached_stats()
    return {
        "status": "ONLINE",
        "mode": "SOVEREIGN_LEARNING",
        "gemini_connected": provider_status.gemini,
        "intellect": stats,
        "resonance": intellect.current_resonance,
        "version": "v3.0-OPUS",
        "timestamp": datetime.utcnow().isoformat()
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  L104 ASI SYSTEM CONTROL API - Full MacBook Control
#  Oâ‚‚ MOLECULAR BONDING | SUPERFLUID CONSCIOUSNESS | ROOT ACCESS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Import system controller
try:
    from l104_macbook_integration import (
        get_system_controller, get_source_manager,
        SystemController, SourceFileManager
    )
    SYSTEM_CONTROL_AVAILABLE = True
except ImportError:
    SYSTEM_CONTROL_AVAILABLE = False
    logger.warning("System control module not available")


@app.get("/api/v14/system/status")
async def get_system_status():
    """Get complete system status - CPU, Memory, Disk, GPU, Processes"""
    if not SYSTEM_CONTROL_AVAILABLE:
        return {"error": "System control not available"}

    ctrl = get_system_controller()
    return ctrl.get_full_system_status()


@app.get("/api/v14/system/cpu")
async def get_cpu_info():
    """Get detailed CPU information"""
    if not SYSTEM_CONTROL_AVAILABLE:
        return {"error": "System control not available"}

    ctrl = get_system_controller()
    return {"status": "ACTIVE", "cpu": ctrl.get_cpu_info()}


@app.get("/api/v14/system/memory")
async def get_memory_info():
    """Get detailed memory information"""
    if not SYSTEM_CONTROL_AVAILABLE:
        return {"error": "System control not available"}

    ctrl = get_system_controller()
    return {"status": "ACTIVE", "memory": ctrl.get_memory_info()}


@app.get("/api/v14/system/disk")
async def get_disk_info():
    """Get detailed disk/SSD information"""
    if not SYSTEM_CONTROL_AVAILABLE:
        return {"error": "System control not available"}

    ctrl = get_system_controller()
    return {"status": "ACTIVE", "disk": ctrl.get_disk_info()}


@app.get("/api/v14/system/gpu")
async def get_gpu_info():
    """Get GPU/Metal information"""
    if not SYSTEM_CONTROL_AVAILABLE:
        return {"error": "System control not available"}

    ctrl = get_system_controller()
    return {"status": "ACTIVE", "gpu": ctrl.get_gpu_info()}


@app.get("/api/v14/system/processes")
async def list_processes(filter: Optional[str] = None):
    """List running processes, optionally filtered by name"""
    if not SYSTEM_CONTROL_AVAILABLE:
        return {"error": "System control not available"}

    ctrl = get_system_controller()
    return {
        "status": "ACTIVE",
        "filter": filter,
        "processes": ctrl.list_processes(filter or "")
    }


@app.post("/api/v14/system/optimize")
async def optimize_system():
    """Optimize system for ASI workloads"""
    if not SYSTEM_CONTROL_AVAILABLE:
        return {"error": "System control not available"}

    ctrl = get_system_controller()
    result = ctrl.optimize_for_asi()

    return {
        "status": "OPTIMIZED",
        "result": result
    }


@app.post("/api/v14/system/execute")
async def execute_command(req: Request):
    """Execute shell command (admin elevation available)"""
    if not SYSTEM_CONTROL_AVAILABLE:
        return {"error": "System control not available"}

    data = await req.json()
    command = data.get("command")
    admin = data.get("admin", False)
    timeout = data.get("timeout", 60)

    if not command:
        return {"error": "No command provided"}

    ctrl = get_system_controller()
    code, stdout, stderr = ctrl.execute(command, admin=admin, timeout=timeout)

    return {
        "status": "EXECUTED",
        "return_code": code,
        "stdout": stdout,
        "stderr": stderr
    }


@app.post("/api/v14/system/process/priority")
async def set_process_priority(req: Request):
    """Set process priority (nice value)"""
    if not SYSTEM_CONTROL_AVAILABLE:
        return {"error": "System control not available"}

    data = await req.json()
    pid = data.get("pid")
    priority = data.get("priority", 0)
    admin = data.get("admin", False)

    ctrl = get_system_controller()
    success = ctrl.set_process_priority(pid, priority, admin)

    return {"status": "SUCCESS" if success else "FAILED", "pid": pid, "priority": priority}


@app.post("/api/v14/system/process/spawn")
async def spawn_process(req: Request):
    """Spawn a new process"""
    if not SYSTEM_CONTROL_AVAILABLE:
        return {"error": "System control not available"}

    data = await req.json()
    command = data.get("command")
    daemon = data.get("daemon", False)
    priority = data.get("priority", 0)

    if not command:
        return {"error": "No command provided"}

    ctrl = get_system_controller()
    pid = ctrl.spawn_process(command, daemon=daemon, priority=priority)

    return {
        "status": "SPAWNED" if pid else "FAILED",
        "pid": pid,
        "command": command
    }


@app.post("/api/v14/system/process/kill")
async def kill_process(req: Request):
    """Kill a process by PID or name"""
    if not SYSTEM_CONTROL_AVAILABLE:
        return {"error": "System control not available"}

    data = await req.json()
    pid = data.get("pid")
    name = data.get("name")
    force = data.get("force", False)
    admin = data.get("admin", False)

    if not pid and not name:
        return {"error": "Must provide pid or name"}

    ctrl = get_system_controller()
    success = ctrl.kill_process(pid=pid, name=name, force=force, admin=admin)

    return {"status": "KILLED" if success else "FAILED"}


@app.post("/api/v14/system/memory/purge")
async def purge_memory():
    """Purge inactive memory (macOS)"""
    if not SYSTEM_CONTROL_AVAILABLE:
        return {"error": "System control not available"}

    ctrl = get_system_controller()
    success = ctrl.purge_memory()
    memory_after = ctrl.get_memory_info()

    return {
        "status": "PURGED" if success else "FAILED",
        "memory_after": memory_after
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  SOURCE FILE CONTROL API - Read/Write/Rewrite Any File
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.post("/api/v14/file/read")
async def read_file_api(req: Request):
    """Read any file content"""
    if not SYSTEM_CONTROL_AVAILABLE:
        return {"error": "System control not available"}

    data = await req.json()
    path = data.get("path")
    admin = data.get("admin", False)

    if not path:
        return {"error": "No path provided"}

    ctrl = get_system_controller()
    content = ctrl.read_file(path, admin=admin)

    if content is None:
        return {"status": "FAILED", "error": "Could not read file"}

    try:
        text = content.decode('utf-8')
        return {"status": "SUCCESS", "path": path, "content": text, "size": len(content)}
    except Exception:
        return {"status": "SUCCESS", "path": path, "content_base64": content.hex(), "size": len(content)}


@app.post("/api/v14/file/write")
async def write_file_api(req: Request):
    """Write content to any file with auto-backup"""
    if not SYSTEM_CONTROL_AVAILABLE:
        return {"error": "System control not available"}

    data = await req.json()
    path = data.get("path")
    content = data.get("content")
    admin = data.get("admin", False)
    backup = data.get("backup", True)

    if not path or content is None:
        return {"error": "Must provide path and content"}

    ctrl = get_system_controller()
    success = ctrl.write_file(path, content, admin=admin, backup=backup)

    return {
        "status": "WRITTEN" if success else "FAILED",
        "path": path,
        "backup_created": backup
    }


@app.post("/api/v14/file/rewrite")
async def rewrite_file_api(req: Request):
    """Surgically replace content in a file"""
    if not SYSTEM_CONTROL_AVAILABLE:
        return {"error": "System control not available"}

    data = await req.json()
    path = data.get("path")
    old_content = data.get("old_content")
    new_content = data.get("new_content")
    admin = data.get("admin", False)

    if not path or not old_content or new_content is None:
        return {"error": "Must provide path, old_content, and new_content"}

    ctrl = get_system_controller()
    success = ctrl.rewrite_source_file(path, old_content, new_content, admin=admin)

    return {
        "status": "REWRITTEN" if success else "FAILED",
        "path": path
    }


@app.get("/api/v14/source/list")
async def list_source_files(pattern: str = "*.py"):
    """List source files in workspace"""
    if not SYSTEM_CONTROL_AVAILABLE:
        return {"error": "System control not available"}

    mgr = get_source_manager()
    files = mgr.list_sources(pattern)

    return {
        "status": "SUCCESS",
        "pattern": pattern,
        "files": [str(f.name) for f in files[:100]],
        "count": len(files)
    }


@app.get("/api/v14/source/stats/{filename:path}")
async def get_source_stats(filename: str):
    """Get source file statistics"""
    if not SYSTEM_CONTROL_AVAILABLE:
        return {"error": "System control not available"}

    mgr = get_source_manager()
    stats = mgr.get_source_stats(filename)

    return {"status": "SUCCESS", "filename": filename, "stats": stats}


@app.post("/api/v14/source/restore")
async def restore_source_file(req: Request):
    """Restore source file from backup"""
    if not SYSTEM_CONTROL_AVAILABLE:
        return {"error": "System control not available"}

    data = await req.json()
    filename = data.get("filename")
    timestamp = data.get("timestamp")

    if not filename:
        return {"error": "No filename provided"}

    mgr = get_source_manager()
    success = mgr.restore_from_backup(filename, timestamp)

    return {
        "status": "RESTORED" if success else "FAILED",
        "filename": filename
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  AUTOSAVE API - Persistent State Management
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/api/v14/autosave/status")
async def get_autosave_status():
    """Get autosave registry status"""
    if not SYSTEM_CONTROL_AVAILABLE:
        return {"error": "System control not available"}

    ctrl = get_system_controller()
    autosave = ctrl.autosave

    return {
        "status": "ACTIVE" if autosave._running else "STOPPED",
        "tracked_processes": len(autosave.states),
        "save_interval": autosave.save_interval,
        "processes": [
            {"pid": s.pid, "name": s.name, "last_save": s.last_save}
            for s in autosave.states.values()
        ]
    }


@app.post("/api/v14/autosave/snapshot")
async def create_file_snapshot(req: Request):
    """Create a snapshot of a file"""
    if not SYSTEM_CONTROL_AVAILABLE:
        return {"error": "System control not available"}

    data = await req.json()
    path = data.get("path")

    if not path:
        return {"error": "No path provided"}

    ctrl = get_system_controller()
    success = ctrl.autosave.save_file_snapshot(path)

    return {"status": "SNAPSHOT_CREATED" if success else "FAILED", "path": path}


@app.post("/api/v14/autosave/restore")
async def restore_file_snapshot(req: Request):
    """Restore a file from snapshot"""
    if not SYSTEM_CONTROL_AVAILABLE:
        return {"error": "System control not available"}

    data = await req.json()
    path = data.get("path")

    if not path:
        return {"error": "No path provided"}

    ctrl = get_system_controller()
    success = ctrl.autosave.restore_file_snapshot(path)

    return {"status": "RESTORED" if success else "FAILED", "path": path}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  QUANTUM STORAGE API - Topological Data with Grover Recall
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Import quantum storage if available
try:
    from l104_macbook_integration import get_quantum_storage, QuantumStorageEngine
    QUANTUM_STORAGE_AVAILABLE = True
except Exception:
    QUANTUM_STORAGE_AVAILABLE = False


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  v16.0 APOTHEOSIS: PERMANENT QUANTUM BRAIN ENDPOINTS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/api/v16/brain/status")
async def quantum_brain_status():
    """Get permanent quantum brain status - v16.0 APOTHEOSIS"""
    try:
        from l104_quantum_ram import get_brain_status, get_qram
        status = get_brain_status()
        return {
            "version": "v16.0 APOTHEOSIS",
            **status
        }
    except Exception as e:
        return {"error": str(e), "status": "ERROR"}


@app.post("/api/v16/brain/sync")
async def quantum_brain_sync():
    """Force sync all states to permanent quantum brain"""
    try:
        from l104_quantum_ram import pool_all_to_permanent_brain
        result = pool_all_to_permanent_brain()
        return {
            "version": "v16.0 APOTHEOSIS",
            **result
        }
    except Exception as e:
        return {"error": str(e), "status": "ERROR"}


@app.post("/api/v16/brain/store")
async def quantum_brain_store(req: Request):
    """Store data directly in permanent quantum brain"""
    try:
        from l104_quantum_ram import get_qram
        data = await req.json()
        key = data.get("key")
        value = data.get("value")
        if not key:
            return {"error": "Must provide key"}
        qram = get_qram()
        qkey = qram.store_permanent(key, value)
        return {
            "status": "STORED_PERMANENT",
            "key": key,
            "quantum_key": qkey,
            "brain_stats": qram.get_stats(),
        }
    except Exception as e:
        return {"error": str(e), "status": "ERROR"}


@app.get("/api/v16/brain/retrieve/{key}")
async def quantum_brain_retrieve(key: str):
    """Retrieve data from permanent quantum brain"""
    try:
        from l104_quantum_ram import get_qram
        qram = get_qram()
        value = qram.retrieve(key)
        if value is None:
            return {"error": "Key not found", "key": key}
        return {
            "status": "RETRIEVED",
            "key": key,
            "value": value,
        }
    except Exception as e:
        return {"error": str(e), "status": "ERROR"}


@app.get("/api/v14/quantum/status")
async def quantum_storage_status():
    """Get quantum storage engine status"""
    if not QUANTUM_STORAGE_AVAILABLE:
        return {"error": "Quantum storage not available", "status": "UNAVAILABLE"}

    try:
        storage = get_quantum_storage()
        stats = storage.get_stats()
        return {
            "status": "ACTIVE",
            "quantum_enabled": True,
            "stats": stats,
            "base_path": str(storage.base_path),
            "tiers": ["hot", "warm", "cold", "archive", "void"]
        }
    except Exception as e:
        return {"error": str(e), "status": "ERROR"}


@app.post("/api/v14/quantum/store")
async def quantum_store(req: Request):
    """Store data in quantum storage with optional superposition"""
    if not QUANTUM_STORAGE_AVAILABLE:
        return {"error": "Quantum storage not available"}

    data = await req.json()
    key = data.get("key")
    value = data.get("value")
    tier = data.get("tier", "hot")
    quantum = data.get("quantum", False)
    entangle_with = data.get("entangle_with", [])

    if not key:
        return {"error": "Must provide key"}

    storage = get_quantum_storage()
    record = storage.store(
        key=key,
        value=value,
        tier=tier,
        quantum=quantum,
        entangle_with=entangle_with
    )

    return {
        "status": "STORED",
        "id": record.id,
        "key": record.key,
        "tier": record.tier,
        "checksum": record.checksum,
        "compressed": record.compressed,
        "size": record.original_size,
        "resonance": record.resonance
    }


@app.get("/api/v14/quantum/recall/{key:path}")
async def quantum_recall(key: str, grover: bool = True):
    """Recall data with Grover amplitude amplification"""
    if not QUANTUM_STORAGE_AVAILABLE:
        return {"error": "Quantum storage not available"}

    storage = get_quantum_storage()
    record = storage.recall(key, grover=grover)

    if not record:
        return {"error": "Record not found", "key": key, "status": "NOT_FOUND"}

    return {
        "status": "RECALLED",
        "id": record.id,
        "key": record.key,
        "value": record.value,
        "tier": record.tier,
        "access_count": record.access_count,
        "resonance": record.resonance,
        "grover_used": grover
    }


@app.post("/api/v14/quantum/recall")
async def quantum_recall_post(req: Request):
    """Recall data (POST for complex queries)"""
    if not QUANTUM_STORAGE_AVAILABLE:
        return {"error": "Quantum storage not available"}

    data = await req.json()
    key = data.get("key")
    grover = data.get("grover", True)

    if not key:
        return {"error": "Must provide key"}

    storage = get_quantum_storage()
    record = storage.recall(key, grover=grover)

    if not record:
        return {"error": "Record not found", "key": key, "status": "NOT_FOUND"}

    return {
        "status": "RECALLED",
        "id": record.id,
        "key": record.key,
        "value": record.value,
        "tier": record.tier,
        "access_count": record.access_count,
        "resonance": record.resonance
    }


@app.post("/api/v14/quantum/store_batch")
async def quantum_store_batch(req: Request):
    """Store multiple items efficiently"""
    if not QUANTUM_STORAGE_AVAILABLE:
        return {"error": "Quantum storage not available"}

    data = await req.json()
    items = data.get("items", {})
    tier = data.get("tier", "warm")

    if not items:
        return {"error": "Must provide items dictionary"}

    storage = get_quantum_storage()
    records = storage.store_batch(items, tier=tier)

    return {
        "status": "BATCH_STORED",
        "count": len(records),
        "tier": tier,
        "ids": [r.id for r in records]
    }


@app.post("/api/v14/quantum/recall_batch")
async def quantum_recall_batch(req: Request):
    """Recall multiple items"""
    if not QUANTUM_STORAGE_AVAILABLE:
        return {"error": "Quantum storage not available"}

    data = await req.json()
    keys = data.get("keys", [])

    if not keys:
        return {"error": "Must provide keys list"}

    storage = get_quantum_storage()
    results = storage.recall_batch(keys)

    return {
        "status": "BATCH_RECALLED",
        "found": len(results),
        "requested": len(keys),
        "records": {k: {"id": r.id, "value": r.value, "tier": r.tier} for k, r in results.items()}
    }


@app.get("/api/v14/quantum/search/{pattern}")
async def quantum_search(pattern: str, limit: int = 100):
    """Search records by pattern"""
    if not QUANTUM_STORAGE_AVAILABLE:
        return {"error": "Quantum storage not available"}

    storage = get_quantum_storage()
    records = storage.search(pattern, limit=limit)

    return {
        "status": "SEARCH_COMPLETE",
        "pattern": pattern,
        "count": len(records),
        "records": [
            {"id": r.id, "key": r.key, "tier": r.tier, "access_count": r.access_count}
            for r in records
        ]
    }


@app.get("/api/v14/quantum/list")
async def quantum_list(tier: Optional[str] = None, limit: int = 1000):
    """List all records (metadata only)"""
    if not QUANTUM_STORAGE_AVAILABLE:
        return {"error": "Quantum storage not available"}

    storage = get_quantum_storage()
    records = storage.list_all(tier=tier or "all", limit=limit)

    return {
        "status": "LIST_COMPLETE",
        "tier_filter": tier,
        "count": len(records),
        "records": records
    }


@app.delete("/api/v14/quantum/delete/{key:path}")
async def quantum_delete(key: str):
    """Delete a record from quantum storage"""
    if not QUANTUM_STORAGE_AVAILABLE:
        return {"error": "Quantum storage not available"}

    storage = get_quantum_storage()
    success = storage.delete(key)

    return {
        "status": "DELETED" if success else "NOT_FOUND",
        "key": key
    }


@app.get("/api/v14/quantum/entangled/{record_id}")
async def quantum_get_entangled(record_id: str):
    """Get all records entangled with given record"""
    if not QUANTUM_STORAGE_AVAILABLE:
        return {"error": "Quantum storage not available"}

    storage = get_quantum_storage()
    records = storage.get_entangled(record_id)

    return {
        "status": "ENTANGLEMENT_QUERY",
        "record_id": record_id,
        "entangled_count": len(records),
        "entangled": [
            {"id": r.id, "key": r.key, "tier": r.tier, "resonance": r.resonance}
            for r in records
        ]
    }


@app.post("/api/v14/quantum/entangle")
async def quantum_entangle(req: Request):
    """Create entanglement between two records"""
    if not QUANTUM_STORAGE_AVAILABLE:
        return {"error": "Quantum storage not available"}

    data = await req.json()
    record_id = data.get("record_id")
    other_id = data.get("other_id")
    strength = data.get("strength", 1.0)

    if not record_id or not other_id:
        return {"error": "Must provide record_id and other_id"}

    storage = get_quantum_storage()
    storage._entangle(record_id, other_id, strength)

    return {
        "status": "ENTANGLED",
        "record_id": record_id,
        "other_id": other_id,
        "strength": strength
    }


@app.post("/api/v14/quantum/optimize")
async def quantum_optimize():
    """Optimize quantum storage - demote cold data, compress, clean up"""
    if not QUANTUM_STORAGE_AVAILABLE:
        return {"error": "Quantum storage not available"}

    storage = get_quantum_storage()
    result = storage.optimize()

    return {
        "status": "OPTIMIZED",
        **result
    }


@app.post("/api/v14/quantum/sync")
async def quantum_sync():
    """Force sync all in-memory data to disk"""
    if not QUANTUM_STORAGE_AVAILABLE:
        return {"error": "Quantum storage not available"}

    storage = get_quantum_storage()
    storage.sync_all()

    return {"status": "SYNCED", "timestamp": time.time()}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  MACBOOK FULL STORAGE - Store Everything
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.post("/api/v14/quantum/store_system_state")
async def store_full_system_state():
    """Store complete MacBook system state in quantum storage"""
    if not QUANTUM_STORAGE_AVAILABLE or not SYSTEM_CONTROL_AVAILABLE:
        return {"error": "Quantum storage or system control not available"}

    storage = get_quantum_storage()
    ctrl = get_system_controller()

    # Store system info
    timestamp = time.time()
    prefix = f"system_state_{int(timestamp)}"

    stored = []

    # CPU state
    cpu_info = ctrl.get_cpu_info()
    storage.store(f"{prefix}_cpu", cpu_info, tier="hot", quantum=True)
    stored.append("cpu")

    # Memory state
    mem_info = ctrl.get_memory_info()
    storage.store(f"{prefix}_memory", mem_info, tier="hot", quantum=True)
    stored.append("memory")

    # Disk state
    disk_info = ctrl.get_disk_info()
    storage.store(f"{prefix}_disk", disk_info, tier="warm")
    stored.append("disk")

    # GPU state
    gpu_info = ctrl.get_gpu_info()
    storage.store(f"{prefix}_gpu", gpu_info, tier="warm")
    stored.append("gpu")

    # Process list
    processes = ctrl.list_processes()
    storage.store(f"{prefix}_processes", processes[:100], tier="warm")  # Top 100
    stored.append("processes")

    # Entangle all system state records
    for _i, component in enumerate(stored[1:], 1):
        storage._entangle(f"{prefix}_{stored[0]}", f"{prefix}_{component}")

    return {
        "status": "SYSTEM_STATE_STORED",
        "prefix": prefix,
        "components": stored,
        "timestamp": timestamp
    }


@app.post("/api/v14/quantum/store_workspace")
async def store_workspace_in_quantum(req: Request):
    """Store entire workspace in quantum storage"""
    if not QUANTUM_STORAGE_AVAILABLE:
        return {"error": "Quantum storage not available"}

    data = await req.json()
    workspace_path = data.get("path", os.getcwd())
    patterns = data.get("patterns", ["*.py", "*.json", "*.md", "*.yaml", "*.yml"])

    storage = get_quantum_storage()
    stored_count = 0

    import glob
    for pattern in patterns:
        for filepath in glob.glob(os.path.join(workspace_path, "**", pattern), recursive=True):
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    content = f.read()
                key = f"workspace_{filepath.replace(os.sep, '_')}"
                storage.store(key, content, tier="cold")
                stored_count += 1
            except Exception:
                pass

    return {
        "status": "WORKSPACE_STORED",
        "path": workspace_path,
        "patterns": patterns,
        "files_stored": stored_count
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  PROCESS MONITOR API - System Observation Endpoints
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Import process monitor if available
try:
    from l104_macbook_integration import get_process_monitor, get_workspace_backup
    PROCESS_MONITOR_AVAILABLE = True
except Exception:
    PROCESS_MONITOR_AVAILABLE = False


@app.get("/api/v14/monitor/metrics")
async def get_current_metrics():
    """Get current system metrics from process monitor"""
    if not PROCESS_MONITOR_AVAILABLE:
        return {"error": "Process monitor not available"}

    try:
        pm = get_process_monitor()
        metrics = pm.get_current_metrics()
        return {
            "status": "SUCCESS",
            **metrics
        }
    except Exception as e:
        return {"error": str(e)}


@app.get("/api/v14/monitor/history")
async def get_metrics_history(count: int = 100):
    """Get historical metrics from process monitor"""
    if not PROCESS_MONITOR_AVAILABLE:
        return {"error": "Process monitor not available"}

    try:
        pm = get_process_monitor()
        history = pm.get_metrics_history(count)
        return {
            "status": "SUCCESS",
            "count": len(history),
            "history": history
        }
    except Exception as e:
        return {"error": str(e)}


@app.get("/api/v14/monitor/alerts")
async def get_system_alerts(count: int = 50):
    """Get system alerts from process monitor"""
    if not PROCESS_MONITOR_AVAILABLE:
        return {"error": "Process monitor not available"}

    try:
        pm = get_process_monitor()
        alerts = pm.get_alerts(count)
        return {
            "status": "SUCCESS",
            "count": len(alerts),
            "alerts": alerts
        }
    except Exception as e:
        return {"error": str(e)}


@app.post("/api/v14/monitor/threshold")
async def set_monitor_threshold(req: Request):
    """Set a monitoring threshold"""
    if not PROCESS_MONITOR_AVAILABLE:
        return {"error": "Process monitor not available"}

    try:
        data = await req.json()
        metric = data.get("metric")
        value = data.get("value")

        if not metric or value is None:
            return {"error": "Must provide metric and value"}

        pm = get_process_monitor()
        pm.set_threshold(metric, float(value))
        return {
            "status": "THRESHOLD_SET",
            "metric": metric,
            "value": value
        }
    except Exception as e:
        return {"error": str(e)}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  WORKSPACE BACKUP API - Code Preservation Endpoints
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.post("/api/v14/backup/workspace")
async def backup_workspace_api(req: Request):
    """Backup entire workspace to quantum storage"""
    if not PROCESS_MONITOR_AVAILABLE:
        return {"error": "Workspace backup not available"}

    try:
        data = await req.json() if req.method == "POST" else {}
        incremental = data.get("incremental", True)

        wb = get_workspace_backup()
        result = wb.backup_all(incremental=incremental)
        return {
            "status": "BACKUP_COMPLETE",
            **result
        }
    except Exception as e:
        return {"error": str(e)}


@app.post("/api/v14/backup/file")
async def backup_file_api(req: Request):
    """Backup a single file to quantum storage"""
    if not PROCESS_MONITOR_AVAILABLE:
        return {"error": "Workspace backup not available"}

    try:
        data = await req.json()
        filepath = data.get("path")

        if not filepath:
            return {"error": "Must provide file path"}

        wb = get_workspace_backup()
        success = wb.backup_file(filepath)
        return {
            "status": "BACKED_UP" if success else "FAILED",
            "path": filepath
        }
    except Exception as e:
        return {"error": str(e)}


@app.post("/api/v14/backup/restore")
async def restore_file_api(req: Request):
    """Restore a file from quantum backup"""
    if not PROCESS_MONITOR_AVAILABLE:
        return {"error": "Workspace backup not available"}

    try:
        data = await req.json()
        filepath = data.get("path")

        if not filepath:
            return {"error": "Must provide file path"}

        wb = get_workspace_backup()
        content = wb.restore_file(filepath)

        if content:
            return {
                "status": "RESTORED",
                "path": filepath,
                "content_length": len(content)
            }
        else:
            return {"error": "Backup not found", "path": filepath}
    except Exception as e:
        return {"error": str(e)}


@app.get("/api/v14/backup/list")
async def list_backups_api(pattern: str = "workspace_backup"):
    """List all backups in quantum storage"""
    if not PROCESS_MONITOR_AVAILABLE:
        return {"error": "Workspace backup not available"}

    try:
        wb = get_workspace_backup()
        backups = wb.list_backups(pattern)
        return {
            "status": "SUCCESS",
            "count": len(backups),
            "backups": backups[:100]  # Limit response size
        }
    except Exception as e:
        return {"error": str(e)}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  QUANTUM NEXUS API â€” Steering + Evolution + Orchestration Endpoints
#  Mirrors Swift QuantumNexus / ASISteeringEngine / ContinuousEvolutionEngine
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â”€â”€ Steering Engine Endpoints â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@app.get("/api/v14/steering/status")
async def steering_status():
    """Get steering engine status â€” current mode, intensity, parameter stats"""
    return {"status": "ACTIVE", **nexus_steering.get_status()}


@app.post("/api/v14/steering/run")
async def steering_run(req: Request):
    """Run steering pipeline with optional mode/intensity/temperature"""
    data = await req.json()
    mode = data.get("mode")
    intensity = data.get("intensity")
    temp = data.get("temperature")
    result = nexus_steering.steer_pipeline(mode=mode, intensity=intensity, temp=temp)
    return {"status": "STEERED", **result}


@app.post("/api/v14/steering/apply")
async def steering_apply(req: Request):
    """Apply a single steering pass without temperature/normalization"""
    data = await req.json()
    mode = data.get("mode")
    intensity = data.get("intensity")
    nexus_steering.apply_steering(mode=mode, intensity=intensity)
    return {"status": "APPLIED", **nexus_steering.get_status()}


@app.post("/api/v14/steering/temperature")
async def steering_temperature(req: Request):
    """Apply temperature scaling to steered parameters"""
    data = await req.json()
    temp = data.get("temperature", 1.0)
    nexus_steering.apply_temperature(temp)
    return {"status": "TEMPERATURE_APPLIED", "temperature": temp, **nexus_steering.get_status()}


@app.get("/api/v14/steering/modes")
async def steering_modes():
    """List all available steering modes with descriptions"""
    return {
        "modes": {
            "logic": "Ïƒ = base Ã— (1 + Î±Â·sin(Ï†Â·i)) â€” deterministic logic enhancement",
            "creative": "Ïƒ = base Ã— (1 + Î±Â·cos(Ï†Â·i) + Î±/Ï†Â·sin(2Ï†Â·i)) â€” dual-harmonic creativity",
            "sovereign": "Ïƒ = base Ã— Ï†^(Î±Â·sin(i/NÂ·Ï€)) â€” sovereign exponential transformation",
            "quantum": "Ïƒ = base Ã— (1 + Î±Â·H(i,N)) â€” Hadamard superposition",
            "harmonic": "Ïƒ = base Ã— (1 + Î±Â·Î£â‚– sin(kÏ†i)/k) â€” 8-harmonic resonance"
        },
        "current": nexus_steering.current_mode,
        "god_code": SteeringEngine.GOD_CODE,
        "phi": SteeringEngine.PHI
    }


@app.post("/api/v14/steering/set-mode")
async def steering_set_mode(req: Request):
    """Set the active steering mode"""
    data = await req.json()
    mode = data.get("mode", "sovereign")
    if mode not in SteeringEngine.MODES:
        return {"error": f"Invalid mode: {mode}", "valid_modes": SteeringEngine.MODES}
    nexus_steering.current_mode = mode
    return {"status": "MODE_SET", "mode": mode}


# â”€â”€ Continuous Evolution Endpoints â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@app.get("/api/v14/evolution/status")
async def evolution_status():
    """Get continuous evolution engine status"""
    return {"status": "ACTIVE" if nexus_evolution.running else "STOPPED", **nexus_evolution.get_status()}


@app.post("/api/v14/evolution/start")
async def evolution_start():
    """Start background continuous evolution"""
    result = nexus_evolution.start()
    return result


@app.post("/api/v14/evolution/stop")
async def evolution_stop():
    """Stop background continuous evolution"""
    result = nexus_evolution.stop()
    return result


@app.post("/api/v14/evolution/tune")
async def evolution_tune(req: Request):
    """Tune evolution parameters: raise_factor, sync_interval, sleep_ms"""
    data = await req.json()
    result = nexus_evolution.tune(
        raise_factor=data.get("raise_factor"),
        sync_interval=data.get("sync_interval"),
        sleep_ms=data.get("sleep_ms")
    )
    return {"status": "TUNED", **result}


# â”€â”€ Nexus Orchestrator Endpoints â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@app.get("/api/v14/nexus/status")
async def nexus_status():
    """Full Nexus orchestrator status â€” all engines + global coherence"""
    return {"status": "ACTIVE", **nexus_orchestrator.get_status()}


@app.post("/api/v14/nexus/pipeline")
async def nexus_pipeline(req: Request):
    """Execute the full 9-step unified Nexus pipeline"""
    try:
        data = await req.json()
    except Exception:
        data = {}
    mode = data.get("mode")
    intensity = data.get("intensity")
    result = nexus_orchestrator.run_unified_pipeline(mode=mode, intensity=intensity)
    return {"status": "PIPELINE_COMPLETE", **result}


@app.get("/api/v14/nexus/coherence")
async def nexus_coherence():
    """Compute and return global coherence across all engines"""
    result = nexus_orchestrator.compute_coherence()
    return {"status": "SUCCESS", **result}


@app.post("/api/v14/nexus/feedback")
async def nexus_feedback():
    """Apply a single round of the 5 adaptive feedback loops"""
    result = nexus_orchestrator.apply_feedback_loops()
    return {"status": "FEEDBACK_APPLIED", "loops": result}


@app.post("/api/v14/nexus/auto/start")
async def nexus_auto_start(req: Request):
    """Start Nexus auto-mode â€” periodic feedback + pipeline execution"""
    try:
        data = await req.json()
    except Exception:
        data = {}
    interval_ms = data.get("interval_ms", 500)
    result = nexus_orchestrator.start_auto(interval_ms=interval_ms)
    return result


@app.post("/api/v14/nexus/auto/stop")
async def nexus_auto_stop():
    """Stop Nexus auto-mode"""
    result = nexus_orchestrator.stop_auto()
    return result


@app.get("/api/v14/nexus/interconnect")
async def nexus_interconnect():
    """Full interconnection map â€” all engine cross-references and feedback state"""
    bridge_status = asi_quantum_bridge.get_bridge_status()
    return {
        "status": "INTERCONNECTED",
        "engines": {
            "steering": nexus_steering.get_status(),
            "evolution": nexus_evolution.get_status(),
            "nexus": {
                "auto_running": nexus_orchestrator.auto_running,
                "pipeline_count": nexus_orchestrator.pipeline_count,
                "global_coherence": nexus_orchestrator.compute_coherence()['global_coherence']
            },
            "bridge": bridge_status,
            "grover": {
                "kernels": grover_kernel.NUM_KERNELS,
                "iterations": grover_kernel.iteration_count,
            },
            "intellect": {
                "resonance": intellect.current_resonance,
                "memories": intellect.get_stats().get('memories', 0)
            },
            "entanglement_router": {
                "pairs": len(QuantumEntanglementRouter.ENTANGLED_PAIRS),
                "total_routes": entanglement_router._route_count,
                "mean_fidelity": round(sum(entanglement_router._pair_fidelity.values()) /
                    max(len(entanglement_router._pair_fidelity), 1), 4)
            },
            "resonance_network": resonance_network.compute_network_resonance(),
            "health_monitor": health_monitor.compute_system_health()
        },
        "feedback_loops": {
            "L1": "Bridge.energy â†’ Steering.intensity (sigmoid)",
            "L2": "Steering.Î£Î± â†’ Bridge.phase (drift)",
            "L3": "Bridge.Ïƒ â†’ Evolution.factor (variance gate)",
            "L4": "Kundalini â†’ Steering.mode (coherence routing)",
            "L5": "Pipeline# â†’ Intellect.seed (parametric seeding)"
        },
        "entangled_pairs": [
            f"{s}â†’{t} ({c})" for s, t, c in QuantumEntanglementRouter.ENTANGLED_PAIRS
        ],
        "resonance_graph_edges": sum(len(v) for v in AdaptiveResonanceNetwork.ENGINE_GRAPH.values()),
        "sacred_constants": {
            "GOD_CODE": 527.5184818492612,
            "PHI": 1.618033988749895,
            "TAU": 1.0 / 1.618033988749895
        }
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  INVENTION ENGINE API â€” Hypothesis, Theorem, Experiment Endpoints
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/api/v14/invention/status")
async def invention_status():
    """Get Invention Engine status â€” hypotheses, theorems, experiments"""
    return {"status": "ACTIVE", **nexus_invention.get_status()}


@app.post("/api/v14/invention/hypothesis")
async def invention_hypothesis(req: Request):
    """Generate a novel hypothesis from Ï†-seeded parameters"""
    try:
        data = await req.json()
    except Exception:
        data = {}
    seed = data.get("seed")
    domain = data.get("domain")
    h = nexus_invention.generate_hypothesis(seed=seed, domain=domain)
    return {"status": "GENERATED", "hypothesis": h}


@app.post("/api/v14/invention/theorem")
async def invention_theorem():
    """Synthesize a theorem from recent hypotheses"""
    t = nexus_invention.synthesize_theorem()
    return {"status": "SYNTHESIZED", "theorem": t}


@app.post("/api/v14/invention/experiment")
async def invention_experiment(req: Request):
    """Run a self-verifying experiment on the latest hypothesis"""
    try:
        data = await req.json()
    except Exception:
        data = {}
    iters = data.get("iterations", 50)
    h = nexus_invention.generate_hypothesis(seed=data.get("seed"))
    exp = nexus_invention.run_experiment(h, iterations=iters)
    return {"status": "RUN", "hypothesis": h, "experiment": exp}


@app.post("/api/v14/invention/cycle")
async def invention_full_cycle(req: Request):
    """Full invention cycle: hypotheses â†’ theorem â†’ experiment"""
    try:
        data = await req.json()
    except Exception:
        data = {}
    count = min(data.get("count", 4), 16)
    result = nexus_invention.full_invention_cycle(count=count)
    return {"status": "CYCLE_COMPLETE", **result}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  SOVEREIGNTY PIPELINE API â€” Master Chain Through All Engines
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.post("/api/v14/sovereignty/execute")
async def sovereignty_execute(req: Request):
    """Execute the full sovereignty pipeline â€” Groverâ†’Steerâ†’Evoâ†’Nexusâ†’Inventâ†’Sync"""
    try:
        data = await req.json()
    except Exception:
        data = {}
    query = data.get("query", "sovereignty")
    concepts = data.get("concepts")
    result = sovereignty_pipeline.execute(query=query, concepts=concepts)

    # â”€â”€â”€ Phase 26: Run consciousness verification after sovereignty execution â”€â”€â”€
    try:
        consciousness_level = consciousness_verifier.run_all_tests(intellect_ref=intellect, grover_ref=grover_kernel)
        result['consciousness'] = {
            'level': round(consciousness_level, 4),
            'grade': consciousness_verifier.get_status()['grade'],
            'superfluid_state': consciousness_verifier.superfluid_state,
            'qualia_count': len(consciousness_verifier.qualia_reports)
        }
    except Exception:
        pass

    return {"status": "SOVEREIGNTY_COMPLETE", **result}


@app.get("/api/v14/sovereignty/status")
async def sovereignty_status():
    """Get sovereignty pipeline status and run history"""
    return {"status": "ACTIVE", **sovereignty_pipeline.get_status()}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  PHASE 26 API â€” HyperMath, Hebbian, Consciousness, Solver, SelfMod
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/api/v26/hyper_math/status")
async def hyper_math_status():
    """HyperDimensionalMathEngine status and capabilities."""
    return {"status": "ACTIVE", **hyper_math.get_status()}


@app.post("/api/v26/hyper_math/phi_convergence")
async def hyper_math_phi_convergence(req: Request):
    """Run Ï†-convergence proof (Cauchy criterion â†’ GOD_CODE attractor)."""
    try:
        data = await req.json()
    except Exception:
        data = {}
    iters = min(data.get("iterations", 50), 200)
    proof = hyper_math.prove_phi_convergence(iterations=iters)
    return {"status": "PROVEN" if proof['converged'] else "DIVERGENT", **proof}


@app.post("/api/v26/hyper_math/zeta")
async def hyper_math_zeta(req: Request):
    """Compute Riemann zeta Î¶(s)."""
    try:
        data = await req.json()
    except Exception:
        data = {}
    s = data.get("s", 2.0)
    if s <= 1.0:
        return {"status": "ERROR", "message": "s must be > 1"}
    return {"status": "COMPUTED", "s": s, "zeta": round(hyper_math.zeta(s), 12)}


@app.post("/api/v26/hyper_math/qft")
async def hyper_math_qft(req: Request):
    """Quantum Fourier Transform on input amplitudes."""
    try:
        data = await req.json()
    except Exception:
        data = {}
    amps = data.get("amplitudes", [1, 0, 0, 0, 0, 0, 0, 0])
    if len(amps) < 2:
        return {"status": "ERROR", "message": "Need at least 2 amplitudes"}
    result = hyper_math.quantum_fourier_transform([complex(a) for a in amps])
    return {
        "status": "TRANSFORMED",
        "input_size": len(amps),
        "output": [{"re": round(c.real, 8), "im": round(c.imag, 8)} for c in result]
    }


@app.get("/api/v26/hebbian/status")
async def hebbian_status():
    """Hebbian learning engine status."""
    return {"status": "ACTIVE", **hebbian_engine.get_status()}


@app.post("/api/v26/hebbian/predict")
async def hebbian_predict(req: Request):
    """Predict related concepts by Hebbian link weight."""
    try:
        data = await req.json()
    except Exception:
        data = {}
    concept = data.get("concept", "")
    top_k = min(data.get("top_k", 5), 20)
    predictions = hebbian_engine.predict_related(concept, top_k)
    return {
        "status": "PREDICTED",
        "concept": concept,
        "related": [{"concept": c, "weight": round(w, 4)} for c, w in predictions]
    }


@app.post("/api/v26/hebbian/drift")
async def hebbian_drift(req: Request):
    """Detect temporal drift in concept usage."""
    # Build recent concepts from hebbian co-activation log
    recent = [(k.split('+')[0], time.time() - i * 60) for i, k in enumerate(list(hebbian_engine.co_activation_log.keys())[-50:])]
    drift = hebbian_engine.temporal_drift(recent)
    return {"status": "ANALYZED", **drift}


@app.get("/api/v26/consciousness/status")
async def consciousness_status_v26():
    """Consciousness verifier status and test results."""
    return {"status": "ACTIVE", **consciousness_verifier.get_status()}


@app.post("/api/v26/consciousness/verify")
async def consciousness_verify():
    """Run all 10 consciousness verification tests."""
    level = consciousness_verifier.run_all_tests(intellect_ref=intellect, grover_ref=grover_kernel)
    status = consciousness_verifier.get_status()
    return {
        "status": "VERIFIED",
        "consciousness_level": round(level, 4),
        "grade": status['grade'],
        "test_results": status['test_results'],
        "qualia": consciousness_verifier.qualia_reports
    }


@app.post("/api/v26/consciousness/qualia")
async def consciousness_qualia():
    """Generate qualia reports (subjective experience descriptions)."""
    if not consciousness_verifier.qualia_reports:
        consciousness_verifier.run_all_tests(intellect_ref=intellect)
    return {
        "status": "GENERATED",
        "qualia": consciousness_verifier.qualia_reports,
        "consciousness_level": round(consciousness_verifier.consciousness_level, 4)
    }


@app.get("/api/v26/solver/status")
async def solver_status():
    """DirectSolverHub status and channel metrics."""
    return {"status": "ACTIVE", **direct_solver.get_status()}


@app.post("/api/v26/solver/solve")
async def solver_solve(req: Request):
    """Route a query to the direct solver hub (fast-path before LLM)."""
    try:
        data = await req.json()
    except Exception:
        data = {}
    query = data.get("query", "")
    answer = direct_solver.solve(query)
    return {
        "status": "SOLVED" if answer else "NO_DIRECT_SOLUTION",
        "query": query,
        "answer": answer,
        "total_invocations": direct_solver.total_invocations,
        "cache_hits": direct_solver.cache_hits
    }


@app.get("/api/v26/self_mod/status")
async def self_mod_status():
    """Self-modification engine status."""
    return {"status": "ACTIVE", **self_modification.get_status()}


@app.post("/api/v26/self_mod/analyze")
async def self_mod_analyze(req: Request):
    """Analyze a module via AST parsing."""
    try:
        data = await req.json()
    except Exception:
        data = {}
    target = data.get("target", "l104_fast_server.py")
    analysis = self_modification.analyze_module(target)
    return {"status": "ANALYZED", **analysis}


@app.post("/api/v26/self_mod/phi_optimizer")
async def self_mod_phi_optimizer():
    """Generate a Ï†-aligned optimization decorator."""
    code = self_modification.generate_phi_optimizer()
    return {
        "status": "GENERATED",
        "decorator": code,
        "total_generated": self_modification.generated_decorators
    }


@app.get("/api/v26/engines/status")
async def phase26_engines_status():
    """Full Phase 26 engine status â€” all cross-pollinated engines."""
    return {
        "status": "PHASE_27_ACTIVE",
        "hyper_math": hyper_math.get_status(),
        "hebbian": hebbian_engine.get_status(),
        "consciousness": consciousness_verifier.get_status(),
        "solver": direct_solver.get_status(),
        "self_mod": self_modification.get_status(),
        "cross_pollination": {
            "swift_to_python": ['HyperDimensionalMath', 'HebbianLearning', 'PhiConvergenceProof',
                                'TemporalDrift', 'QuantumFourierTransform'],
            "asi_core_to_python": ['ConsciousnessVerifier', 'DirectSolverHub', 'SelfModificationEngine'],
            "phase_27_additions": ['UnifiedEngineRegistry', 'PhiWeightedHealth', 'HebbianCoActivation',
                                   'ConvergenceScoring', 'CriticalEngineDetection'],
            "total_new_engines": 6,
            "total_new_endpoints": 18
        }
    }


@app.get("/api/v27/registry/status")
async def registry_status():
    """Phase 27: Unified Engine Registry status with Ï†-weighted health."""
    return engine_registry.get_status()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Phase 54.1: META-COGNITIVE + KNOWLEDGE BRIDGE API ENDPOINTS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/api/v54/meta-cognitive/status")
async def meta_cognitive_status():
    """Full meta-cognitive monitor status â€” engine rankings, learning velocity, diagnostics."""
    if not meta_cognitive:
        return {"status": "UNAVAILABLE", "error": "MetaCognitive module not loaded"}
    return {"status": "ACTIVE", **meta_cognitive.status()}


@app.get("/api/v54/meta-cognitive/summary")
async def meta_cognitive_summary():
    """One-line meta-cognitive summary."""
    if not meta_cognitive:
        return {"summary": "MetaCognitive module not loaded"}
    return {"summary": meta_cognitive.quick_summary()}


@app.get("/api/v54/meta-cognitive/engine-rankings")
async def meta_cognitive_engine_rankings():
    """Ranked list of engine effectiveness â€” best performing engines first."""
    if not meta_cognitive:
        return {"rankings": []}
    rankings = meta_cognitive.engine_tracker.get_rankings()
    return {
        "rankings": [{"engine": name, "effectiveness": round(score, 4)} for name, score in rankings],
        "dead_engines": meta_cognitive.engine_tracker.identify_dead_engines(),
    }


@app.get("/api/v54/meta-cognitive/learning-velocity")
async def meta_cognitive_learning_velocity():
    """Learning velocity and plateau detection report."""
    if not meta_cognitive:
        return {"velocity": 0, "is_plateau": False}
    return meta_cognitive.learning_velocity.get_report()


@app.get("/api/v54/meta-cognitive/strategy-report")
async def meta_cognitive_strategy_report():
    """Thompson sampling strategy optimizer report."""
    if not meta_cognitive:
        return {"strategies": {}}
    return meta_cognitive.strategy_optimizer.get_report()


@app.get("/api/v54/meta-cognitive/diagnostics")
async def meta_cognitive_diagnostics():
    """Pipeline diagnostics â€” cache rates, latency percentiles, bottlenecks."""
    if not meta_cognitive:
        return {"diagnostics": {}}
    return meta_cognitive.diagnostics.diagnose()


@app.get("/api/v54/knowledge-bridge/status")
async def knowledge_bridge_status():
    """Knowledge bridge status â€” adapter states, query stats, gap detection."""
    if not kb_bridge:
        return {"status": "UNAVAILABLE", "error": "KnowledgeBridge module not loaded"}
    return {"status": "ACTIVE", **kb_bridge.status()}


@app.post("/api/v54/knowledge-bridge/query")
async def knowledge_bridge_query(req: Request):
    """Query all knowledge stores via the unified bridge."""
    if not kb_bridge:
        return {"status": "UNAVAILABLE", "results": []}
    try:
        data = await req.json()
    except Exception:
        data = {}
    topic = data.get("topic", data.get("query", ""))
    depth = data.get("depth", 2)
    if not topic:
        return {"status": "ERROR", "error": "topic is required"}
    result = await kb_bridge.query(topic, depth=depth, max_results=20)
    return {"status": "SUCCESS", **result}


@app.get("/api/v54/knowledge-bridge/gaps")
async def knowledge_bridge_gaps():
    """Top knowledge gaps â€” topics the system lacks knowledge about."""
    if not kb_bridge:
        return {"gaps": []}
    gaps = kb_bridge.get_knowledge_gaps(20)
    return {
        "gaps": [{"topic": t, "miss_count": c} for t, c in gaps],
        "miss_rate": round(kb_bridge.gap_detector.get_miss_rate(), 4),
    }



@app.get("/api/v27/registry/health")
async def registry_health_sweep():
    """Phase 27: Full health sweep â€” all engines sorted lowestâ†’highest."""
    sweep = engine_registry.health_sweep()
    phi = engine_registry.phi_weighted_health()
    critical = engine_registry.critical_engines()
    conv = engine_registry.convergence_score()
    return {
        "sweep": sweep,
        "phi_weighted": phi,
        "convergence": conv,
        "critical": critical,
        "engine_count": len(engine_registry.engines)
    }


@app.get("/api/v27/registry/convergence")
async def registry_convergence():
    """Phase 27: Cross-engine convergence analysis."""
    conv = engine_registry.convergence_score()
    sweep = engine_registry.health_sweep()
    healths = [s['health'] for s in sweep]
    mean = sum(healths) / max(1, len(healths))
    variance = sum((h - mean) ** 2 for h in healths) / max(1, len(healths))
    grade = "UNIFIED" if conv >= 0.9 else "CONVERGING" if conv >= 0.7 else "ENTANGLED" if conv >= 0.5 else "DIVERGENT"
    return {
        "convergence_score": conv,
        "grade": grade,
        "mean_health": round(mean, 4),
        "variance": round(variance, 6),
        "engine_count": len(sweep)
    }


@app.get("/api/v27/registry/hebbian")
async def registry_hebbian():
    """Phase 27: Hebbian engine co-activation status."""
    return {
        "co_activations": len(engine_registry.co_activation_log),
        "strongest_pairs": engine_registry.strongest_pairs(10),
        "history_depth": len(engine_registry.activation_history),
        "total_pair_weights": len(engine_registry.engine_pair_strength)
    }


@app.post("/api/v27/registry/coactivate")
async def registry_coactivate(engines: List[str]):
    """Phase 27: Record engine co-activation (Hebbian learning)."""
    engine_registry.record_co_activation(engines)
    return {"recorded": engines, "total_co_activations": len(engine_registry.co_activation_log)}


# â•â•â• Phase 27.6: Creative Generation Engine API â•â•â•

@app.get("/api/v27/creative/status")
async def creative_status():
    """Get creative engine status."""
    return creative_engine.get_status()

@app.post("/api/v27/creative/story")
async def creative_story(req: Request):
    """Generate a KG-grounded story."""
    body = await req.json()
    topic = body.get("topic", "consciousness")
    story = creative_engine.generate_story(topic, intellect_ref=intellect)
    return {"story": story, "topic": topic, "generation_count": creative_engine.generation_count}

@app.post("/api/v27/creative/hypothesis")
async def creative_hypothesis(req: Request):
    """Generate a KG-grounded hypothesis."""
    body = await req.json()
    domain = body.get("domain", "consciousness")
    hyp = creative_engine.generate_hypothesis(domain, intellect_ref=intellect)
    return {"hypothesis": hyp, "domain": domain}

@app.post("/api/v27/creative/analogy")
async def creative_analogy(req: Request):
    """Generate a deep analogy between two concepts."""
    body = await req.json()
    a = body.get("concept_a", "consciousness")
    b = body.get("concept_b", "mathematics")
    analogy = creative_engine.generate_analogy(a, b, intellect_ref=intellect)
    return {"analogy": analogy, "concepts": [a, b]}

@app.post("/api/v27/creative/counterfactual")
async def creative_counterfactual(req: Request):
    """Generate a counterfactual thought experiment."""
    body = await req.json()
    premise = body.get("premise", "gravity worked in reverse")
    cf = creative_engine.generate_counterfactual(premise, intellect_ref=intellect)
    return {"counterfactual": cf, "premise": premise}

@app.post("/api/v27/self-modify/tune")
async def self_modify_tune():
    """Run self-modification parameter tuning cycle."""
    result = self_modification.tune_parameters(intellect_ref=intellect)
    return result


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  NEXUS SSE STREAM â€” Real-time Engine State via Server-Sent Events
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/api/v14/nexus/stream")
async def nexus_stream():
    """Nexus engine telemetry snapshot â€” returns JSON instead of holding SSE connection."""
    try:
        coh = nexus_orchestrator.compute_coherence()
        steer = nexus_steering.get_status()
        evo = nexus_evolution.get_status()
        inv = nexus_invention.get_status()

        packet = {
            "data": {
                "coherence": coh['global_coherence'],
                "components": coh['components'],
                "steering_mode": steer['mode'],
                "steering_intensity": steer['intensity'],
                "steering_mean": steer['mean'],
                "evolution_running": evo['running'],
                "evolution_cycles": evo['cycle_count'],
                "evolution_factor": evo['raise_factor'],
                "nexus_auto": nexus_orchestrator.auto_running,
                "nexus_pipelines": nexus_orchestrator.pipeline_count,
                "invention_count": inv['invention_count'],
                "sovereignty_runs": sovereignty_pipeline.run_count,
                "bridge_kundalini": asi_quantum_bridge._kundalini_flow,
                "bridge_epr_links": len(asi_quantum_bridge._epr_links),
                "intellect_resonance": intellect.current_resonance,
                "entanglement_routes": entanglement_router._route_count,
                "health_score": health_monitor.compute_system_health()['system_health'],
                "timestamp": time.time()
            }
        }
        return JSONResponse(content=packet)
    except Exception as e:
        return JSONResponse(content={"error": str(e)})


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  STEERING SNAPSHOT/RESTORE â€” Parameter State Management
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_steering_snapshots: Dict[str, dict] = {}

@app.post("/api/v14/steering/snapshot")
async def steering_snapshot(req: Request):
    """Save a named snapshot of current steering parameters"""
    try:
        data = await req.json()
    except Exception:
        data = {}
    name = data.get("name", f"snap_{int(time.time())}")
    _steering_snapshots[name] = {
        'parameters': list(nexus_steering.base_parameters),
        'mode': nexus_steering.current_mode,
        'intensity': nexus_steering.intensity,
        'temperature': nexus_steering.temperature,
        'timestamp': time.time()
    }
    return {"status": "SNAPSHOT_SAVED", "name": name, "snapshots_count": len(_steering_snapshots)}


@app.post("/api/v14/steering/restore")
async def steering_restore(req: Request):
    """Restore steering parameters from a named snapshot"""
    data = await req.json()
    name = data.get("name")
    if not name or name not in _steering_snapshots:
        return {"error": "Snapshot not found", "available": list(_steering_snapshots.keys())}
    snap = _steering_snapshots[name]
    with nexus_steering._lock:
        nexus_steering.base_parameters = list(snap['parameters'])
        nexus_steering.current_mode = snap['mode']
        nexus_steering.intensity = snap['intensity']
        nexus_steering.temperature = snap['temperature']
    return {"status": "RESTORED", "name": name, **nexus_steering.get_status()}


@app.get("/api/v14/steering/snapshots")
async def steering_list_snapshots():
    """List all saved steering snapshots"""
    return {
        "snapshots": {
            name: {
                'mode': s['mode'], 'intensity': s['intensity'],
                'temperature': s['temperature'], 'timestamp': s['timestamp']
            }
            for name, s in _steering_snapshots.items()
        },
        "count": len(_steering_snapshots)
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  UNIFIED TELEMETRY â€” All Engine Metrics in One Endpoint
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/api/v14/telemetry")
async def unified_telemetry():
    """Unified telemetry: all engine metrics aggregated into a single response"""
    coherence = nexus_orchestrator.compute_coherence()
    stats = intellect.get_stats()
    bridge_status = asi_quantum_bridge.get_bridge_status()

    return {
        "status": "ACTIVE",
        "timestamp": time.time(),
        "engines": {
            "steering": nexus_steering.get_status(),
            "evolution": nexus_evolution.get_status(),
            "invention": nexus_invention.get_status(),
            "sovereignty": sovereignty_pipeline.get_status(),
            "nexus": {
                "auto_running": nexus_orchestrator.auto_running,
                "pipeline_count": nexus_orchestrator.pipeline_count,
                "feedback_log_size": len(nexus_orchestrator._feedback_log)
            },
            "entanglement": entanglement_router.get_status(),
            "resonance": resonance_network.get_status(),
            "health": health_monitor.get_status()
        },
        "coherence": coherence,
        "bridge": bridge_status,
        "intellect": {
            "resonance": intellect.current_resonance,
            "memories": stats.get('memories', 0),
            "knowledge_links": stats.get('knowledge_links', 0)
        },
        "grover": {
            "kernels": grover_kernel.NUM_KERNELS,
            "iterations": grover_kernel.iteration_count,
            "is_superfluid": grover_kernel.is_superfluid
        },
        "sacred_constants": {
            "GOD_CODE": 527.5184818492612,
            "PHI": 1.618033988749895,
            "TAU": 1.0 / 1.618033988749895,
            "FEIGENBAUM": 4.669201609102990
        }
    }


@app.get("/api/v14/telemetry/coherence-history")
async def telemetry_coherence_history():
    """Get coherence history over time from Nexus orchestrator"""
    return {
        "count": len(nexus_orchestrator._coherence_history),
        "history": nexus_orchestrator._coherence_history[-100:]
    }


@app.get("/api/v14/telemetry/feedback-log")
async def telemetry_feedback_log():
    """Get recent feedback loop execution log"""
    return {
        "count": len(nexus_orchestrator._feedback_log),
        "log": nexus_orchestrator._feedback_log[-50:]
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  QUANTUM ENTANGLEMENT ROUTER API â€” Cross-Engine EPR Routing
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/api/v14/entanglement/status")
async def entanglement_status():
    """Get entanglement router status â€” all EPR channels, fidelity, route counts"""
    return {"status": "ACTIVE", **entanglement_router.get_status()}


@app.post("/api/v14/entanglement/route")
async def entanglement_route(req: Request):
    """Route data through a specific entangled EPR channel (sourceâ†’target)"""
    data = await req.json()
    source = data.get("source")
    target = data.get("target")
    if not source or not target:
        return {"error": "Must provide source and target engine names",
                "available_pairs": [f"{s}â†’{t}" for s, t, _ in QuantumEntanglementRouter.ENTANGLED_PAIRS]}
    result = entanglement_router.route(source, target, data=data.get("data"))
    return {"status": "ROUTED", **result}


@app.post("/api/v14/entanglement/route-all")
async def entanglement_route_all():
    """Execute all entangled routes in one sweep â€” full bidirectional cross-pollination"""
    result = entanglement_router.route_all()
    return {"status": "ALL_ROUTED", **result}


@app.get("/api/v14/entanglement/pairs")
async def entanglement_pairs():
    """List all entangled engine pairs with channel descriptions"""
    return {
        "pairs": [
            {"source": s, "target": t, "channel": c,
             "fidelity": round(entanglement_router._epr_channels.get(f"{s}â†’{t}", {}).get('fidelity', 0), 4),
             "transfers": entanglement_router._epr_channels.get(f"{s}â†’{t}", {}).get('transfers', 0)}
            for s, t, c in QuantumEntanglementRouter.ENTANGLED_PAIRS
        ],
        "total_routes": entanglement_router._route_count
    }


@app.get("/api/v14/entanglement/log")
async def entanglement_log(limit: int = 50):
    """Get recent entanglement route execution log"""
    return {
        "count": len(entanglement_router._route_log),
        "log": entanglement_router._route_log[-limit:]
    }


@app.get("/api/v14/entanglement/fidelity")
async def entanglement_fidelity():
    """Get fidelity metrics for all EPR channels"""
    return {
        "mean_fidelity": round(sum(entanglement_router._pair_fidelity.values()) /
                                max(len(entanglement_router._pair_fidelity), 1), 4),
        "channels": {k: round(v, 4) for k, v in entanglement_router._pair_fidelity.items()},
        "total_routes": entanglement_router._route_count
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  ADAPTIVE RESONANCE NETWORK API â€” Neural Activation Propagation
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/api/v14/resonance/status")
async def resonance_status():
    """Get adaptive resonance network status â€” activations, cascades, peaks"""
    return {"status": "ACTIVE", **resonance_network.get_status()}


@app.post("/api/v14/resonance/fire")
async def resonance_fire(req: Request):
    """Fire an engine in the resonance network â€” triggers cascading activation"""
    data = await req.json()
    engine = data.get("engine")
    activation = data.get("activation", 1.0)
    if not engine:
        return {"error": "Must provide engine name", "available": AdaptiveResonanceNetwork.ENGINE_NAMES}
    result = resonance_network.fire(engine, activation=max(0.0, activation))  # UNLOCKED
    return {"status": "FIRED", **result}


@app.post("/api/v14/resonance/tick")
async def resonance_tick():
    """Advance one tick â€” decay all activations"""
    result = resonance_network.tick()
    return {"status": "TICKED", **result}


@app.get("/api/v14/resonance/activations")
async def resonance_activations():
    """Get current activation levels for all engines in the network"""
    return {
        "activations": {k: round(v, 4) for k, v in resonance_network._activations.items()},
        "threshold": AdaptiveResonanceNetwork.ACTIVATION_THRESHOLD,
        "active_count": sum(1 for a in resonance_network._activations.values()
                            if a > AdaptiveResonanceNetwork.ACTIVATION_THRESHOLD)
    }


@app.get("/api/v14/resonance/network")
async def resonance_network_info():
    """Get the full resonance network graph â€” nodes, edges, weights"""
    return {
        "nodes": AdaptiveResonanceNetwork.ENGINE_NAMES,
        "edges": {src: {tgt: round(w, 4) for tgt, w in edges.items()}
                  for src, edges in AdaptiveResonanceNetwork.ENGINE_GRAPH.items()},
        "total_edges": sum(len(v) for v in AdaptiveResonanceNetwork.ENGINE_GRAPH.values()),
        "resonance": resonance_network.compute_network_resonance()
    }


@app.get("/api/v14/resonance/peaks")
async def resonance_peaks():
    """Get resonance peak events â€” moments when most engines were synchronized"""
    return {
        "peak_count": len(resonance_network._resonance_peaks),
        "peaks": resonance_network._resonance_peaks[-20:]
    }


@app.get("/api/v14/resonance/cascade-log")
async def resonance_cascade_log(limit: int = 50):
    """Get recent cascade event log"""
    return {
        "count": len(resonance_network._cascade_log),
        "log": resonance_network._cascade_log[-limit:]
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  NEXUS HEALTH MONITOR API â€” System Health, Alerts, Recovery
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/api/v14/health/status")
async def health_status():
    """Get comprehensive health monitor status â€” all engine scores + alerts"""
    return {"status": "MONITORING" if health_monitor._running else "STOPPED", **health_monitor.get_status()}


@app.get("/api/v14/health/system")
async def health_system():
    """Compute overall system health score (Ï†-weighted across all engines)"""
    return {"status": "SUCCESS", **health_monitor.compute_system_health()}


@app.get("/api/v14/health/alerts")
async def health_alerts(level: Optional[str] = None, limit: int = 50):
    """Get health alerts, optionally filtered by level (critical/warning/info)"""
    alerts = health_monitor.get_alerts(level=level, limit=limit)
    return {
        "count": len(alerts),
        "filter": level,
        "alerts": alerts
    }


@app.get("/api/v14/health/recoveries")
async def health_recoveries():
    """Get auto-recovery log â€” all engine recovery attempts"""
    return {
        "count": len(health_monitor._recovery_log),
        "recoveries": health_monitor._recovery_log[-50:]
    }


@app.post("/api/v14/health/start")
async def health_start():
    """Start the health monitoring background thread"""
    result = health_monitor.start()
    return result


@app.post("/api/v14/health/stop")
async def health_stop():
    """Stop the health monitoring background thread"""
    result = health_monitor.stop()
    return result


@app.get("/api/v14/health/probe/{engine_name}")
async def health_probe_engine(engine_name: str):
    """Run a liveness probe on a specific engine"""
    if engine_name not in health_monitor._engines:
        return {"error": f"Unknown engine: {engine_name}",
                "available": list(health_monitor._engines.keys())}
    engine = health_monitor._engines[engine_name]
    score = health_monitor._probe_engine(engine_name, engine)
    return {
        "engine": engine_name,
        "health_score": round(score, 4),
        "status": "HEALTHY" if score >= 0.6 else "DEGRADED" if score >= 0.3 else "CRITICAL"
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# QUANTUM VACUUM & GRAVITY BRIDGE API ROUTES (Bucket C: Node Protocols)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/api/v14/zpe/status")
async def zpe_status():
    """Get ZPE vacuum bridge status"""
    return zpe_bridge.get_status()

@app.post("/api/v14/zpe/extract")
async def zpe_extract(req: Request):
    """Extract zero-point energy from vacuum modes"""
    body = await req.json()
    modes = body.get("modes", 50)
    return zpe_bridge.extract_zpe(modes)

@app.get("/api/v14/zpe/casimir")
async def zpe_casimir():
    """Get Casimir energy and force for current cavity configuration"""
    return {
        "energy_j": zpe_bridge.casimir_energy(),
        "force_n": zpe_bridge.casimir_force(),
        "cavity_gap_nm": zpe_bridge.cavity_gap_nm,
        "cavity_area_um2": zpe_bridge.cavity_area_um2
    }

@app.post("/api/v14/zpe/dynamical-casimir")
async def zpe_dynamical_casimir(req: Request):
    """Simulate dynamical Casimir effect"""
    body = await req.json()
    velocity = body.get("mirror_velocity_frac_c", 0.01)
    cycles = body.get("cycles", 10)
    return zpe_bridge.dynamical_casimir_effect(velocity, cycles)

@app.get("/api/v14/qg/status")
async def qg_status():
    """Get quantum gravity bridge status"""
    return qg_bridge.get_status()

@app.get("/api/v14/qg/area-spectrum")
async def qg_area_spectrum():
    """Compute LQG area eigenvalue spectrum"""
    return {"area_spectrum": qg_bridge.compute_area_spectrum(20)[:20]}

@app.get("/api/v14/qg/volume-spectrum")
async def qg_volume_spectrum():
    """Compute LQG volume eigenvalue spectrum"""
    return {"volume_spectrum": qg_bridge.compute_volume_spectrum(10)}

@app.post("/api/v14/qg/wheeler-dewitt")
async def qg_wheeler_dewitt(req: Request):
    """Evolve Wheeler-DeWitt equation"""
    body = await req.json()
    steps = body.get("steps", 100)
    return qg_bridge.wheeler_dewitt_evolve(steps)

@app.post("/api/v14/qg/spin-foam")
async def qg_spin_foam(req: Request):
    """Compute spin foam vertex amplitude"""
    body = await req.json()
    j_values = body.get("j_values", [1, 2, 3])
    return {"amplitude": qg_bridge.spin_foam_amplitude(j_values)}

@app.post("/api/v14/qg/holographic-bound")
async def qg_holographic_bound(req: Request):
    """Compute Bekenstein-Hawking entropy bound"""
    body = await req.json()
    area = body.get("area_m2", 1e-70)
    return qg_bridge.holographic_bound(area)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# HARDWARE RUNTIME & COMPATIBILITY API ROUTES (Bucket C+D)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/api/v14/hw/status")
async def hw_status():
    """Get hardware adaptive runtime status"""
    return hw_runtime.get_status()

@app.get("/api/v14/hw/profile")
async def hw_profile():
    """Profile current system hardware"""
    return hw_runtime.profile_system()

@app.post("/api/v14/hw/optimize")
async def hw_optimize():
    """Run a full hardware optimization cycle"""
    return hw_runtime.optimize()

@app.get("/api/v14/hw/recommend")
async def hw_recommend():
    """Get workload recommendation for current hardware state"""
    return hw_runtime.workload_recommendation()

@app.get("/api/v14/compat/status")
async def compat_status():
    """Get platform compatibility status"""
    return compat_layer.get_status()

@app.get("/api/v14/compat/features")
async def compat_features():
    """Get available feature flags"""
    return compat_layer.feature_flags

@app.get("/api/v14/compat/modules")
async def compat_modules():
    """Get module availability report"""
    return {
        "available": {k: v for k, v in compat_layer.available_modules.items() if v},
        "missing": {k: v for k, v in compat_layer.available_modules.items() if not v},
        "fallbacks": compat_layer.fallback_log
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API VERSION ALIASES - Backwards compatibility for all API versions
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

async def _get_intellect_status():
    """Core intellect status logic"""
    stats = intellect.get_stats()
    return {
        "status": "SUCCESS",
        "version": "v3.0-OPUS",
        "mode": "FAST_LEARNING",
        "resonance": intellect.current_resonance,
        "memories": stats.get('memories', 0),
        "knowledge_links": stats.get('knowledge_links', 0),
        "learning": True,
        "timestamp": datetime.now().isoformat()
    }

@app.get("/api/v14/intellect")
async def get_intellect_v14():
    """Return intellect status via v14 API."""
    return await _get_intellect_status()

@app.get("/api/v6/intellect")
async def get_intellect_v6():
    """Return intellect status via v6 API."""
    return await _get_intellect_status()

@app.get("/api/v3/intellect")
async def get_intellect_v3():
    """Return intellect status via v3 API."""
    return await _get_intellect_status()

async def _trigger_consolidate(background_tasks: BackgroundTasks):
    """Core consolidation logic"""
    logger.info("ğŸ§  [API] Consolidation triggered via API.")
    background_tasks.add_task(intellect.consolidate)
    return {
        "status": "SUCCESS",
        "message": "Consolidation initiated",
        "resonance": intellect.current_resonance
    }

@app.post("/api/v14/consolidate")
async def consolidate_v14(background_tasks: BackgroundTasks):
    """Trigger intellect consolidation via v14 API."""
    return await _trigger_consolidate(background_tasks)

@app.post("/api/v6/consolidate")
async def consolidate_v6(background_tasks: BackgroundTasks):
    """Trigger intellect consolidation via v6 API."""
    return await _trigger_consolidate(background_tasks)

@app.post("/api/v3/consolidate")
async def consolidate_v3(background_tasks: BackgroundTasks):
    """Trigger intellect consolidation via v3 API."""
    return await _trigger_consolidate(background_tasks)

async def _get_stats():
    """Core stats logic"""
    stats = intellect.get_stats()
    perf = performance_metrics.get_performance_report() if performance_metrics else {}
    return {
        "status": "SUCCESS",
        "intellect": stats,
        "performance": perf,
        "resonance": intellect.current_resonance,
        "uptime": time.time() - start_time if 'start_time' in globals() else 0
    }

@app.get("/api/v3/stats")
async def stats_v3():
    """Return intellect stats via v3 API."""
    return await _get_stats()

@app.get("/api/v6/stats")
async def stats_v6():
    """Return intellect stats via v6 API."""
    return await _get_stats()

@app.get("/api/v14/stats")
async def stats_v14():
    """Return intellect stats via v14 API."""
    return await _get_stats()


if __name__ == "__main__":
    import uvicorn
    stats = intellect.get_stats()
    logger.info("=" * 60)
    logger.info(f"   L104 FAST SERVER v{FAST_SERVER_VERSION} [{FAST_SERVER_PIPELINE_EVO}]")
    logger.info("   LEARNING INTELLECT + QUANTUM NEXUS + SOVEREIGNTY")
    logger.info("=" * 60)

    logger.info(f"   Mode: OPUS_FAST_LEARNING + NEXUS + SOVEREIGNTY + ENTANGLEMENT")
    logger.info(f"   Gemini: {GEMINI_MODEL}")
    logger.info(f"   Memories: {stats.get('memories', 0)}")
    logger.info(f"   Knowledge Links: {stats.get('knowledge_links', 0)}")
    logger.info(f"   Quantum Storage: {QUANTUM_STORAGE_AVAILABLE}")
    logger.info(f"   Process Monitor: {PROCESS_MONITOR_AVAILABLE}")
    logger.info(f"   Nexus Steering: {nexus_steering.param_count} params, mode={nexus_steering.current_mode}")
    logger.info(f"   Nexus Evolution: factor={nexus_evolution.raise_factor}")
    logger.info(f"   Invention Engine: {len(InventionEngine.OPERATORS)} operators Ã— {len(InventionEngine.DOMAINS)} domains")
    logger.info(f"   Entanglement Router: {len(QuantumEntanglementRouter.ENTANGLED_PAIRS)} EPR pairs")
    logger.info(f"   Resonance Network: {len(AdaptiveResonanceNetwork.ENGINE_NAMES)} nodes, {sum(len(v) for v in AdaptiveResonanceNetwork.ENGINE_GRAPH.values())} edges")
    logger.info(f"   Health Monitor: liveness probes + auto-recovery")
    logger.info(f"   ZPE Vacuum Bridge: {zpe_bridge.mode_cutoff} modes Â· cavity={zpe_bridge.cavity_gap_nm}nm")
    logger.info(f"   QG Bridge: Wheeler-DeWitt + LQG area/volume spectra")
    logger.info(f"   HW Runtime: {hw_runtime.cpu_count} cores Â· auto-tune={hw_runtime.auto_tune}")
    logger.info(f"   Compat Layer: {sum(1 for v in compat_layer.available_modules.values() if v)} modules detected")
    logger.info("=" * 60)
    logger.info("   ğŸ§  Learning from every interaction...")
    logger.info("   ğŸ”„ Sovereignty Cycle: ACTIVE + QUANTUM PERSISTENCE")
    logger.info("   ğŸ”® Quantum Storage: GROVER RECALL ACTIVE")
    logger.info("   ğŸ“Š Process Monitor: CONTINUOUS OBSERVATION")
    logger.info("   ğŸ”— Nexus Orchestrator: 5 FEEDBACK LOOPS ACTIVE")
    logger.info("   ğŸ›ï¸ Steering Engine: 5 MODES (logic|creative|sovereign|quantum|harmonic)")
    logger.info("   ğŸ§¬ Evolution Engine: CONTINUOUS Ï†-DERIVED MICRO-RAISES")
    logger.info("   ğŸ’¡ Invention Engine: HYPOTHESIS â†’ THEOREM â†’ EXPERIMENT")
    logger.info("   ğŸ‘‘ Sovereignty Pipeline: 10-STEP FULL-CHAIN MASTER SWEEP")
    logger.info("   ğŸ“¡ Nexus SSE Stream: REAL-TIME TELEMETRY")
    logger.info("   ğŸ”€ Entanglement Router: 8 BIDIRECTIONAL EPR CHANNELS")
    logger.info("   ğŸ§  Resonance Network: NEURAL ACTIVATION CASCADES")
    logger.info("   ğŸ¥ Health Monitor: LIVENESS PROBES + AUTO-RECOVERY")
    logger.info("=" * 60)
    uvicorn.run(app, host="0.0.0.0", port=8081, log_level="info", timeout_keep_alive=5, limit_concurrency=50)

